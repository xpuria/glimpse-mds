id,best_rsa,best_base,gold
https://openreview.net/forum?id=BkCPyXm1l,"['The paper introduced a regularization scheme through soft-target. It is very similar to (Hinton et al. 2016)'
 'This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels. They claim that this prevents the disappearing of co-label similarity after early training.'
 'This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself.']","['The paper introduced a regularization scheme through soft-target. It is very similar to (Hinton et al. 2016)'
 'This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels. They claim that this prevents the disappearing of co-label similarity after early training.'
 'This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself.']",The reviewers unanimously recommend rejection.
https://openreview.net/forum?id=S1J0E-71l,"['This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input. Authors have shown a result on language modeling tasks.'
 'This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function.'
 'This paper proposes to leverage ""surprisal"" as top-down signal in RNN. The paper in its current form has some important flaws.']","['This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input. Authors have shown a result on language modeling tasks.'
 'This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function.'
 'This paper proposes to leverage ""surprisal"" as top-down signal in RNN. The paper in its current form has some important flaws.']","Based on the feedback, I'm going to be rejecting the paper on the following grounds:  1. Results are not SOTA as reported.  2. No real experiments other than cursory experiments on Hutter prize data.  2. Writing is very poor.    However, just for playing devil's advocate, to the reviewers, I would like to point out that I am in agreement with the author that dynamic evaluation is not equivalent to this method. The weights are not changed in this model, as far as I can see, for the test set. Surprisal is just an extra input to the model. I think the reviewers were puzzled by the fact that at test time, the actual sequence needs to be known. While this may be problematic for generative modeling, I do not see why this would be a problem for language modeling, where the goal of the model is only to provide a log prob to evaluate how good a sequence of text is. Long before language modeling started being used to generate text, this was the main reason to use it - in speech recognition, spelling correction etc.."
https://openreview.net/forum?id=r1rhWnZkg,"['The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance.'
 'Results on the VQA task are good for this simple model. Missing are some explanations about the language embedding.'
 'The paper discusses how the Hadamard product can be used to approximate the full outer product. The full model archives a slight improvement over prior state-of-the-art on the challenging VQA challenge.']","['The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance.'
 'Results on the VQA task are good for this simple model. Missing are some explanations about the language embedding.'
 'The paper discusses how the Hadamard product can be used to approximate the full outer product. The full model archives a slight improvement over prior state-of-the-art on the challenging VQA challenge.']","The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR."
