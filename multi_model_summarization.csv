id,text,gold,best_summary,best_model_by_rsa,best_model_by_uniqueness,rsa_scores,uniqueness_scores,summary_facebook/bart-large-cnn,summary_sshleifer/distilbart-cnn-12-6,summary_Falconsai/text_summarization,rouge_rouge1,rouge_rouge2,rouge_rougeL,bertscore_precision,bertscore_recall,bertscore_f1
https://openreview.net/forum?id=r1rhWnZkg,"Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3. The various design choices made in model development have been experimentally verified. ----------------Weaknesses/Suggestions:----------------1. When authors explicitly (keeping rest of the model architecture same) compared low-rank bilinear pooling with compact bilinear pooling, they found that low-rank bilinear pooling performs worse. Hence, it could not be experimentally verified that low-rank bilinear pooling is better in performance than compact bilinear pooling (at least for the task of VQA).----------------2. The authors argue that low-rank bilinear pooling uses 25% less parameters than compact bilinear pooling. So, could the authors please explain how does the reduction in number of parameters help experimentally? Does the training time of the model reduce significantly? Can we train the model with less data? ----------------3. One of the contributions of the paper is that the proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than the current state-of-art.----------------4. I would like the authors to explicitly mention the differences between MRN, MARN and MLB. It is not very clear from reading the paper.----------------5. In the caption for Table 1, fix the following: “have not” -> “have no” ----------------Review Summary: I like the insights about low-rank bilinear pooling using Hadamard product (element-wise multiplication) presented in the paper. However, it could not be justified that low-rank bilinear pooling leads to better performance than compact biliear pooling. It does lead to reduction in number of parameters but it is not clear how much that helps experimentally. So, to be more convinced I would like the authors to provide experimental justification of why low-rank bilinear pooling is better than other forms of pooling.","The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR.",The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) The proposed model outperforms the current state-of-art on VQA by 0.42,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) The proposed model outperforms the current state-of-art on VQA by 0.42," The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-",of low-rank bilinear pooling using Hadamard product (element-wise multiplication) The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise,0.17948717948717946,0.026315789473684213,0.15384615384615383,0.8174753189086914,0.8402736186981201,0.8287177085876465
https://openreview.net/forum?id=r1rhWnZkg,"Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. ----------------Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the--------paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible?  How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components when that part of the sentence is encoded, then we stack according to your equation 9?","The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR.","Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. Missing are some explanations about the language embedding and the importance in deciding embedding dimension",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. Missing are some explanations about the language embedding and the importance in deciding embedding dimension"," Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important . Missing are some explanations about the language embedding and the importance in deciding embedding dimension","in the compact bilinear model, the ablation study of table 1 gives some insights as to what is important. ------------Missing are some explanations about the language embedding and the importance in",0.25287356321839083,0.023529411764705882,0.16091954022988506,0.830254852771759,0.8366507291793823,0.8334404826164246
https://openreview.net/forum?id=r1rhWnZkg,"This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). --------This formulation is evaluated on the visual question answering (VQA) task together with several other model variants.----------------Strength:--------1. The paper discusses how the Hadamard product can be used to approximate the full outer product.--------2. The paper provides an extensive experimental evaluation of other model aspect for VQA.--------3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.----------------Weaknesses:--------1. Novelty: The paper presents only a new “interpretation” of the Hadamard product which has previously been widely used for pooling, including for VQA.--------2. Experimental evaluation:--------2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar to Fukui et al. several other changes have been made, including question encoding (GRU vs. LSTM), normalization (tanh vs. L2 vs. none). The small difference in performance (0.44% om Table 1) could easily be attributed to these differences.--------2.2. An experimental comparison to the full outer product (e.g. for a lower dimension) is missing. It remains unclear how good the proposed approximation for the full outer product is. While a comparison to MCB is presented this seems insufficient as MCB is a very different model.--------2.3. One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?--------2.4. Comparison with other pooling strategies, e.g. elementwise sum instead of elementwise product.--------3. No theoretical analysis or properties of the approximation are presented.--------4. The paper seems to be general at the beginning, but the claim of the benefit of the Hadamard product is only shown experimentally on the VQA dataset.--------5. Related work: The comparison to the related works in the appendix should at least be mentioned in the main paper, even if the details are the supplemental.------------------------Minor--------- It is not clear why the Lu et al, 2015 is cited rather than the published paper from Antol et al.--------- Sect 2, first sentence: “every pairs” -> “every pair”------------------------Summary:--------While the paper provides a new best performance and an interesting interpretation of Hadamard product, to be a strong paper, either a more theoretical analysis of the properties of this approximation is required or a corresponding experimental evaluation. It is a bit unfortunate that most of the experimental evaluation is not about the main claim of the paper (the Hadamard product) but of unrelated aspects which are important to achieve high performance in the VQA challenge.----------------To be more convincing I would like to see the following experiments --------- Comparison with Outer product in the identical model--------- Comparison with MCB in the identical model--------- Comparison with elementwise sum instead of elementwise product--------- One of the most important hyper parameters for the Hadamard Product seems to be the dimension of the lower dimensional embedding d. What effect does changing this have?","The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR.",This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product) This formulation is evaluated on the visual question answering (VQA) task together,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product) This formulation is evaluated on the visual question answering (VQA) task together, This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product) This formulation is evaluated on the visual question answering (VQA) task together,"Novelty: The paper presents only a new “interpretation” of the Hadamard product which has previously been widely used for pooling, including for VQA.-----------2. Experimental evaluation:-------",0.14999999999999997,0.0,0.125,0.8045913577079773,0.8363639116287231,0.8201701045036316
https://openreview.net/forum?id=S1J0E-71l,"Summary:--------This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.----------------Contributions:--------The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.----------------Questions:--------A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.----------------Criticisms:--------The paper is really badly written, authors should rethink the organization of the paper.--------Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. --------The justification is not convincing enough.--------Experimental results are lacking, only results on a single dataset are provided.--------Although the authors claim that they got SOTA on enwiki8, there are other papers such as the HyperNetworks that got better results (1.34) than the result they achieve. This claim is wrong.--------The model requires the ground-truth labels for the test-set, however, this assumption really limits the application of this technique to a very limited set of applications(more or less rules out most conditional language modeling tasks).----------------High-level Review:--------    Pros: --------        - A simple modification of the model that seems to improve the results and it is an interesting modification.----------------    Cons:--------       - The authors need to use test-set labels.--------       - Writing of the paper is bad.--------       - The authors assume that they have access to the ground-truth labels during the test-set.--------       - Experimental results are lacking","Based on the feedback, I'm going to be rejecting the paper on the following grounds:  1. Results are not SOTA as reported.  2. No real experiments other than cursory experiments on Hutter prize data.  2. Writing is very poor.    However, just for playing devil's advocate, to the reviewers, I would like to point out that I am in agreement with the author that dynamic evaluation is not equivalent to this method. The weights are not changed in this model, as far as I can see, for the test set. Surprisal is just an extra input to the model. I think the reviewers were puzzled by the fact that at test time, the actual sequence needs to be known. While this may be problematic for generative modeling, I do not see why this would be a problem for language modeling, where the goal of the model is only to provide a log prob to evaluate how good a sequence of text is. Long before language modeling started being used to generate text, this was the main reason to use it - in speech recognition, spelling correction etc..",This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks., Authors have shown a result on language modeling tasks . Authors claim that they got SOTA on enwiki8 . Authors need to use test-set labels for the model .,:----------This paper proposes to use surprisal-driven feedback for training recurrent neural networks. Authors have shown a result on language modeling tasks.----------------Contributions,0.19469026548672566,0.044642857142857144,0.10619469026548672,0.8593553304672241,0.8133391737937927,0.8357143402099609
https://openreview.net/forum?id=S1J0E-71l,"This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. -----------------This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.-----------------The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other papers, but the manuscript should be updated before the final decision.-----------------The model size is still missing and without this information, it is hard to judge the contribution of the proposed trick.","Based on the feedback, I'm going to be rejecting the paper on the following grounds:  1. Results are not SOTA as reported.  2. No real experiments other than cursory experiments on Hutter prize data.  2. Writing is very poor.    However, just for playing devil's advocate, to the reviewers, I would like to point out that I am in agreement with the author that dynamic evaluation is not equivalent to this method. The weights are not changed in this model, as far as I can see, for the test set. Surprisal is just an extra input to the model. I think the reviewers were puzzled by the fact that at test time, the actual sequence needs to be known. While this may be problematic for generative modeling, I do not see why this would be a problem for language modeling, where the goal of the model is only to provide a log prob to evaluate how good a sequence of text is. Long before language modeling started being used to generate text, this was the main reason to use it - in speech recognition, spelling correction etc..",This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs., This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs .,". -----------------This paper makes an erroneous assumption: test label information is not given in most of the real world applications, except few applications. -----------------This paper proposes to use",0.16289592760180993,0.03652968036529681,0.09954751131221719,0.854399561882019,0.8098597526550293,0.831533670425415
https://openreview.net/forum?id=S1J0E-71l,"This paper proposes to leverage ""surprisal"" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.----------------The general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.----------------- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.----------------- The experimental evaluation is limited. Only one dataset enwik8 is explored. I think it is necessary to try the idea on different datasets to see if feedback LSTM sees some consistent improvements.--------Also, author claims state-of-art on enwik8, but hypernetwork, already cited in the paper, achieves better results (1.34 BPC, table 4 in the hypernetworks paper).----------------- Author only compares to methods that do not use last prediction error as extra signal. I would argue that a comparison with dynamic evaluation would be more fair. -------- Feedback LSTM uses prediction error as extra input in the forward prop, while dynamic evaluation  backprop it through the network and change the weight accordingly. Also they don't propagate the prediction error in the same way, they both leverage ""extra"" supervised information through the prediction errors.------------------------In summary:--------Pros: --------- Interesting idea--------- Seems to improve performances----------------Cons:--------- Paper writing--------- Weak evaluation (only one dataset)--------- Compare only with approaches that does not use the last-timestep error signal","Based on the feedback, I'm going to be rejecting the paper on the following grounds:  1. Results are not SOTA as reported.  2. No real experiments other than cursory experiments on Hutter prize data.  2. Writing is very poor.    However, just for playing devil's advocate, to the reviewers, I would like to point out that I am in agreement with the author that dynamic evaluation is not equivalent to this method. The weights are not changed in this model, as far as I can see, for the test set. Surprisal is just an extra input to the model. I think the reviewers were puzzled by the fact that at test time, the actual sequence needs to be known. While this may be problematic for generative modeling, I do not see why this would be a problem for language modeling, where the goal of the model is only to provide a log prob to evaluate how good a sequence of text is. Long before language modeling started being used to generate text, this was the main reason to use it - in speech recognition, spelling correction etc..","This paper proposes to leverage ""surprisal"" as top-down signal in RNN. More specifically, author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes to leverage ""surprisal"" as top-down signal in RNN. More specifically, author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM."," This paper proposes to leverage ""surprisal"" as top-down signal in RNN . More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM . The","to leverage ""surprisal"" as top-down signal in RNN. This paper proposes to leverage ""surprisal"" as top-down signal in RNN. More specifically author uses the error corresponding to the",0.1719457013574661,0.0273972602739726,0.11764705882352941,0.8441947102546692,0.8121722936630249,0.827873945236206
https://openreview.net/forum?id=BkCPyXm1l,"The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). ----------------Pros: --------+ Comprehensive analysis on the co-label similarity.----------------Cons:--------- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. --------- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.",The reviewers unanimously recommend rejection.,"The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the", The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction . This is probably not enough for ICLR .,. --------- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.,0.045454545454545456,0.0,0.045454545454545456,0.8024027943611145,0.8631160855293274,0.831652820110321
https://openreview.net/forum?id=BkCPyXm1l,"Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.----------------In order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.----------------The baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with augmentation. It is not clear why number of epochs are set to a small number like 100 without putting some convergence tests.. Therefore the significance of the method is not convincingly demonstrated in empirical study.----------------Co-label similarities could be calculated using softmax results at final layer rather than using predicted labels.  The advantage over dropout is not clear in Figure 4, the dropout is set to 0.2 without any cross-validation.  ------------------------Regularizing by enforcing the training steps to keep co-label similarities is interesting idea but not very novel and the results are not significant.----------------Pros : --------- provides an investigation of regularization on co-label similarity during training----------------Cons:---------The empirical results do not support the intuitive claims regarding proposed procedure--------Iterative version can be unstable in practice",The reviewers unanimously recommend rejection.,This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early, Paper proposes soft-target regularization that trains network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss . They claim that this prevents the disappearing of co-label similarity after early training and yields,. This paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss.-------------,0.041666666666666664,0.0,0.041666666666666664,0.8111125826835632,0.8771452903747559,0.8428375124931335
https://openreview.net/forum?id=BkCPyXm1l,"This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a ""roll-in"" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.----------------This is an incremental improvement on the idea of label softening/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening/temperature of the label distribution is gradually reduced (as one would expect for this method to do as the model gets closer and closer to reproducing the target distribution).----------------It's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful. The dropout baselines for MNIST seem quite far from results already in the literature (Srivastava et al 2014 achieves 1.06% with a 3x1024 MLP with dropout and a simple max norm constraint; the dropout baselines here fail to break 1.3% which is rather high by contemporary standards on the permutation-invariant task), and results for CIFAR10 are quite far from the current state of the art, making it difficult to judge the contribution in light of other innovations. The largest benchmark considered is SVHN where the reported accuracies are quite bad indeed; SOTA for single net performance has been less than half the reported error rates for 3-4 years now. It's unclear what conclusions can be drawn about how this would help (or even hurt) in a better-tuned setting.----------------I have remaining reservations about data hygiene, namely reporting minimum test loss/maximum test accuracy rather than an unbiased method for model selection (minimum validation set error, for example). Relatedly, the regularization potential of early stopping on a validation set is not considered. See, e.g. the protocol in Goodfellow et al (2013).",The reviewers unanimously recommend rejection.,This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. It's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. It's an interesting and somewhat appealing idea but the case is not clearly made that this is all that useful., This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself . It's an interesting idea but the case is not clearly made that this is all that useful .,", but this is an incremental improvement on the idea of label softening/smoothing that has recently been revived.----------------This is an incremental improvement on the idea of label softening/",0.04444444444444445,0.0,0.04444444444444445,0.8225005865097046,0.8699073791503906,0.8455400466918945
https://openreview.net/forum?id=B1jnyXXJx,"Summary:--------This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be pushed away from saddle points due to the saddle point being positively changed as well. Authors of the paper show results over several different datasets.----------------Overview of the Review:--------    Pros:--------        - The idea is very interesting.--------        - The diverse set of results on different datasets.--------    Cons:--------        - The justification is not strong enough.--------        - The paper is not well-written.--------        - Experiments are not convincing enough.----------------Criticisms:----------------I liked the idea and the intuitions coming from the paper. However, I think this paper is not written well. There are some variables introduced in the paper and not explained good-enough, for example in 2.3, the authors start to talk about p without introducing and defining it properly. The only other place it appears before is Equation 6. The Equations need some work as well, some work is needed in terms of improving the flow of the paper, e.g., introducing all the variables properly before using them.----------------Equation 6 appears without a proper explanation and justification. It is necessary to explain it what it means properly since I think this is one of the most important equation in this paper. More analysis on what it means in terms of optimization point of view would also be appreciated.---------------- is not a parameter, it is a function which has its own hyper-parameter . ----------------It would be interesting to report validation or test results on a few tasks as well. Since this method introduced as an additional cost function, its effect on the validation/test results would be interesting as well.--------The authors should discuss more on how they choose the hyper-parameters of their models. ----------------The Figure 2 and 3 does not add too much to the paper and they are very difficult to understand or draw any conclusions from. ----------------There are lots of Figures under 3.4.2 without any labels of captions. Some of them are really small and difficult to understand since the labels on the figures appear very small and somewhat unreadable.------------------------A small question:----------------* Do you also backpropagate through --------?","The paper proposes a method for accelerating optimization near saddle points when training deep neural networks. The idea is to repel the current parameter vector from a running average of recent parameter values. The proposed method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.    The author presents a fresh idea in the area of stochastic optimization for deep neural networks. However the paper doesn't quite appear to be above the Accept bar, due to remaining doubts about the thoroughness of the experiments.We therefore invite this paper for presentation at the Workshop track.","This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle over the error surface.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle over the error surface."," The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be pushed away from saddle points due to the saddle point being positively changed as well . Criticisms","is a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be",0.2535211267605634,0.08571428571428572,0.18309859154929578,0.8614615201950073,0.8434329628944397,0.8523519039154053
https://openreview.net/forum?id=B1jnyXXJx,"This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.----------------On the surface, the proposed method seems extremely close to momentum. It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point. The illustration of better convergence on the toy saddle example is not what I mean here—optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from—and clearly qualitatively better than—momentum.----------------Another way of getting at the relationship to momentum would be to try to find a form for R_t(f) that yields the exact momentum update. You could then compare this with the R_t(f) used in CPN.----------------The overly general notation  etc should be dropped—Eqn 8 is enough.----------------The theoretical results (Eqn 1 and Thm 1) should be removed, they are irrelevant until the joint density can be specified.----------------Experimentally, it would be valuable to standardize the results to allow comparison to other methods. For instance, recreating Figure 4 of Dauphin et al, but engaging the CPN method rather than SFN, would clearly demonstrate that CPN can escape something that momentum cannot.----------------I think the idea here is potentially very valuable, but needs more rigorous comparison and a clear relation to momentum and other work.","The paper proposes a method for accelerating optimization near saddle points when training deep neural networks. The idea is to repel the current parameter vector from a running average of recent parameter values. The proposed method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.    The author presents a fresh idea in the area of stochastic optimization for deep neural networks. However the paper doesn't quite appear to be above the Accept bar, due to remaining doubts about the thoroughness of the experiments.We therefore invite this paper for presentation at the Workshop track.",This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average. This method is shown to optimize faster than a variety of other methods.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average. This method is shown to optimize faster than a variety of other methods., This paper proposes a novel method for accelerating optimization near saddle points . The basic idea is to repel the current parameter vector from a running average of recent parameter values . This method is shown to optimize faster than a variety of other methods in,". It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point.-------------I think the idea here is potentially very valuable,",0.5034965034965035,0.43971631205673756,0.48951048951048953,0.9635979533195496,0.8839557766914368,0.9220603108406067
https://openreview.net/forum?id=B1jnyXXJx,"The research direction taken by this paper is of great interest. --------However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). --------""Throughout this paper the selection of hyper-parameters was kept rather simple."" but the momentum term of CPN is set to 0.95 --------and not 0.9 as in all/most optimizers CPN is compared to. I suppose that the positive effect of CPN (if any) is mostly due to its momentum term.","The paper proposes a method for accelerating optimization near saddle points when training deep neural networks. The idea is to repel the current parameter vector from a running average of recent parameter values. The proposed method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.    The author presents a fresh idea in the area of stochastic optimization for deep neural networks. However the paper doesn't quite appear to be above the Accept bar, due to remaining doubts about the thoroughness of the experiments.We therefore invite this paper for presentation at the Workshop track.",The research direction taken by this paper is of great interest. The empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6).,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The research direction taken by this paper is of great interest. The empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6).," The research direction taken by this paper is of great interest . However, the empirical results are not great enough to pay for the weaknesses of the proposed approach .","(see Section 6). --------The empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). -------- ""Throughout this paper the selection of hyper-parameters was kept rather simple",0.1791044776119403,0.045454545454545456,0.14925373134328357,0.8538229465484619,0.8394830226898193,0.8465922474861145
https://openreview.net/forum?id=S1vyujVye,"This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. ----------------Strengths:----------------- The training objective is reasonable. In particular, high-level features show translation invariance. ----------------- The proposed methods are effective for initializing neural networks for supervised training on several datasets. ------------------------Weaknesses:----------------- The methods are technically similar to the “exemplar network” (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). ----------------- The paper is experimentally misleading.--------The results reported in this paper are based on fine-tuning the whole network with supervision. However, in Table 2, the results of exemplar convnets (Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). Therefore, the comparison is not fair. I suspect that exemplar convnets (Dosovitskiy 2015) would achieve similar improvements from fine-tuning; so, without such comparisons (head-to-head comparison with and without fine-tuning based on the same architecture except for the loss), the experimental results are not fully convincing. ----------------Regarding the comparison to “What-where” autoencoder (Zhao et al, 2015), it will be interesting to compare against it in large-scale settings, as shown by Zhang et al, ICML 2016 (Augmenting Supervised Neural Networks with Unsupervised Objectives for Large-Scale Image Classification). Training an AlexNet is not very time-consuming with latest (e.g., TITAN-X level) GPUs. ----------------The proposed method seems useful only for natural images where different patches from the same image can be similar to each other. ","The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.","This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images."," This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks . It will be interesting to compare against “What-where’s autoencoder (Zhao et al,","(Dosovitskiy 2015) is from unsupervised feature learning (the network is not finetuned with labeled samples, and only a classifier is trained upon the features). ---------------",0.34615384615384615,0.1176470588235294,0.25,0.8956931233406067,0.8365604877471924,0.8651175498962402
https://openreview.net/forum?id=S1vyujVye,"The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches, and any other important measures taken to avoid trivial solutions. ","The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.","The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that"," The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image . The loss is very similar in spirit to that","of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. Intuitively, it seems that the self-supervised problem posed by this method is",0.30476190476190473,0.058252427184466014,0.24761904761904757,0.8587530851364136,0.8303145170211792,0.8442944288253784
https://openreview.net/forum?id=S1vyujVye,"This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:------------------------The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (“probability” can be replaced with another word).----------------I would like to know more about how the method is using the “batch statistics” (end of Section 4.2) by sampling from it, unless the authors simply mean that the just sample from all the possible triples in their batch.----------------Shouldn’t the number of patches sampled in Algorithm 1 be a hyper-parameter rather than just be 1? Have the authors tried any other value?----------------I think there are some missing details in the paper, like the patch size or whether the authors have played with it at all (I think this is an important hyper-parameter).----------------The STL results are quite impressive, but CIFAR-10 maybe not so much. For CIFAR I’d expect that one can try to pre-train on, say, Imagenet + CIFAR to build a better representation. Have the authors considered this?------------------------All in all, this is an interesting piece of work with some obvious applications, and it seems relatively straightforward to implemenent and try. I think I would’ve liked more understanding of what the spatial contrasting actually learns, more empirical studies on the effects of various parameter choices (e.g., patch size) and more attempts at beating the state of the art (e.g. CIFAR).","The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution.","This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches"," This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting . The method is that of ‘spatial constrasting’, i.e. of building triplets from patches","is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image.",0.23913043478260868,0.044444444444444446,0.15217391304347824,0.8790784478187561,0.8321364521980286,0.8549636006355286
https://openreview.net/forum?id=S1HEBe_Jl,"The submission proposes to modify the typical GAN architecture slightly to include ""encrypt"" (Alice) and ""decrypt"" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:--------""As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.""----------------The idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption.  In this way, some of the data can remain unencrypted, while the portion that is e.g. correlated with the encrypted signal will have to be encrypted in order for Eve to not be able to predict the encrypted part.----------------While this is a nice thought experiment, there are significant barriers to this submission having a practical impact:--------1) GANs, and from the convergence figures also the objective considered here, are quite unstable to optimize.  The only guarantees of privacy are for an Eve that is converged to a very strong adversary (stronger than a dedicated attack over time).  I do not see how one can have any sort of reliable guarantee of the safety of the data transmission from the proposed approach, at least the paper does not outline such a guarantee.--------2) Public key encryption systems are readily available, computationally feasible, and successfully applied almost anywhere.  The toy examples given in the paper do not at all convince me that this is solving a real-world problem at this point.  Perhaps a good example will come up in the near future, and this work will be shown to be justified, but until such an example is shown, the approach is more of an interesting thought experiment.",Interesting paper but not over the accept bar.,"The submission proposes to modify the typical GAN architecture. The modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption. While this is a nice thought experiment, there are significant barriers",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The submission proposes to modify the typical GAN architecture. The modules can learn to encrypt and decrypt a signal, while an adversary is simultaneously learned that tries to break the encryption. While this is a nice thought experiment, there are significant barriers"," The submission proposes to modify the typical GAN architecture slightly to include ""encrypt"" (Alice) and ""decrypt"" modules as well as a module trying to decrypt the signal without a key (Eve) The idea is cute",":----------------""An Eve that is converged to a very strong adversary (stronger than a dedicated attack over time) The submission proposes to modify the typical GAN architecture slightly.",0.04,0.0,0.04,0.8269970417022705,0.8314871191978455,0.8292359709739685
https://openreview.net/forum?id=S1HEBe_Jl,"The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.----------------The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is described in section 3, however, is that D and D-public are both reconstructed by Bob, but why would Bob reconstruct the latter (he is not public, in particular because he is allowed to reconstruct C, which is not tested here)? Also, Eve only tries to estimate C, thus rendering the scenario not different in any way to the scenario considered in section 2.----------------I have two more minor concerns:----------------1) As raised in the pre-review, Eve should actually be stronger then Alice and Bob in order to be able to compensate for the missing key. The authors noted they have been doing these experiments and are going to add the results.----------------2) In any natural encryption case I would expect the length of the key to be much shorter then the length of the message. This, however, could potentially make the scenario much easier for Eve (although I doubt any of the results will change if the key is long enough).----------------I like the creative application of adversarial training to a completely different domain, and I believe it could be the starting point of a very interesting direction in cryptographic systems or in privacy applications (although it is unclear whether the weak guarantees of neural network based approaches can ever be overcome). At the same time the application in the privacy setting leaves me quite confused, and the symmetric encryption example is not particularly strong either. I'd appreciate if the authors could address the major concern I raised above, and I will be quite happy to raise the score in case this confusion can be resolved.",Interesting paper but not over the accept bar.,"The paper deals with an interesting application of adversarial training to encryption. Experiments are performed in a simple symmetric 16 bit encryption task. The concepts, ideas and previous literature are quite nicely and carefully presented.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper deals with an interesting application of adversarial training to encryption. Experiments are performed in a simple symmetric 16 bit encryption task. The concepts, ideas and previous literature are quite nicely and carefully presented."," The paper deals with an interesting application of adversarial training to encryption . Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy .",": I want to give the public the value of D (e.g. movies watched) without gaining information about C (e.g. gender). In any natural encryption case, Eve would need to reconstruct D as good as possible",0.1395348837209302,0.0,0.09302325581395347,0.816691517829895,0.8245888948440552,0.8206211924552917
https://openreview.net/forum?id=S1HEBe_Jl,"This paper proposed to use GAN for encrypted communications.----------------In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.----------------In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text.",Interesting paper but not over the accept bar.,This paper proposed to use GAN for encrypted communications. I'm having trouble thinking of an application where this system is better than simply decorrelating the data.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposed to use GAN for encrypted communications. I'm having trouble thinking of an application where this system is better than simply decorrelating the data., This paper proposed to use GAN for encrypted communications . It is strictly worse than any provable cryptography system .,".----------------In section 2, the authors proposed a 3 part neural network trained to encode and decode data.------------In section 2, the authors proposed a 3 part neural network trained to encode and de",0.11428571428571428,0.0,0.11428571428571428,0.829319417476654,0.8379356861114502,0.8336052298545837
https://openreview.net/forum?id=BJO-BuT1g,"This paper addresses the problem of efficient neural stylization.  Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set “conditional instance normalization” parameters dependent on the desired output style.  The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network.  Qualitative results are shown.----------------I have not worked in this area, but I’m generally aware of the main issues in transferring artistic style.  The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits.  As far as I can tell the results look compelling.  As I’m less confident in my expertise in this area, I’m happy to support another reviewer who is willing to champion this paper.----------------My main comments are on the paper writing.  As far as I understand, the main novelty of the approach starts in Section 2.2, and before that is review of prior art.  If this is indeed the case, one suggestion is to remove the subsection heading for 2.1 so it’s grouped with the first part of Section 2, and to cite related work for the feedforward network (e.g., Johnson et al.) in the text and in Fig 2 so it’s clear.  In fact, I’m wondering if Figs 2 and 3 can be combined somehow so that the contribution is clearer in the figures.  ----------------I was at first confused by Eq (5) as x and z are not defined anywhere.  Also, it may be helpful to write out everything explicitly as is done in the instance normalization paper.  ----------------In Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles.  ----------------Fig 5 left: I’m assuming the different colors correspond to the different styles.  If so, perhaps mention this in the caption.  Also, this figure is hard to read.  Maybe instead show single curves with error bars that are averages over the loss curves for N-styles and individual styles.----------------Typos:--------Page 1: Shouldn’t it be “VGG-16” network (not ""VGG-19”)?--------Page 2: “newtork” => “network”.--------Paragraph after Eq. (5): “much less” => “fewer”.",The reviewers (two of whom stated maximum confidence) are in consensus that this is a high-quality paper. It also attracted some public feedback which was also positive. The authors have already incorporated much of the feedback into their revised paper. This seems to be a clear accept in my opinion.,"This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller"," This paper addresses the problem of efficient neural stylization . Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller","in Eq (4), perhaps you could write T_s to emphasize that there are separate networks for different styles.----------Page 1: Shouldn’t it be “VGG-16” network (not """,0.24444444444444446,0.0,0.17777777777777776,0.8178123235702515,0.8307342529296875,0.8242226243019104
https://openreview.net/forum?id=BJO-BuT1g,"The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. ----------------This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.  ----------------Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different style images in the iterative style transfer algorithm by Gatys et al. and comes with similar limitations: Right now the parameters of the style feature space are hard to interpret and therefore there is little control over the stylisation outcome when moving in that feature space.--------Here I see the most potential for improvement of the paper: The parameterisation of style in terms of scale and shift parameters of individual features seems like a promising basis to achieve interpretable style features. It would be a great addition to explore to what extend statements such as “The parameters of neuron N in layer L encodes e.g. the colour or brush-strokes of the styles” can be made. I agree that this is a potentially laborious endeavour, but even just qualitative statements of this kind that are demonstrated with the respective manipulations in the stylisation would be very interesting.----------------In conclusion, this is a good paper presenting an elegant and valuable contribution that will have considerable impact on the design of feed-forward stylisation networks.",The reviewers (two of whom stated maximum confidence) are in consensus that this is a high-quality paper. It also attracted some public feedback which was also positive. The authors have already incorporated much of the feedback into their revised paper. This seems to be a clear accept in my opinion.,"The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network."," The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles . This is achieved by a global, style-dependent scale and shift parameter for each feature in the network .",is a promising basis to achieve interpretable style features.------------This paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by ,0.22222222222222224,0.022727272727272724,0.13333333333333333,0.8267213702201843,0.8372103571891785,0.831932783126831
https://openreview.net/forum?id=BJO-BuT1g,"CONTRIBUTIONS--------The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results.----------------NOVELTY--------The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer is also novel. This paper is also the first that demonstrates feedforward neural style blending; though not described in published literature, optimization-based neural style blending had previously been demonstrated in https://github.com/jcjohnson/neural-style.----------------MISSING CITATION--------The following paper was concurrent with Ulyanov et al (2016a) and Johnson et al in demonstrating feedforward neural style transfer, though it did not use the Gram-based formulation of Gatys et al:----------------Li and Wand, ""Precomputed Real-Time Texture Synthesis with Markovian Generative Adversarial Networks"", ECCV 2016----------------CLARITY--------The paper is very well written and easy to follow.----------------SIGNIFICANCE--------Though simple, the proposed method is a significant addition to the growing field of neural style transfer. Its benefits are especially clear for mobile applications, which are often constrained in both disk space and bandwidth. Using the proposed method, only a single trained network needs to be transmitted and stored on the mobile device; in addition the ability of the proposed method to incrementally learn new styles means that new styles can be added by transmitting only a small number of new style-specific parameters to a mobile device.----------------EVALUATION--------Like many other papers on neural style transfer, the results are mostly qualitative. Following existing literature, the authors use style and content loss as a quantitative measure of quality, but these metrics are unfortunately not always well-correlated with the perceptual quality of the results. I find the results of this paper convincing, but I wish that this and other papers on this topic could find a way to evaluate their results more quantitatively.----------------SUMMARY--------The problem and method are slightly incremental, but the several improvements over prior work make this paper a significant addition to the growing literature on neural style transfer. The paper is well-written and its experiments convincingly validate the benefits of the method. Overall I believe the paper would be a valuable addition to the conference.----------------Pros--------- Simple modification to feedforward neural style transfer with several improvements over prior work--------- Strong qualitative results--------- Well-written--------- Open-source code has already been released----------------Cons--------- Slightly incremental--------- Somewhat lacking in quantitative evaluation, but not any more so than prior work on this topic",The reviewers (two of whom stated maximum confidence) are in consensus that this is a high-quality paper. It also attracted some public feedback which was also positive. The authors have already incorporated much of the feedback into their revised paper. This seems to be a clear accept in my opinion.,The paper is the first to my knowledge that uses a single network to apply different styles to input images. The proposed conditional instance normalization layer is also novel. The paper is well-written and its experiments convincingly validate the benefits of,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper is the first to my knowledge that uses a single network to apply different styles to input images. The proposed conditional instance normalization layer is also novel. The paper is well-written and its experiments convincingly validate the benefits of, Authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images . Open-source code and pretrained models have been released,-------------SIGNIFICANCE-----------The paper is very well-written and its experiments convincingly validate the benefits of the method. The paper is well-written and its experiments convincingly validate,0.2580645161290323,0.0,0.12903225806451615,0.8487371802330017,0.8521538972854614,0.850442111492157
https://openreview.net/forum?id=rkpACe1lx,"A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics.  For example, recognizing animals will likely require different features than categorizing flowers.  Using different parameters for different types of examples has the potential to greatly reduce underfitting.  This can be seen in recent results with generative models, where image quality is much better for less diverse datasets.  However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network).  ----------------The hypernetworks paper cleverly proposes to get around this problem by adapting different ""parameters"" for different time steps in recurrent networks and different.  The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so there is no computational issue involved with using different ""parameters"".  In this paper, the ""parameters"" are modified for different positions based on the output from a hypernetwork which conditions on the time step.  Hypothetically, this hypernetwork could also condition on other features that are shared by all sequences in the minibatch.  ----------------I expect this method to become standard for training RNNs, especially where the length of the sequences is the same during the training and testing phases.  Penn Treebank is a highly competitive baseline, so the SOTA result reported here is impressive.  The experiments on convolutional networks are less experimentally impressive.  I suspect that the authors were aiming to achieve state of the art results here but settled with achieving a reduction in the number of parameters.  It might even be worthwhile to consider a synthetic experiment where two completely different types of image are appended (i.e. birds on the left and flowers on the right) and show that the hypernetwork helps in this situation.  It may be the case that for convnets, the cases where hypernetworks help are very specific.  ----------------For RNNs, it seems to be the case that explicitly changing the nature of the computation depending on the position in the sequence greatly improves generalization.  While a usual RNN could learn to store a counter (indicating the position in the sequence), the hypernetwork could be a more efficient way to add capacity.  ----------------Applications to time series forecasting and modeling could be an interesting area for future work.  ","The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2).",Using different parameters for different types of examples has the potential to greatly reduce underfitting. I expect this method to become standard for training RNNs. The experiments on convolutional networks are less experimentally impressive.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Using different parameters for different types of examples has the potential to greatly reduce underfitting. I expect this method to become standard for training RNNs. The experiments on convolutional networks are less experimentally impressive.," Using different parameters for different types of examples has the potential to greatly reduce underfitting . This can be seen in recent results with generative models, where image quality is much better for less diverse datasets .","in recurrent networks and different. ---------------The hypernetworks paper cleverly proposes to get around this problem by adapting different ""parameters"" for different time steps in recurrent networks and different",0.10126582278481014,0.0,0.0759493670886076,0.8410355448722839,0.8299212455749512,0.8354414105415344
https://openreview.net/forum?id=rkpACe1lx,"Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.--------Unfortunately, as the results show, the authors could not get better results with less parameters.--------However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.----------------The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.--------E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.----------------Could the authors provide the num. of trainable parameters for Table 6?----------------Probably presenting less results could also improve the readability.--------Only marginal accept due to the writing style.","The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2).","Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way. The paper should be reorganized, and shortened.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way. The paper should be reorganized, and shortened."," Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way .","e.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on",0.2222222222222222,0.028571428571428574,0.1111111111111111,0.8548480272293091,0.8387390375137329,0.8467169404029846
https://openreview.net/forum?id=rkpACe1lx,"This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    ------------------pros----------------This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.------------------cons----------------The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters. However, in the running time, the computational complexity is the same as the standard main network for static networks, such as ConvNet, and the computational cost is even larger for dynamic networks such as LSTMs. The improvements of hyperLSTMs over conventional LSTM and its variants seem mainly come from increasing the number of model parameters.------------------minor question,---------------- The ConvNet and LSTM used in the experiments do not have a large softmax layer. For most of the word-level tasks for either LM or MT, the softmax layer could be more than 100K. Is it going to be challenging for the hyperNetwork generate large number of weights for that case, and is it going to slowing the training down significantly?","The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2).","The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and"," The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task . In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and","and its variants on a couple of LM talks, which is very inspiring.---------------This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is",0.14285714285714288,0.0,0.14285714285714288,0.8352797627449036,0.8323378562927246,0.8338062167167664
https://openreview.net/forum?id=rkpACe1lx,"*** Paper Summary ***----------------The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.----------------*** Review Summary ***----------------Pros: --------- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. --------- LM and MT results are excellent.----------------Cons:  --------- The paper could be better written. It is too long for the conference format and need refocussing. --------- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way it is better to say that something is revisiting an old idea or training it at modern scale/on modern tasks than ommitting it).--------- on focus, it is not clear if your goal is to achieve better performance or more compact networks. In the RNN section you lean toward the former, in the CNN section you seem to lean toward the latter.----------------I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. The relation with multiplicative/order 2 networks and eventual differences need to be explained.----------------*** Detailed Review ***----------------Multiplicative networks are an extremely powerfull architecture and bringing them back into the spotlight is excellent. This paper has excellent results but suffer poor presentation, lack of a clear focus. It spends time on details and ommit important points. In its current form, it is much too long to long and his not self contained without the appendices.----------------Spending more time on multiplicative RNNs, order 2 networks at the begining of the paper would be excellent. This will let you highlight the difference between this paper and earlier work. It would also be necessary to spend a little time on why multiplicative RNN were less used than gated RNN: it seems that the optimization problem their training involve is tricker and it would be helpful to explain whether you had a harder time tweaking optimization parameters or whether you needed longer training sessions compared to LSTMs, regular CNN. On name, I am not sure that ""hypernetwork"" help the reader understand better what the proposed architecture compared to multiplicative interactions.----------------In section 3.2, you seem to imply that there are different settings of hypernetworks that allow to vary from an RNN to a CNN, this is not clear to me, maybe you could show how this would work on a simple temporal problem with equations. ----------------The work on CNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights, showing that similar performance can be achieved with less weights. It is not clear to me why to pursue that goal. Do you expect speedups? less memory for embedded applications? In that case you should compare with alternative strategies, e.g. model compression (Caruana et al 2006, aka Dark Knowledge, Hinton et al 2014) or hashed networks (Chen et al 2015). ----------------For RNN, you seem to target better perplexity/BLEU and model compactness is not a priority. Instead of making the weights have a simpler structure, you make them richer, i.e. dependent over time. It seems in that case models might be bigger and take longer to train. You might want to comment on training time, inference time, memory requirement in that case, as you highlight it might be an important goal in the CNN section. Overall, I am not sure it helps to have this mixed message. I would rather see the paper fit in the conference format with the RNN results alone and a clearer explanation and defers the publications of the CNN results when a proper comparison with memory concerned methods is performed.----------------Some of the discussions are not clear to me, I am not sure what message the reader should get from Figure 2 or from the discussion on saturation statistics (p10, Figure 5). Similarly, I am not sure if Figure 4 is showing anything: everything should change more drastically at word boundaries even in a regular LSTM (states, gates units should look very different before/after a space); without such a comparison it is hard to see if this is unique to your network.----------------The results on handwriting generation are harder to compare for me. Log-loss are hard to understand, I have no sense whether the difference between models is significant (what would be the variance in this metric under boostrap sampling of the training set?). I am not sold either on qualitative metric were human can assess quality but human cannot evaluate if the network is repeating the training set. Did you thing at precision/recall metric for ink, possibly with some spatial tolerance ? (e.g. evaluation of segmentation tasks in vision).----------------The MT experiments are insufficiently discussed in the main text.----------------Overall, I would suggest to make the paper shorter and clearer possibly leaving the CNN results for latter publication. You need to properly discuss the relation to multiplicative/order 2 networks and highlight the differences. Unclear discussion can be eliminated to make the experimental setup and the results presentation clearer in the main text.----------------*** References ***----------------M.W. Goudreau, C.L. Giles, S.T. Chakradhar, D. Chen, ""First-Order Vs. Second-Order Single Layer Recurrent Neural Networks,""IEEE Trans. on Neural Networks, 5 (3), p. 511, 1994.----------------Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil, ""Model Compression,"" The Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2006), August 2006, pp. 535-541.----------------Dark knowledge, G Hinton, O Vinyals, J Dean 2014----------------W. Chen, J. Wilson, S. Tyree, K. Weinberger and Y. Chen, Compressing Neural Networks with the Hashing Trick, Proc. International Conference on Machine Learning (ICML-15)","The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2).",The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from, The paper proposes to a new neural network architecture . The layer weights of a classical network are computed as a function of a latent representation associated with the layer . The results on handwriting generation are harder to compare for me .,".-------------------The work on RNN and RNN are rather disconnected to me: for CNN, you seem to be interested in a low rank structure of the weights.----------------",0.2272727272727273,0.023255813953488372,0.20454545454545459,0.8292491436004639,0.8331819176673889,0.8312109112739563
https://openreview.net/forum?id=rkGabzZgl,"summary----------------The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.----------------The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).--------This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.----------------Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.----------------Experiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)------------------------The study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.----------------The framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.----------------The proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.----------------p6 line 8 typo: expecatation","This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper ""Variational Dropout and the Local Reparameterization Trick"" by Diederik P. Kingma, Tim Salimans, Max Welling.",The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to, Paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised . Maximum likelihood under this model is not tractable but standard drop,a latent variable model where standard dropout is then a monte carlo approximation.-------------------The paper explains dropout with a latent variable model where standard dropout,0.18000000000000002,0.0,0.14,0.8400272727012634,0.8183335661888123,0.8290385007858276
https://openreview.net/forum?id=rkGabzZgl,"This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout “inference gap” which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.----------------One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I’d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.----------------MC dropout on page 8 is not defined, please define.----------------On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?----------------From Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.----------------Couple of typos:--------- Pg. 2 “ … x is he input …” -> “ … x is the input …”--------- Pg. 5 “ … as defined in (1), is …” -> ref. to (1) is not right at two places in this paragraph----------------Overall it is a good paper, I think should be accepted and discussed at the conference.","This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper ""Variational Dropout and the Local Reparameterization Trick"" by Diederik P. Kingma, Tim Salimans, Max Welling.",This paper introduces dropout as a latent variable model (LVM) Authors analyze the dropout “inference gap” which they define to be the gap between network output during training and test. They propose use of per-,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper introduces dropout as a latent variable model (LVM) Authors analyze the dropout “inference gap” which they define to be the gap between network output during training and test. They propose use of per-, This paper introduces dropout as a latent variable model (LVM) Authors analyze the dropout “inference gap’s gap between network output during training and test . They propose use of per-sample based inference gap as,"MC dropout is not defined, please define.-------------------On page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.",0.23157894736842102,0.02150537634408602,0.16842105263157894,0.8415886163711548,0.8151172995567322,0.8281415104866028
https://openreview.net/forum?id=rkGabzZgl,"This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.----------------Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don’t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of “Dropout as a bayesian approximation” and the insights and theorems in this paper might enable future work in that direction.-------- ","This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper ""Variational Dropout and the Local Reparameterization Trick"" by Diederik P. Kingma, Tim Salimans, Max Welling.","This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing drop out regularized networks",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing drop out regularized networks"," This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization . Authors derive useful theorems that estimate or put bounds on key quantities that are of interest . They introduce an explicit regularization term",.----------------The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective.----------------,0.21999999999999997,0.04081632653061224,0.16,0.85408616065979,0.8263300657272339,0.8399788737297058
https://openreview.net/forum?id=HJ0NvFzxl,"The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.----------------This idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. ","The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable.",The main contribution of this paper seems to be an introduction of a set of differential graph transformations. This will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The main contribution of this paper seems to be an introduction of a set of differential graph transformations. This will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented, The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent . This maps naturally to a task of learning a cellular automaton represented as,", with nodes pointing to neighbors and special nodes 0/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved",0.1935483870967742,0.0,0.11290322580645161,0.8577463030815125,0.8404821753501892,0.8490265011787415
https://openreview.net/forum?id=HJ0NvFzxl,"This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.----------------The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has more learning or generalization capacity than other approaches. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction (see http://www.public.asu.edu/~cbaral/papers/aaai2016-sub.pdf). This is still clearly promising.---------------- The sequence of transformation in algorithm 1 looks sensible, though the authors do not discuss any other operation ordering. In particular, it is not clear to me that you need the node state update step T_h if you have the direct reference update step T_h,direct. ----------------It is striking that the only trick that is essential for proper performance is the ‘direct reference’ , which actually has nothing to do with the graph building process, but is rather an attention mechanism for the graph input: attention is focused on words that are relevant to the node type rather than the whole sentence. So the question “how useful are all these graph operations” remain. A much simpler version of a similar trick may have been proposed in the context of memory networks, also for ICLR'17 (see match type in ""LEARNING END-TO-END GOAL-ORIENTED DIALOG"" by Bordes et al)------------------------The authors also mention the time and size needed to train the model: is the issue arising for learning, inference or both? A description of the actual implementation would help  (no pointer to open source code is provide). The author mentions Theano in one of my questions: how are the transformations compiled in advance as units? How is the gradient back-propagated through the graph is this one is only described at runtime?------------------------Typo: in the appendices B.2 and B.2.1, the right side of the equation that applies the update gate has h’_nu while it should be h_nu.----------------In the references, the author could mention the pioneering work  of Lee Giles on representing graphs with  RNNs.----------------Revision: I have improved my rating for the following reasons:--------- Pointers to an highly readable and well structured Theano source is provided.--------- The delta improvement of the paper has been impressive over the review process, and I am confident this will be an impactful paper.--------- Much simpler alternatives approaches such as Memory Networks seem to be plateauing for problems such as dialog modeling, we need alternatives.--------- The architecture is this work is still too complex, but this is often as we start with DNNs, and then find simplifications that actually improve performance","The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable.","This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory) Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory) Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences"," This paper proposes learning on the fly to represent a dialog as a graph . Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph .",.------------The author proposes learning on the fly to represent a dialog as a graph (which acts as the memory) and is first demonstrated on the bAbI tasks.---------------The,0.28800000000000003,0.032520325203252036,0.176,0.8493471741676331,0.8434386849403381,0.8463826179504395
https://openreview.net/forum?id=HJ0NvFzxl,"The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.----------------The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus easily learnable through gradient-descent techniques. It can be seen as a succesfull attempt to mix continuous and symbolic representations. It moreover seems more general that the recent attempts made to add some 'symbolic' stuffs in differentiable models (Memory networks, NTM, etc...) since the shape of the state is not fixed here and can evolve. My main concerns is about the way the model is trained i.e by providing the state of the graph at each timestep which can be done for particular tasks (e.g Babi) only, and cannot be the solution for more complex problems. My other concern is about the whole content of the paper that would perhaps best fit a journal format and not a conference format, making the article still difficult to read due to its density. ","The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable.",The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. My main concerns is about the way the model is trained i.e by providing the state of the graph,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. My main concerns is about the way the model is trained i.e by providing the state of the graph, The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations . The underlying idea is to propose a method that will be able build/modify a graph-structure,. The author proposes 5 different possible differentiable transformations that will be learned on a training set. The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability,0.272,0.016260162601626018,0.176,0.8586239814758301,0.8456317782402039,0.8520783185958862
https://openreview.net/forum?id=HkNKFiGex,"The paper presents two main contributions:----------------(1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software.----------------(2) A hybridization of GANs and VAEs called Introspective Adversarial Network.----------------The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together.----------------On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models.----------------On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to assign credit to individual ideas when they are presented in a ""list of things to make it work"" fashion. I would like to see more empirical results in that direction to help clear up things.----------------Overall I think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story I think it is not yet ready for publication.----------------UPDATE: The rating has been updated to a 6 following the authors' reply.","Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster.",The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush. The main problem I have with the paper is that it feels very much like two papers in one with a,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush. The main problem I have with the paper is that it feels very much like two papers in one with a, The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush . A hybridization of GANs and VAEs called Introspective Adversarial Network .,"the paper presents two main contributions:----------------(1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software.----------------",0.2146341463414634,0.04926108374384237,0.1463414634146341,0.8467078804969788,0.7887583374977112,0.8167064189910889
https://openreview.net/forum?id=HkNKFiGex,"UPDATE: The rewritten paper is more focused and precise than the previous version. The ablation studies and improved evaluation of the IAN model help to the make clear the relative contributions of the proposed MDC and orthogonal regularization. Though the paper is much improved, in my opinion there is still too much emphasis on the photo-editing interface. In addition, the MDC blocks are used in the generator of the model but their efficacy is measured via discriminative experiments. All-in-all I am updating my score to a 5.----------------==========----------------This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data, and the discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.----------------Pros:--------+ The IAN model itself is interesting as standard GAN-based approaches do not simultaneously train an autoencoder.----------------Cons:--------- The writing is unclear at times and the mathematical formulation of the IAN is not very precise.--------- Many different ideas are proposed in the paper without sufficient empirical validation to characterize their individual contributions.--------- The photo editing interface, though interesting, is probably better suited for a conference with more of an HCI focus.----------------* Section 2: The gradient descent step procedure seems to be quite similar to the approach proposed by [2]. More elaboration on the differences would be helpful.--------* Section 3: It is unclear whether the discriminator has three binary outputs or if there is a softmax over the three possible labels. The paper does not mention whether L_G and L_D are minimized or maximized. Presumably they are both minimized, but in that case it is counter-intuitive that the generator attempts to decrease the probability that the discriminator assigns the ""real"" label to the generated samples and reconstructions. In addition, in the minimization scenario L_D maximizes the probability of the correct labels being assigned to X_gen and \hat{X} but minimizes the probability of the ""real"" label being assigned to X.--------* Section 3.2: It is odd that not training MADE leads to better performance, as training MADE should lead to a better variational approximation to the true posterior. More exploration seems warranted here.--------* Section 3.3.2: One drawback of autoregressive approaches is that sampling is slow. How do you reconcile this with its use in an interactive application, where speed is important?--------* Figure 7: The Inception score is typically expressed as an exponentiated KL-divergence. It is odd that the scores are being presented here as percents.--------* Section 4.1: The Inception score's direct application to non-natural images is indeed problematic. One potential workaround is to compute exponentiated KL for a discriminative net trained specifically for the dataset, e.g. to predict binary attributes on CelebA. ----------------Overall, the paper attempts to simultaneously do too many things. It could be made much stronger by focusing on the primary contribution, the IAN, and performing a comparison against similar approaches such as [1]. The other techniques, such as IAF with randomized MADE, MDC blocks, and orthogonal regularization are potentially interesting in their own right but the current results are not conclusive as to their specific benefits.----------------[1] Larsen, Anders Boesen Lindbo, Søren Kaae Sønderby, and Ole Winther. ""Autoencoding beyond pixels using a learned similarity metric."" arXiv preprint arXiv:1512.09300 (2015).--------[2] J.-Y. Zhu, P. Krähenbühl, E. Shechtman, and A. A. Efros, “Generative Visual Manipulation on the Natural Image Manifold,” ECCV 2016.","Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster.","This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being"," The paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data . The discriminator attempts to classify each true data point, sample, and reconstruction as being",", but the paper attempts to simultaneously do too many things.------------- Section 3.3.2: It is odd that not training MADE leads to better performance.-------------------------------------",0.17,0.020202020202020204,0.11999999999999998,0.8311649560928345,0.7807199358940125,0.8051531314849854
https://openreview.net/forum?id=HkNKFiGex,"After rebuttal:----------------I think the presentation improved in the revised version (although still quite cluttered and confusing), and new quantitative results look quite convincing. Therefore I raise my rating. Still, the paper could use polishing. If written in a better way, it would be a definite accept in my opinion.---------------------------------Initial review:----------------The paper presents a tool for exploring latent spaces of generative models, and ""introspective adversarial network"" model - a new hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE). On the plus side, the presented tool is interesting and may be useful for analysis of generative models, and the proposed architecture seems to perform well. On the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by Zhu et al. [1] is not discussed in enough detail. Overall, I am in the borderline mode, and may change my opinion depending on how the discussion phase goes. ----------------Detailed comments:----------------1) The presented model combines elements of a large number of existing techniques: GAN, VAE, VAE/GAN (Lamb et al. 2016), inverse autoregressive flow (IAF), PixelRNN, ResNet, dilated convolutions. In addition, the authors propose new modifications: orthogonal regularization (inspired by Saxe et al.), ternary discriminator in a GAN. This makes the overall architecture complicated. An extensive ablation study could allow to judge about the effect of different components, but the ablation study presented in the paper is somewhat restricted. What do we learn from the proposed architecture? Can other researchers gain any new insights? What is important, what is not? Answering these question would significantly raise the potential impact of this paper.----------------2) Related to the previous point, proper analysis requires adequate measures of performance. Qualitative results are nice, but with the current surge of interest in generative models it gets very difficult to rely on qualitative evaluations: many methods produce visually similar results, and unless there is an obvious large jump in the quality of the produced images, it is unclear how to compare these. I do appreciate the effort authors have already put into evaluating the model: especially the keypoint error is interesting. Unfortunately, none of the presented measures evaluates visual quality of the images. A user study would be useful - I realize it is additional effort, but what is so restrictively difficult about it? Perhaps not with AMT, but with some fellow researchers/students.----------------3) Work [1] looks very related to the proposed visualization tool and deserves a more thorough discussion than a single sentence in the related work section. The paper by Zhu et al. appeared on arxiv more than 1,5 months before the ICLR deadline, and, more importantly, it has been published at ECCV before the ICLR deadline. This is unfortunate for the authors, but I think this makes the paper count as prior work, not concurrent. In the end this is up to ACs and PCs to decide. Anyway, I strongly suggest the authors to add a detailed discussion of differences of the two approaches, their capabilities, strengths and weaknesses. The authors could also try to directly compare to the approach of Zhu et al. or explain why it is impossible.----------------4) May be a good idea to extend Appendix A with more approaches (VAE and DCGAN are not state of the art, are they? why not show at least VAE/GAN?) and more datasets. If this is not possible, please explain why. Samples of faces from IAN do look very impressive, but a fair comparison with SOTA would strengthen your point. By the way, I assume the samples are random, not cherry-picked? Please mention it in the paper. ----------------5) The analysis capabilities of the proposed tool are not fully explored. What does it teach us about generative models? Does it work on non-face datasets? Overall, it seems that since the paper includes two largely disjoint contributions (a tool and a generative model), none of the two gets analyzed in depth, which makes the paper look somewhat incomplete.  ----------------Small remarks:----------------1) Why not include 8% result of IAN on SVHN into the table? It is easy to miss otherwise. From the table it is absolutely unclear that some of the results are not comparable. The table should be more self-explanatory.----------------[1] Zhu et al., ""Generative Visual Manipulation on the Natural Image Manifold"", ECCV 2016, https://arxiv.org/pdf/1609.03552v2.pdf","Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster.",The paper presents a tool for exploring latent spaces of generative models. The tool is a hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE) The proposed architecture seems to perform,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents a tool for exploring latent spaces of generative models. The tool is a hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE) The proposed architecture seems to perform," The paper presents a tool for exploring latent spaces of generative models, and ""introspective adversarial network"" model . The proposed architecture seems to perform well, but experimental evaluation does not allow for confident conclusions .","generative models, and ""introspective adversarial network"" model - a new hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE) ",0.19487179487179487,0.04145077720207255,0.13333333333333333,0.83349609375,0.7846487760543823,0.8083351850509644
https://openreview.net/forum?id=S1di0sfgl,"This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.----------------Question) Can you extend it to bidirectional RNN? ","This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.",The method is elegantly formulated within a recurrent neural network framework. The paper is well written. Can you extend it to bidirectional RNN?,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The method is elegantly formulated within a recurrent neural network framework. The paper is well written. Can you extend it to bidirectional RNN?," The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks . The paper is well written .",-------------Question) Can you extend it to bidirectional RNN?----------------Question) Can you extend it to bidirectional RNN?,0.22950819672131145,0.03389830508474577,0.13114754098360656,0.8847448229789734,0.8670157194137573,0.8757905960083008
https://openreview.net/forum?id=S1di0sfgl,"The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.----------------Pros:--------- Paper is well-motivated, exceptionally well-composed--------- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation--------- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.--------Cons:--------- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a single level (where the data contains separators such as spaces and pen up/down) does not seem to be demonstrated.--------- It's unclear whether better downstream performance is due to use of hierarchical information or due to the architecture changes acting as regularization, something which could hopefully be addressed.","This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.","The paper proposes a modified RNN architecture with multiple layers. Higher layers are only passed lower layer states if a FLUSH operation is predicted. The paper is well-motivated, exceptionally well-composed.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper proposes a modified RNN architecture with multiple layers. Higher layers are only passed lower layer states if a FLUSH operation is predicted. The paper is well-motivated, exceptionally well-composed.", The paper proposes a modified RNN architecture with multiple layers . The authors propose using the straight-through estimator with a slope-annealing trick during training . Empirical results and visualizations illustrate that the modified architecture performs well at,"with a slope-annealing trick during training. The paper proposes a modified RNN architecture with multiple layers. The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower",0.22857142857142856,0.029411764705882353,0.14285714285714285,0.8705471754074097,0.8615646362304688,0.866032600402832
https://openreview.net/forum?id=S1di0sfgl,"This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).----------------Their model is organized as a set of layers that aim at capturing the information form different “level of abstraction”. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. ----------------The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.----------------Overall this paper presents a strong and novel model with promising experimental results.--------------------------------On a minor note, I have few remarks/complaints about the writing and the related work:----------------- In the introduction:--------“One of the key principles of learning in deep neural networks as well as in the human brain” : please provide evidence for the “human brain” part of this claim.--------“For modelling temporal data, the recent resurgence of recurrent neural networks (RNN) has led to remarkable advances” I believe you re missing Mikolov et al. 2010 in the references.--------“in spite of the fact that hierarchical multiscale structures naturally exist in many temporal data”: missing reference to Lin et al., 1996----------------- in the related work:--------“A more recent model, the clockwork RNN (CW-RNN) (Koutník et al., 2014) extends the hierarchicalRNN (El Hihi & Bengio, 1995)” : It extends the NARX model of Lin et al. 1996, not the El Hihi & Bengio, 1995.--------While the above models focus on online prediction problems, where a prediction needs to be made…”: I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks.--------“The norm of the gradient is clipped with a threshold of 1 (Pascanu et al., 2012)”: this is not the first work using gradient clipping. I believe it was introduced in Mikolov et al., 2010.----------------Missing references:--------“Recurrent neural network based language model.”, Mikolov et al. 2010--------“Learning long-term dependencies in NARX recurrent neural networks”, Lin et al. 1996--------“Sequence labelling in structured domains with hierarchical recurrent neural networks“, Fernandez et al. 2007--------“Learning sequential tasks by incrementally adding  higher  orders”, Ring, 1993","This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model.",This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data. Their approach does not require boundary information to segment the sequence in meaningful groups. The experiment section is thorough and their model obtain competitive,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data. Their approach does not require boundary information to segment the sequence in meaningful groups. The experiment section is thorough and their model obtain competitive," This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data . Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016)","based on a threshold of 1 (Pascanu et al., 2016): I believe there is a lot of missing references, in particular to Socher’s work or older recursive networks",0.3373493975903614,0.0,0.19277108433734938,0.8629620671272278,0.8547074794769287,0.8588149547576904
https://openreview.net/forum?id=ryXZmzNeg,"This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.----------------The paper in its current form is not acceptable due to the following reasons:--------1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.--------2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors ""must be doing"". What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it's P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior P(Z | X)?----------------Comments:--------1. Using additive noise in the input does not seem like a reasonable idea. Any justification of why this is being done?--------2. Approaches which learn transition operators are usually very amenable to data augmentation-based semi-supervised learning. I encourage the authors to improve their paper by testing their model on semi-supervised learning benchmarks.","This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference.","This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution."," This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples .","is not acceptable due to the following reasons:--------1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.----------",0.10778443113772455,0.012121212121212121,0.0718562874251497,0.8209609985351562,0.8109704256057739,0.8159350752830505
https://openreview.net/forum?id=ryXZmzNeg,"The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.----------------Comments: -------- - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper-------- - Notation is nonstandard / confusing. At page 1, it’s unclear what the authors mean with “p(x|z) which is approximated as q(x|z)”.--------- It’s also not clear what’s meant with q(z). At page 2, q(z) is called the learned distribution, while p(z) can in general also be a learned distribution.--------- It’s not true that it’s impossible to draw samples from q(z): one can sample x ~ q(x) from the dataset, then draw z ~ q(z|x).--------- It's not explained whether the analysis only applies to continuous observed spaces, or also discrete observed spaces--------- Figures 3 and 4 are not very convincing.","This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference.","The authors propose to sample from VAEs through a Markov chain. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing,",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors propose to sample from VAEs through a Markov chain. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing,"," The authors propose to sample from VAEs through a Markov chain . The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results . The qualitative difference between regular sampling and this Gibbs chain is not very convincing .",.------------ - Rezende et al's (2014) original VAE paper already discusses the Markov chain.--------------- It’s not true that it’s impossible to draw samples from,0.18497109826589597,0.04678362573099415,0.1271676300578035,0.84752357006073,0.8285806775093079,0.8379450440406799
https://openreview.net/forum?id=ryXZmzNeg,"The authors argues that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial--------Autoencoder) imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov Chain Monte Carlo approach that avoids the need to specify a simple parametric form for the prior.----------------The paper is not clearly written. Most critically, the notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. For example, the authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are parametrized. For this to be true the model must either be trivially simple, or an energy-based model. There is no indication that they are speaking of an energy-based model. Another example of possible confusion is the statement that the ratio of distributions Q(Z|X)/P(Z) = 1. I believe this is supposed to be a ratio of marginals: Q(Z)/P(X) = 1. Overall, it seems like there is a confusion of what Q and P represent. The standard notation used in VAEs is to use P to represent the decoder distribution and--------Q to represent the encoder distribution. This seems not to be how the authors are using these terms. Nor does it seem like there is a single consistent interpretation. ----------------The empirical results consist entirely of qualitative results (samples and reconstructions) from a single dataset (CelebA). The samples are also not at all up to the quality of the SOTA models. The interpolations shown in Figures 1 and 3 both seems to look like interpolation in pixel space for both the VAE model and the proposed DVAE. ","This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference.","The authors argue that the standard ancestral sampling from stochastic autoencoders imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors argue that the standard ancestral sampling from stochastic autoencoders imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov"," The paper is not clearly written. The notation the authors use is either deeply flawed, or there are simple misunderstanding . The authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are paramet",. The authors argue that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial----------Autoencoder) imposes the,0.10650887573964497,0.011976047904191616,0.08284023668639053,0.8192514181137085,0.8157771229743958,0.8175105452537537
https://openreview.net/forum?id=HyQWFOVge,"I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. ----------------Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all. It appears to me the comparison is not apple vs apple between the proposed method and Rippel et al. ----------------It would be great if we know adding feature reduction or adding another layer on top of pool5 can improve finetued softmax result. However, I am not sure what is the biggest contributing factor to the superior results. Before getting more clarifications from the authors, I lean toward rejection. ","The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted.","I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method. All the experimental results show",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method. All the experimental results show"," For classification task, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image . All the experimental results show that the softmax features work better than",. I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. All the experimental results show that the softmax features work better than Rippel et al DML method,0.35135135135135137,0.11111111111111112,0.2162162162162162,0.8723146319389343,0.8890736103057861,0.8806143403053284
https://openreview.net/forum?id=HyQWFOVge,"I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding). I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc). As far as I can tell, there is really only a single ""retrieval""-like benchmark being used here - the Stanford Online Products database. All the other datasets are used in a ""classification-by-retrieval"" approach which seems contrived. While ostensibly evaluating ""retrieval"", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category. With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task.----------------It seems obvious that if your task is classification, a network trained with a classification loss will be best. Even when these datasets are used in a ""retrieval"" setting, the ground truth is still defined by category membership. It's still a classification task. ----------------I don't really see the point of using DML in these scenarios. I guess prior work claims to outperform SoftMax in these settings so this paper is fighting back against this and I should be thankful for this paper. But I think this paper's narrative is a bit off. The narrative shouldn't be ""We can get good retrieval features from softmax networks with appropriate normalization"". It should be ""It never made sense to train or evaluate these things as retrieval tasks. Direct classification is better"". For example, why are you taking the second to last layer or pool5 layer from these networks? Why aren't you taking the last layer? That should do well in these evaluations, right? Table 1 and 2 do show that using softmax probabilities directly tends to be better than doing classification-by-retrieval (works better or the same as doing retrieval with an earlier layer of features, except on Oxford flowers).----------------GoogLeNet is quite deep and gets auxiliary supervision. By the second-to-last layer of the network, the activations could look a lot like category membership already. And category membership is all that's needed for the tasks in 4.2 and 4.3. -------- --------I don't think my pre-review question was adequately addressed. I was getting at this concern by pointing out numerous scenarios where distance metric learning makes sense because you have fine-grained associations between instances at training time, NOT categorical associations -- e.g. this product photo corresponds to this photo of the object in a scene [Bell et al. 2015], this 3d model correspond to this sketch [Wang et al. 2015], this sketch corresponds to this photo [Sangkloy et al. 2016], this ground view corresponds to this aerial view [Lin et al., 2015]. DeepFace and follow-up works on LFW could also fit into this space because there are few training samples per class (few training samples per person identity). You cite DeepFace and Bell et al. 2015 but you don't compare on those benchmarks. I think those are exactly the tasks where DML makes sense.----------------Maybe the ""retrieval on classification datasets"" would be a reasonable benchmark if the test and train classes were completely different. Then you could argue that softmax is learning a useful representation yet the last layer isn't directly useful since the categories change. But that's not the case here, is it?----------------With all of this said, I'm not sure whether I'm positive or negative about this paper. I think you're onto something significant -- people have been using DML where it is not appropriate -- but addressed it in the wrong way -- by using softmax for ""classification by retrieval"". But you don't need to do retrieval! Softmax is already telling you the class prediction! Why go through the extra step of finding nearest neighbors with some intermediate feature?----------------AnonReviewer3 also raises some good points and you should be thankful that a reviewer is willing to dig so deep to help make your experiments sound! I don't think his/her concerns are disqualifying for this paper, though, as long as it is fixed.----------------I look forward to hearing your response. I want this paper to be published, but I think it needs to be tweaked.","The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted.","I don't understand why Distance Metric Learning (DML) is being used for classification tasks. There is really only a single ""retrieval""-like benchmark being used here. Even when these datasets are used in a ""ret",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","I don't understand why Distance Metric Learning (DML) is being used for classification tasks. There is really only a single ""retrieval""-like benchmark being used here. Even when these datasets are used in a ""ret"," Distance Metric Learning (DML) is being used for classification tasks . As far as I can tell, there is really only a single ""retrieval""-like benchmark being used here . Even when these datasets are used in a",". I don't understand why DML is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc.)",0.20289855072463767,0.02985074626865672,0.14492753623188404,0.8227843642234802,0.8529523611068726,0.8375968337059021
https://openreview.net/forum?id=HyQWFOVge,"There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance / interest to ICLR. This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches? Thorough experiments seem to indicate that this is indeed the case. Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets / tasks for which the DML approaches were designed. ----------------This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature. The authors argue that previous approaches did not fully/properly tune the softmax baselines, or that comparisons were not apples-to-apples. Also, one change in the current paper is the addition of L2 normalization, which is well motivated and helps improve SoftMax feature distances. Different dimensionality reduction approaches are also tested. These changes are minor, but especially the L2 normalization proves to be a simple but effective improvement for SoftMax features.----------------A big issue is with how pre-training is performed (in Magnet Loss the softmax baselines were pretrained for less time on ImageNet). The approach taken here is reasonable, but so is the approach in Magnet Loss (for different reasons). Ultimately, both are fine. Unfortunately, due to use of different schemes, the results are not comparable. Let me copy-paste what I wrote in an earlier comment: ----------------My main concern about the paper is that the comparisons in Tables 1 and 2 and Figure 4 to Rippel et al. are not apples-to-apples. Basically, the papers shows that absolute results of using SoftMax w full pre-training (PT) on ImageNet is superior to any of the results in Rippel's paper (including both the Softmax and Magnet results). But as the current results show, PT appears to be critical to obtaining such good numbers - The Rippel SoftMax numbers use only 3 PT stages and are dramatically worse than the full PT on ImageNet. As it stands, I am not convinced that SoftMax is actually better than Magnet. Here is the evidence we have (I'll use Stanford Dogs as an example, but any of the datasets have the same conclusion): (1) Softmax w 3 stages of PT: 26.6% (from Rippel paper) and 32.7% (from authors' reproduction) (2) Magnet w 3 stages of PT: 24.9% (3) Softmax w full PT: 18.3% (4) Magnet w full PT: not shown From this all I see is that PT is critical for getting absolute good results. However, what about Magnet w full PT? These results are not shown either here or in the original Rippel paper (I went back and looked). As such, I do not think it is justifiable to claim superiority of Softmax to Magnet based on available evidence. (Note: I looked back carefully at Rippel's paper, and it appears that the authors use 3 PT stages as a form of ""warmup"". There is a statement that using full PT would ""defeat the purpose of pursuing DML"". I'm not sure if I agree w Rippel's statement since in the present paper there is clear evidence that full PT is hugely helpful, at least for softmax. That being said, I did not see any evidence in the Rippel paper that PT is harmful or that DML wouldn't work with full PT.)----------------The authors responded to my concern by claiming that “from Rippel's results, it is no doubt that Magnet@3epochPT > Magnet@FullPT and Magnet@3epoch > Softmax@3epochPT.” However, I went back to Rippel’s paper, and simply the Magnet@FullPT experiment never appears. I further went and contacted Oren Rippel himself, and he verified he never ran the Magnet@FullPT experiment. I encourage the authors to contact Oren Rippel regarding this if they wish to verify (I have asked Oren Rippel to not reveal my identity). [Disclaimer: I am NOT Oren Rippel]. The authors mentioned that they are training Magnet@FullPT. If results were shown for Magnet@FullPT and also retrain the Magnet@3epochPT as a sanity check, that would help alleviate this concern. Alternatively, the language in Section 4 and the Tables could be altered to make clear that the methods use different pretraining and hence are not comparable.----------------Overall, I am actually quite sympathetic to this work. I think it could serve as an important sanity-check paper for the community and quite relevant to ICLR. Having proper and strong SoftMax baselines should prove quite useful to the DML community and to this line of work.----------------However, currently I find the main results (table 1, table 2, figure 4, etc.) to be misleading. If indeed it were the case that Magnet@3epochPT > Magnet@FullPT, then it would be fine. However, at this point as far as I know no one has actually tried Magnet@FullPT. And, given the general importance and effectiveness of pre-training, especially when transferring to small dataset, I would be hugely surprised if Magnet@FullPT was not superior by a large margin. I think either having this experiment in place or altering the writing / presentation of the results would be critical to allow for publishing.","The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted.","There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance / interest to ICLR.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance / interest to ICLR.", A recent paper shows that a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches . The authors argue that previous approaches did not fully/properly tune the soft,: Magnet w full PT: 26.6% (from Rippel paper) and 18.3% (from authors' reproduction) Magnet w full PT: not shown (from Rippel paper)-------------,0.3142857142857143,0.08823529411764706,0.20000000000000004,0.8576719760894775,0.8710728883743286,0.8643205165863037
https://openreview.net/forum?id=ryAe2WBee,"The paper proposes a semantic embedding based approach to multilabel classification. --------Conversely to previous proposals, SEM considers the underlying parameters determining the--------observed labels are low-rank rather than that the observed label matrix is itself low-rank. --------However, It is not clear to what extent the difference between the two assumptions is significant----------------SEM models the labels for an instance as draws from a multinomial distribution--------parametrized by nonlinear functions of the instance features. As such, it is a neural network.--------The proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. ----------------The paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. ","This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions.",The paper proposes a semantic embedding based approach to multilabel classification. The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes a semantic embedding based approach to multilabel classification. The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear.," SEM proposes a semantic embedding based approach to multilabel classification . SEM models the labels for an instance as draws from a multinomial distribution . As such, it is a neural network .","is not very clear. -----------The paper proposes a semantic embedding based approach to multilabel classification. -----------Conversely to previous proposals, SEM considers the underlying parameters ",0.2459016393442623,0.05,0.19672131147540986,0.860373854637146,0.8481186032295227,0.8542022705078125
https://openreview.net/forum?id=ryAe2WBee,"The paper presents the semantic embedding model for multi-label prediction.--------In my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!--------I was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.--------Regarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.--------One last question: why is it called ""semantic"" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.","This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions.",The paper presents the semantic embedding model for multi-label prediction. The proposed approach assumes the number of labels to predict is known.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents the semantic embedding model for multi-label prediction. The proposed approach assumes the number of labels to predict is known., The paper presents the semantic embedding model for multi-label prediction . The proposed approach assumes the number of labels to predict is known .,from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent.--------I was trying to understand how different is SEM from a basic MLP,0.2105263157894737,0.017857142857142856,0.12280701754385966,0.8858241438865662,0.8370848894119263,0.8607651591300964
https://openreview.net/forum?id=ryAe2WBee,"This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.----------------Given that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.","This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions.","This paper proposes SEM, a simple large-size multilabel learning algorithm. It models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes SEM, a simple large-size multilabel learning algorithm. It models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network."," This paper proposes SEM, a simple large-size multilabel learning algorithm . It models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network .","is not novel, nor is the idea of optimizing this by adagrad. This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(",0.18032786885245902,0.05,0.14754098360655737,0.8369249701499939,0.8378093838691711,0.8373669981956482
https://openreview.net/forum?id=rJXTf9Bxg,"Apologies for the late review.----------------This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.----------------Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).----------------The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).----------------Discriminability: Figure 3 doesn’t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place.----------------Diversity: The authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation…). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1. Evaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself. Conversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure.----------------Overall, the paper proposes a new way to incorporate class labels into training GAN-type models. As far as I know the particular algorithm is novel, but I consider it incremental compared to what has been done before. I think the proposed evaluation metrics are flawed, especially when evaluating the diversity of the samples for the aforementioned reasons.","Ratings summary:  3: Clear rejection  6: Marginally above acceptance threshold  6: Marginally above acceptance threshold    Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The authorÕs point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.    Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference.",This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two, This submission proposes method for class-conditional generative image modeling using auxiliary classifiers . Compared to normal GANs the generator receives a randomly sampled class label c from the class distribution . The discriminator has two outputs and two objectives,- not surprisingly - that higher resolution improves discriminability (because more information is present).----------------This submission proposes method for class-conditional generative image modeling using auxiliary classifiers ,0.16793893129770993,0.0,0.10687022900763357,0.8367687463760376,0.8058533668518066,0.8210201263427734
https://openreview.net/forum?id=rJXTf9Bxg,"This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows:--------- Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.--------- Training different models on different subsets of imagenet classes improves performance.--------- They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)--------- They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .----------------The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the inception score of [1] except with class information. However, I think there is still a contribution to be made my putting these tricks together and successfully demonstrating image synthesis gains. ----------------Questions for the authors: --------(1) Why do you think splitting the imagenet training into 100 different models improves performance?  Is the issue with the representation of the class? In other words, if an encoding more meaningful that 1-hot vector was used do you still think 100 models would be needed. Ideally we should hope that a generative model can leverage information from different classes to help with the generation of a particular class and also text-image synthesis models [2] have been quite successful when trained on diverse datasets (and these are conditioned on a semantically meaningful text encoding) which suggests to be that the issue is with the representation. --------(2) In section 3 the AC-GAN classification objective (omitting expectation for brevity) is given as L_S = log P(C=c|X_real) + log P(C=c|X_fake) and you say that both the discriminator and generator are trained to maximize this quantity. Obviously the generator would want to maximize  log P(C=c|X_fake) for its given conditioning class c. But can you explain why you would want the discriminator to also maximize the classification accuracy of generated samples? Why not do something similar to the CatGAN paper [3] and train the discriminator to be as uncertain as possible about the generated examples. Seems counterintuitive to me to have both the generator and discriminator trying to optimize the same classification objective rather than not be adversarial wrt to this loss as well as the real/fake loss. ----------------Overall, this paper makes a clear contribution to GAN research both in terms of image quality and evaluation metrics and I would recommend it for acceptance. ----------------[1] Salimans et al. Improved Techniques for Training GANs (https://arxiv.org/abs/1606.03498)--------[2] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396)--------[3] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with categorical generative adversarial networks (https://arxiv.org/abs/1511.06390)","Ratings summary:  3: Clear rejection  6: Marginally above acceptance threshold  6: Marginally above acceptance threshold    Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The authorÕs point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.    Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference.","This is a clear, easy to read, highly relevant paper that improves GAN training for images. The main contributions are as follows: Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This is a clear, easy to read, highly relevant paper that improves GAN training for images. The main contributions are as follows: Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to"," The paper is a clear, easy to read, highly relevant paper that improves GAN training for images . The main contributions are to adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the",a classifier head to a GAN discriminator and training a classification objective in addition to the real/fake objective improves performance. -----------[1] Salimans et al,0.21212121212121213,0.06153846153846154,0.16666666666666669,0.8463989496231079,0.8109360933303833,0.8282880783081055
https://openreview.net/forum?id=rJXTf9Bxg,"This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.----------------Pros:--------+ The paper is clear and well-written.--------+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.--------+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.----------------Cons:--------- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.--------- Diversity metric is of limited use for training non class-conditional GANs.--------- No experimental comparison of AC-GAN to other class-conditional models.----------------To my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.----------------* Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B?--------* Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison.--------* Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.----------------[1] Salimans, Tim, et al. ""Improved techniques for training GANs."" Advances in Neural Information Processing Systems. 2016.","Ratings summary:  3: Clear rejection  6: Marginally above acceptance threshold  6: Marginally above acceptance threshold    Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The authorÕs point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.    Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference.",This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to, The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models . The AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class,of class-conditional GANs. This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether ,0.16666666666666669,0.0,0.10606060606060606,0.8433235287666321,0.8069535493850708,0.8247377276420593
https://openreview.net/forum?id=SkqMSCHxe,"This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.---------------- It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the “integrated prediction” obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.----------------It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger model? ----------------A large amount of additional work is required to sustain the claim that this scheme is successfully extracting driver ‘intentions’. It would be interesting to see if the scheme, suitably extended, can automatically infer the intention to stop at a stop sign vs slowing but not stopping due to a car in front, say, or to pass a car vs simply changing lanes. Adding labels to the dataset may enable this comparison more clearly.----------------More generally, the intention of the driver seems more related to the goals they are pursuing at the moment; there is a fair amount of work in inverse reinforcement learning that examines this problem (some of it in the context of driving style as well).","The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either.","This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only"," This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input . The architecture allows several RNNs to compete to make the best predictions, with only",. This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. This paper introduces a neural network architecture and training procedure,0.22,0.0,0.13999999999999999,0.8650467395782471,0.8587779998779297,0.8619009256362915
https://openreview.net/forum?id=SkqMSCHxe,"This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. ----------------The competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it’s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.----------------There is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of free parameters comparable is critical to validating the proposed approach.----------------The graphical model of the driving process depicted in figure 1 seems nonsensical. If e is observed then all variables are known given the dependencies shown. Further, it is at best very poor notation to say that the driving action d decided at time t affects the vehicle state s at that same time. It should be that s_t depends on d_(t-1). Also, according to this figure the driving decision d does not depend on the observed vehicle state x which also seems invalid.----------------Odd to have a paragraph break in abstract----------------Figure 1 caption should include a brief explanation of the variables shown","The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either.",This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The paper is understandable but could benefit,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The paper is understandable but could benefit, This paper proposes a neural network architecture for car state prediction while driving based on competitive learning . Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss .,.----------------The competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling.-------------The competitive learning approach seems rather ,0.17821782178217824,0.0,0.1188118811881188,0.8551842570304871,0.854564368724823,0.8548741936683655
https://openreview.net/forum?id=SkqMSCHxe,"Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.--------It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?--------The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited.","The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either.","Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts. This work is applied to the task of predictive different driving behaviors from human drivers.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts. This work is applied to the task of predictive different driving behaviors from human drivers."," Authors propose a competitive learning architecture that learns different RNN predictors independently, akin to a committee of experts . This work is applied to the task of predictive different driving behaviors from human drivers .",". Authors propose a competitive learning architecture that learn different RNN predictors independently. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within",0.28888888888888886,0.0,0.2222222222222222,0.881719708442688,0.8611637353897095,0.8713205456733704
https://openreview.net/forum?id=S1RP6GLle,"The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments:----------------1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.----------------2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still need the {HR, LR} image pairs. Does it mean that we merely transfer this training procedure to the training of affine projection?----------------3) The paper presents many results of the framework, including the results of natural images from ImageNet. Can the author also provide the results of Set5, Set14 or BSD100, which are conventional test dataset for SR, so that we can perform a fair comparison with previous work.----------------4) I see that the size of the results of nature images presented in this paper are limited to 128*128. Can this framework perform well on images with larger size? Because SR will encounter input with arbitrary size.----------------5) A normal GAN will have a noise term as a latent space, so that it can be better illustrated as learning a distribution. Do the author try the noise vector?----------------Overall, this paper provides a new framework for SR with solid theoretical analysis. The idea is novel and the author explore many methods. Though there still exist questions like the necessity and more experiments are needed. I think this work will will provide good inspiration to the community.","All the reviewers agreed that the paper is original, of high quality, and worth publishing.","The paper presents a new framework to solve the SR problem - amortized MAP inference. It adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents a new framework to solve the SR problem - amortized MAP inference. It adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to", The paper presents a new framework to solve the SR problem - amortized MAP inference . It adopts a pre-learned affine projection layer to ensure the output is consistent with LR . The idea is novel and the author explore,.----------------3) The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output,0.15384615384615385,0.04,0.11538461538461539,0.8274205923080444,0.8628239631652832,0.8447515368461609
https://openreview.net/forum?id=S1RP6GLle,"Sincere apologies for the late review.----------------This paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout. Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested. ----------------Summary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain. ----------------Manuscript should be proof-read once more, there were some very few typos that may be worth fixing.","All the reviewers agreed that the paper is original, of high quality, and worth publishing.",This paper argues to approach Super-Resolution as amortised MAP estimation. It presents well chosen toy examples and real world applications.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper argues to approach Super-Resolution as amortised MAP estimation. It presents well chosen toy examples and real world applications., This paper argues to approach Super-Resolution as amortised MAP estimation . A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified . Further three different methods to solve the resulting cross-entropy problem in,. ---------------This paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout,0.1111111111111111,0.0,0.1111111111111111,0.8391910791397095,0.868074893951416,0.8533886671066284
https://openreview.net/forum?id=S1RP6GLle,"The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.--------Results are nicely demonstrated on several datasets.----------------I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? ","All the reviewers agreed that the paper is original, of high quality, and worth publishing.",The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method., The paper presents an amortised MAP estimation method for SR problems . By learning a neural network which learns to project to an affine subspace of SR solutions .,".----------Results are nicely demonstrated on several datasets.-------------I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer",0.163265306122449,0.0425531914893617,0.12244897959183675,0.8177733421325684,0.8556370139122009,0.8362768292427063
https://openreview.net/forum?id=SJqaCVLxx,"The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. ----------------Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? ----------------In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard way of training. The experimental section is equally confusing and unconvincing. ----------------Other comments: ---------- The figures still say LeCun-5---------- The legends of the plots are not in english. Hence I'm not sure what is going on there. ---------- The paper is riddled with typos and hard to understand phrasing. ","This paper is unfortunately quite unclear and unreadable and nowhere near ready for any conference.  I would advise the authors to 1) restructure their paper to present first some type of context and identify a problem that they are trying to solve, 2) explain what novel method they propose to solve the identified problem and why this method is promising and how it relates to existing methods, 3) explain what their experiments are trying to do and what the results of the experiments are, 4) enlist someone fluent in English to help with writing and re-reading.  A way to do this is to find a set of well-cited papers in the same domain with similar ideas and see how they are structured, then try to follow similar outlines.",The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. The paper is extremely hard to,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. The paper is extremely hard to, The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset .,the algorithm is validated on some version of the MNIST dataset. ------------------Unfortunately the paper is extremely hard to follow and it is not at all clear what the exact training algorithm is. ------,0.22485207100591717,0.03592814371257485,0.14201183431952663,0.8298675417900085,0.816349983215332,0.823053240776062
https://openreview.net/forum?id=SJqaCVLxx,"Unfortunately, this paper is very difficult to understand.  The current version of this paper seems improved compared to the initial version, but still far from a finished level.  I'd encourage the authors to keep editing over the language and presentation.----------------I also think it would be good to also try answering some of the following questions very clearly in the paper:----------------- What is the advantage, if any, of the proposed algorithm over SGD?  What is the motivation and goal of the work beyond MNIST benchmarking?----------------- Why are few training examples used?  Is this a scenario in which the system might have an advantage?----------------- Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network.----------------- Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms.----------------- A single experiment on MNIST is too small to adequately describe the algorithm performance.  Consider using a second or third dataset and/or experimental application.----------------Much work is still needed on the paper's writing before it can be understood well enough.  I hope that some of this might be useful in helping to improve. I would encourage the authors to try to find outside readers, preferably fluent in English, to work with on a frequent basis before resubmitting to another venue.","This paper is unfortunately quite unclear and unreadable and nowhere near ready for any conference.  I would advise the authors to 1) restructure their paper to present first some type of context and identify a problem that they are trying to solve, 2) explain what novel method they propose to solve the identified problem and why this method is promising and how it relates to existing methods, 3) explain what their experiments are trying to do and what the results of the experiments are, 4) enlist someone fluent in English to help with writing and re-reading.  A way to do this is to find a set of well-cited papers in the same domain with similar ideas and see how they are structured, then try to follow similar outlines.","The current version of this paper seems improved compared to the initial version, but still far from a finished level. I would encourage the authors to try to find outside readers, preferably fluent in English, to work with on a frequent basis before",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The current version of this paper seems improved compared to the initial version, but still far from a finished level. I would encourage the authors to try to find outside readers, preferably fluent in English, to work with on a frequent basis before"," The current version of this paper seems improved compared to the initial version, but still far from a finished level . I'd encourage the authors to keep editing over the language and presentation .",", if any, of the proposed algorithm over SGD?-------------- What is the advantage, if any, of the proposed algorithm over SGD?-------------- What is the motivation and goal",0.24277456647398846,0.10526315789473685,0.16184971098265896,0.8723628520965576,0.8241481184959412,0.8475703597068787
https://openreview.net/forum?id=SJqaCVLxx,The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.,"This paper is unfortunately quite unclear and unreadable and nowhere near ready for any conference.  I would advise the authors to 1) restructure their paper to present first some type of context and identify a problem that they are trying to solve, 2) explain what novel method they propose to solve the identified problem and why this method is promising and how it relates to existing methods, 3) explain what their experiments are trying to do and what the results of the experiments are, 4) enlist someone fluent in English to help with writing and re-reading.  A way to do this is to find a set of well-cited papers in the same domain with similar ideas and see how they are structured, then try to follow similar outlines.",The paper is still extremely poorly written and presented. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult. Consider getting help from someone with good english and presentation skills.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper is still extremely poorly written and presented. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult. Consider getting help from someone with good english and presentation skills., The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue . The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming . Consider getting help from someone with,", incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.",0.2142857142857143,0.012048192771084338,0.14285714285714285,0.8859734535217285,0.8205838203430176,0.8520258665084839
https://openreview.net/forum?id=SkpSlKIel,"SUMMARY --------This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. ----------------PROS --------The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. ----------------CONS--------The main result appears to address only strongly convex univariate functions. ----------------SPECIFIC COMMENTS ----------------- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. --------I still have not read the revision and maybe you have already addressed some of these points there. ----------------- The problem statement is close to that from [Montufar, Pascanu, Cho, Bengio NIPS 2014], which specifically arrives at exponential gaps between deep and shallow ReLU networks, albeit from a different angle. I would suggest to include that paper it in the overview. ----------------- In Lemma 3, there is an i that should be x----------------- In Theorem 4, ``\tilde f'' is missing the (x). ----------------- Theorem 11, the lower bound always increases with L ? ----------------- In Theorem 11, \bf x\in [0,1]^d? ","The paper makes a solid technical contribution in proving that the deep networks are exponentially more efficient in function approximation compared to the shallow networks. They take the case of piecewise smooth networks, which is practically motivated (e.g. images have edges with smooth regions), and analyze the size of both the deep and shallow networks required to approximate it to the same degree.    The reviewers recommend acceptance of the paper and I am happy to go with their recommendation.",This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible, This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units . The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible,SUMMARY -----------This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. --------------PROS --------,0.38016528925619836,0.05042016806722689,0.2479338842975207,0.8696401715278625,0.8541408777236938,0.8618208169937134
https://openreview.net/forum?id=SkpSlKIel,The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:----------------1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?----------------2- Is there an example of piecewise smooth function that requires at least poly(1/eps) hidden units with a shallow network?,"The paper makes a solid technical contribution in proving that the deep networks are exponentially more efficient in function approximation compared to the shallow networks. They take the case of piecewise smooth networks, which is practically motivated (e.g. images have edges with smooth regions), and analyze the size of both the deep and shallow networks required to approximate it to the same degree.    The reviewers recommend acceptance of the paper and I am happy to go with their recommendation.",The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network. The paper is well written and clear. The arguments and proofs are easy to follow.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network. The paper is well written and clear. The arguments and proofs are easy to follow., The paper is well written and clear . It would be great to have similar results without binary step units . To what extent do you find the binary step unit central to the proof?,(1/eps)) layers and O(poly log(1/eps)) hidden units where the activation functions can be ReLU or binary step or any combination of them. The paper is well written and clear,0.3448275862068966,0.03508771929824561,0.20689655172413793,0.872094452381134,0.8516722917556763,0.8617623448371887
https://openreview.net/forum?id=SkpSlKIel,"This paper shows:----------------  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.--------  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.--------  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.----------------The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance.","The paper makes a solid technical contribution in proving that the deep networks are exponentially more efficient in function approximation compared to the shallow networks. They take the case of piecewise smooth networks, which is practically motivated (e.g. images have edges with smooth regions), and analyze the size of both the deep and shallow networks required to approximate it to the same degree.    The reviewers recommend acceptance of the paper and I am happy to go with their recommendation.","The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures."," The paper is well written and easy to follow . The technical content, including the proofs in the Appendix, look correct . The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures .","------------- 1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units.----",0.3247863247863248,0.034782608695652174,0.18803418803418806,0.881725549697876,0.8502168655395508,0.8656845688819885
https://openreview.net/forum?id=HyAddcLge,"This paper was easy to read, the main idea was presented very clearly.----------------The main points of the paper (and my concerns are below) can be summarized as follows:--------1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a ""MPI_Reduce"" would be used (even if we wait for the slowest guy)?--------2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.--------3.they propose to take gradient from the first ""N"" workers out of ""N+b"" --------workers available. My concern here is that they focused only on the --------workers, but what if the ""parameter server"" will became to slow? What --------if the parameter server would be the bottleneck? How would you address --------this situation? But still if the number of nodes (N) is not large, and --------the deep DNN is used, I can imagine that the communciation will not --------take more than 30% of the run-time.------------------------My largest concern is with the experiments. Different batch size --------implies that different learning rate should be chosen, right? How did --------you tune the learning rates and other parameters for e.g. Figure 5 you --------provide some formulas in (A2) but clearly this can bias your Figures, --------right? meaning, that if you tune ""\gamma, \beta"" for each N, it could --------be somehow more representative? also it would be nicer if you run the --------experiment many times and then report average, best and worst case --------behaviour. because now it can be just coinsidence, right? ","Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.","This paper was easy to read, the main idea was presented very clearly. The main points of the paper (and my concerns are below) can be summarized as follows. synchronous algoriths suffer from some struggeling nodes, for which",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper was easy to read, the main idea was presented very clearly. The main points of the paper (and my concerns are below) can be summarized as follows. synchronous algoriths suffer from some struggeling nodes, for which"," Synchronous algorithms suffer from some struggeling nodes, for which the algorithm has to wait . From my own experience, this has never happened for me on e.g. Amazon EC2 cloud, however, it happens on our own","synchronous algorithms suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happened for me on e.g. Amazon EC2",0.22807017543859648,0.0,0.08771929824561402,0.8425487279891968,0.8339275121688843,0.8382159471511841
https://openreview.net/forum?id=HyAddcLge,"This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. ----------------My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:----------------- provide more experiments to show the performance with different efficiency distributions of learners.--------- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.","Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.","This paper proposed a synchronous parallel SGD by employing several backup machines. My main concern is that this approach is only suitable for some very specific scenario. If the efficiency of learners does not follow such distribution, I do not think that",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposed a synchronous parallel SGD by employing several backup machines. My main concern is that this approach is only suitable for some very specific scenario. If the efficiency of learners does not follow such distribution, I do not think that", This paper proposed a synchronous parallel SGD by employing several backup machines . The parameter server does not have to wait for the return from all machines to perform the update on the model . It sounds like a reasonable and straightforward idea .,", which reduces the synchronization overhead. It sounds like a reasonable and straightforward idea. --------------- My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (",0.288135593220339,0.0,0.1864406779661017,0.861748218536377,0.8547189831733704,0.8582192063331604
https://openreview.net/forum?id=HyAddcLge,"The paper claim that, when supported by a number of backup workers, synchronized-SGD --------actually works better than async-SGD. The paper first analyze the problem of staled updates--------in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the --------authors shows the effectiveness of the proposed method in applications to Inception Net--------and PixelCNN.----------------The idea is very simple, but in practice it can be quite useful in industry settings where --------adding some backup workders is not a big problem in cost. Nevertheless, I think the --------proposed solution is quite straightforward to come up with when we assume that --------each worker contains the full dataset and we have budge to add more workers. So, --------under this setting, it seems quite natural to have a better performance with the additional --------backup workers that avoid the staggering worker problem. And, with this assumtion I'm not --------sure if the proposed solution is solving difficult enough problem with novel enough idea. ----------------In the experiments, for fair comparison, I think the Async-SGD should also have a mechanism --------to cut off updates of too much staledness just as the proposed method ignores all the remaining --------updates after having N updates. For example, one can measure the average time spent to --------obtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD --------so that Async-SGD does not perform so poorly.","Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware.","The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than async-SGDs. The idea is very simple, but in practice it can be quite useful in industry settings.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than async-SGDs. The idea is very simple, but in practice it can be quite useful in industry settings."," The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than Async-SGDs . The idea is very simple, but in practice it can be quite useful in industry settings .","--------in Async-SGD --------actually works better than async-SGD. The paper claim that, when supported by backup workers, synchronized-SGD --------",0.23423423423423423,0.01834862385321101,0.14414414414414414,0.8829585313796997,0.8583839535713196,0.8704978227615356
https://openreview.net/forum?id=BkfiXiUlg,"This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.  This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.----------------The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.  Improving on these would greatly strengthen the paper, as the core idea is interesting.",All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.,"This paper introduces a novel hierarchical memory architecture for neural networks. It is based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper introduces a novel hierarchical memory architecture for neural networks. It is based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks"," This paper introduces a novel hierarchical memory architecture for neural networks . It is based on a binary tree with leaves corresponding to memory cells . This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks",", and the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length. This paper introduce a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to",0.1492537313432836,0.030769230769230767,0.059701492537313425,0.821639358997345,0.8412675261497498,0.831337571144104
https://openreview.net/forum?id=BkfiXiUlg,"This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN…).----------------The model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...----------------The main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log(N)). It would be great to see if the gain in complexity allows to tackle problem which are a few orders of magnitude bigger than the one addressed with full softmax. However the authors only test on toy sequences up to 32 tokens, which is quite small. ----------------The model requires a relatively complex search mechanism that can only be trained with REINFORCE. While this seems to work on problems with relatively small and simple sequences, it would be great to see how performance changes with the size of the problem. ----------------Overall, while the idea of replacing the softmax in the attention mechanism by a hierachical softmax is appealing, this work is not quite convincing yet. Their approach is not very natural, may be hard to train and may not be that simple to scale. The experiment section is very weak.",All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.,"This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN…) The main advantage of replacing the full softmax is that during inference, the",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN…) The main advantage of replacing the full softmax is that during inference, the"," This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN…) In experimental section they test their approach on several algorithmic tasks such as search","a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN...).----------------The model requires a relatively complex search mechanism",0.1639344262295082,0.03389830508474576,0.09836065573770492,0.8146547079086304,0.8402253985404968,0.8272425532341003
https://openreview.net/forum?id=BkfiXiUlg,"The authors introduce a new memory model which allows memory access in O(log n) time.----------------Pros:--------* The paper is well written and everything is clear.--------* It's a new model and I'm not aware of a similar model.--------* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.----------------Cons:--------* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.--------* The model was also not tested on any real-world task.----------------I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.",All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.,The authors introduce a new memory model which allows memory access in O(log n) time. It's clear that memory access time is an issue for longer sequences.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors introduce a new memory model which allows memory access in O(log n) time. It's clear that memory access time is an issue for longer sequences., The authors introduce a new memory model which allows memory access in O(log n) time . The motivation for the model is to be able to use the model on very long sequences . The model was also not tested on any real-,a new memory model which allows memory access in O(log n) time.-------------------Pros:----------- The paper is well written and everything is clear.-------------,0.11111111111111112,0.03846153846153846,0.11111111111111112,0.8435045480728149,0.8508267998695374,0.8471498489379883
https://openreview.net/forum?id=S1OufnIlx,"The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. ----------------Pros:--------1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.--------2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. ----------------Cons:--------1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are still small.--------2. Some hypotheses proposed in the paper based on one-shot experiments seems too rushy.--------3. As mentioned above, the results of this paper seems not really improving the understanding of the adversarial example phenomenon.","This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device.     The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions.     In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track.","The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge."," The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples . Some observations of the experiments are interesting, but overall photo transformation does not affect accuracy on clean image .","is that this paper tends to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration (the real pictures), it is arguable that the perturbations in the experiments are",0.16589861751152074,0.037209302325581395,0.11059907834101383,0.8605825304985046,0.8209965825080872,0.8403236269950867
https://openreview.net/forum?id=S1OufnIlx,"In some sense, the Sharif et al. work ""scooped"" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study.","This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device.     The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions.     In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track.","In some sense, the Sharif et al. work ""scooped"" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","In some sense, the Sharif et al. work ""scooped"" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited", Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns .,is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. The present work is less sensational and more methodical in its study of physical manifestations of adversarial,0.1461187214611872,0.04608294930875576,0.091324200913242,0.8468081951141357,0.8172380924224854,0.8317604064941406
https://openreview.net/forum?id=S1OufnIlx,"Description.--------The paper investigates whether adversarial examples survive different geometric and photometric image transformations,--------including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera.--------The paper considers three different methods to generate adversarial examples — images with added small amount of noise that changes the output of a classification neural network.  In the quantitative experiments the paper assumes available access to the neural network and its parameters. Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network.   ----------------Strong  points.--------- adversarial examples are an interesting phenomenon that is worth detailed investigation.--------- the paper is well written and presented.--------- Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting.--------- Experiments are well done and solid.----------------Weak points:--------- Probably the main negative point is the amount of novelty and contribution. The paper essentially presents a set of experiments evaluating whether adversarial examples survive different image transformations. Apart from that there is no other main contribution / novelty. While the experiments are solid and well-done, this seems borderline.----------------Detailed evaluation.--------Originality:--------- the main contribution of this work is the experimental evaluation showing (and quantifying) how adversarial examples behave under various image transformations.  ----------------Quality:--------- The shown experiments are solid and well done.----------------Clarity:--------- The paper is well written and clear.----------------Significance:--------- The findings and shown experiments are interesting, but I not sure if the scale and amount of contribution is significant enough for the main conference track. ----------------Overall:--------Experimental paper. Well written. Solid experiments. Not sure if contribution is significant enough.","This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device.     The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions.     In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track.",The paper considers three different methods to generate adversarial examples. Images with added small amount of noise that changes the output of a classification neural network.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper considers three different methods to generate adversarial examples. Images with added small amount of noise that changes the output of a classification neural network.," The paper investigates whether adversarial examples survive different geometric and photometric image transformations, including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera .","image transformations,--------Including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera.----------The paper investigates whether adversarial examples survive different image",0.10576923076923077,0.019417475728155338,0.08653846153846154,0.8685286641120911,0.8165082931518555,0.8417155146598816
https://openreview.net/forum?id=BJ46w6Ule,"The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published.",This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.,The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. The related work section does not exist and instead the analysis of the literature,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. The related work section does not exist and instead the analysis of the literature, The paper addresses the problem of learning compact binary data representations . I have a hard time understanding the setting and the writing of the paper is not making it any easier .,. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these lines of research.,0.25333333333333335,0.040540540540540536,0.16,0.8610669374465942,0.8325693011283875,0.8465784192085266
https://openreview.net/forum?id=BJ46w6Ule,"The goal of this paper is to learn “ a collection of experts that are individually--------meaningful and that have disjoint responsibilities.” Unlike a standard mixture model, they “use a different mixture for each dimension d.” While the results seem promising, the paper exposition needs significant improvement.----------------Comments:----------------The paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.----------------The subsequent exposition is not very clear. There are assertions made with no justification, e.g. “the experts only have a small variance for some subset of the variables while the variance of the other variables is large.” ----------------Since you’re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.----------------The horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize.",This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.,"The goal of this paper is to learn “a collection of experts that are individually--------meaningful and that have disjoint responsibilities” Unlike a standard mixture model, they “use a different mixture for each dimension d�",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The goal of this paper is to learn “a collection of experts that are individually--------meaningful and that have disjoint responsibilities” Unlike a standard mixture model, they “use a different mixture for each dimension d�"," The goal of this paper is to learn “a collection of experts that are individually--------meaningful and that have disjoint responsibilities” Unlike a standard mixture model, they “use a different mixture for each dimension d�",---------------The paper jumps in with no motivation at all.----------------The paper jumps in with no motivation at all.----------------The subsequent exposition is not very clear.,0.2,0.028985507246376815,0.12857142857142856,0.8197759985923767,0.8194025754928589,0.8195893168449402
https://openreview.net/forum?id=BJ46w6Ule,"This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.--------I find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed “EM-like” algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?--------We also note that the “product of unifac models” from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: http://www.cs.toronto.edu/~hinton/absps/tr00-004.pdf--------I tried to derive the update rule on top of page 4 from the “conditional objective for p(x|h)” in sec. 3.2 But I am getting something different (apart form the extra smoothing factors eps and mu_o). Does this follow? (If we define R=R_nk, mu-mu_k and X=X_n, I get mu = (XR)*inv(R^TR) as the optimal solution, which then needs to be projected back onto the probability simplex).--------The experiments are only illustrative. They don’t compare with other methods (such as an RBM or VAE) nor do they give any quantitative results. We are left with eyeballing some images. I have no idea whether what we see is impressive or not. ",This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.,The experiments are only illustrative. They don't compare with other methods (such as an RBM or VAE) nor do they give any quantitative results. We are left with eyeballing some images. I have no idea whether what,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The experiments are only illustrative. They don't compare with other methods (such as an RBM or VAE) nor do they give any quantitative results. We are left with eyeballing some images. I have no idea whether what, This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE . The experiments are only illustrative. They don't compare with other,a new type of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.--------I find the paper very unclear.,0.0979020979020979,0.0,0.06993006993006994,0.8346761465072632,0.8193295001983643,0.8269315958023071
https://openreview.net/forum?id=HkzuKpLgg,"This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. ----------------Comments:----------------- The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation.  ----------------- The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. ----------------- The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by 2x and log p  times. More specifically, Eq (2) shows T_broadcase_BE/ T_broadcase_LP < 2. This does not provide an upper-bound of T_broadcase_LP and it can be arbitrary worse when comparing with T_broadcase_BE from this inequality. Therefore, instead of showing T_broadcase_BE/ T_broadcase_LP < 2, the authors should state T_broadcase_BE/ T_broadcase_LP > 1 when n approaches infinity. ----------------- It would be interesting to emphasize more on the differences between designing parallel algorithms on CPU v.s. on GPU to motivate the paper. ","The authors propose improvements for the utilization of modern hardware when training using stochastic gradient. However, the reviewers bring up several issues with the paper, including major clarity issues as well as notational issues and some comments about the theory vs. practice.","This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved."," This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU . The paper provides both theoretical analysis and experiments . Overall, the results presented in the paper are interesting, but the writing can be improved .",". ----------------- The paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but",0.19753086419753083,0.02531645569620253,0.14814814814814817,0.8732192516326904,0.8640096783638,0.8685900568962097
https://openreview.net/forum?id=HkzuKpLgg,"This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.--------Comments--------1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature. The author should use the standard name to make the connection easier. --------2) The cost analysis of ring-based Allreduce is already provided in the existing literature. This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.--------3) The ring-based allreduce approach is already supported by NVidia’s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.--------4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop of t-1 while doing reduce.  Note that the dependency pattern can be further exploited; with the forward of layer t depend on update of parameter of layer t in last iteration. This can be done by a dependency scheduler. --------5) Since this paper is about analysis of Allreduce, it would be nice to include detailed analysis of tree-shape reduction, ring-based approach and all-to-all approach. The discussion of all-to-all approach is missing in the current paper. --------In summary, this is a paper discussed existing Allreduce techniques for data parallel multi-GPU training of deep net, with cost analysis based on existing results. While I personally find the claimed result not surprising as it follows from existing analysis of Allreduce, the analysis might help some other readers. I view this as a baseline paper. The analysis of Allreduce could also been improved (see comment 5).","The authors propose improvements for the utilization of modern hardware when training using stochastic gradient. However, the reviewers bring up several issues with the paper, including major clarity issues as well as notational issues and some comments about the theory vs. practice.",This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. The authors claim that their implementation comes earlier than the NCCL implementation.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. The authors claim that their implementation comes earlier than the NCCL implementation.," This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net . The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allred",. -----------This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. -----------Comments--------1) The name linear pipeline is somewhat confusing,0.22535211267605634,0.028985507246376812,0.1408450704225352,0.8615278601646423,0.8616704940795898,0.8615991473197937
https://openreview.net/forum?id=Bk8N0RLxx,"This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.----------------A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.----------------The experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.----------------A major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid, the lack of originality lets it down.----------------Minor comments: in 2.1, the word co-occurence measure - was any smoothing used to make this measure more robust to low counts?","The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough.","This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation. A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation. A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use"," This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation . A range of techniques are investigated, ranging from simple methods such as word co-occurences, to the relatively complex use of","of vocabulary selection strategies to reduce computational cost of neural machine translation.-------------------A major criticism I have with this paper is that there is little novelty here. The techniques are mostly standard methods and very simple, and in",0.12903225806451615,0.0,0.12903225806451615,0.8280348181724548,0.8432377576828003,0.8355671763420105
https://openreview.net/forum?id=Bk8N0RLxx,"This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.----------------My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.----------------Minor comments:--------In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.----------------It would be useful to know what the coverage rate of the actual full vocabulary would be (rather than the 100k “full vocabulary”). Since presumably this technique could be used to work with much larger vocabularies.----------------When reducing the vocabulary size for training, the Mi et al. (2016) technique of taking the union of all the vocabularies in a mini batch seems like a rather strange objective. If the vocabulary of a single sentence is used, the probabilistic semantics of the translation model can still be preserved since p(e | f, vocab(f)) = p(e | f) if p(vocab(f) | f) = 1, i.e., is deterministic, which it is here. Whereas the objective is no longer a sensible probability model in the mini batch vocabulary case. Thus, while it may be a bit more difficult to implement, it seems like it would at least be a sensible comparison to make.","The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough.",This paper compares several strategies for guessing a short list of vocabulary for the target language. The primary findings are that word alignment dictionaries work better than a variety of other techniques.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper compares several strategies for guessing a short list of vocabulary for the target language. The primary findings are that word alignment dictionaries work better than a variety of other techniques., This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation . The primary findings are that word alignment dictionaries work better than other techniques .,lexicon of Mauser et al. (2009) and the neural version of Ha et al. (2014) are also worth mentioning.-------------This paper compares several strategies for guessing a short,0.1509433962264151,0.0,0.07547169811320754,0.8542404174804688,0.8481907248497009,0.8512047529220581
https://openreview.net/forum?id=Bk8N0RLxx,"In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue.","The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough.","In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks."," In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence . The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks . However","the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2se",0.0967741935483871,0.0,0.0967741935483871,0.8480149507522583,0.8565222024917603,0.8522473573684692
https://openreview.net/forum?id=Bk8N0RLxx,"This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.----------------- Do the reported decoding times take into account the vocabulary reduction step?--------- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?--------- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem.","The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough.","This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks", This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training . The proposed methods are helpful because of the difficulties induced by using a word-level model . Starting from a character or even lower-,"is a useful resource for practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.-----------",0.09523809523809523,0.0,0.09523809523809523,0.8483145833015442,0.8589138388633728,0.8535813093185425
https://openreview.net/forum?id=ryuxYmvel,"Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. ----------------I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2):--------- Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow.  --------- Section 2 describes how the data was splits into train and test set. One thing which is unclear is – can the examples in the train and test set be statements about the same conjecture or are they always statements about different conjectures? ------------------------It also unclear how the deep learning models are applied. Let’s consider the leftmost architecture in Figure 1. Each character is embedded into 256-D vector – and processed until the global max-pooling layer. Does this layer take a max along each feature and across all characters in the input? ----------------My another concern is only deep learning methods are presented as baselines. It would be great to compare with standard NLP techniques such as Bag of Words followed by SVM. I am sure these would be outperformed by neural networks, but the numbers would give a sense of how easy/hard the current problem setup is. ----------------Did the authors look at the success and failure cases of the algorithm? Are there any insights that can be drawn from such analysis that can inform design of future models? ----------------Overall I think the research direction of using ML for theorem proving is an interesting one. However, I also feel the paper is quite opaque. Many parts of how the data is constructed is unclear (atleast to someone with little knowledge in ITPs). If authors can revise the text to make it clearer – it would be great. The baseline models seem to perform quite well, however there are no insights into what kind of ability the models are lacking. Authors mention that they are unable to perform logical reasoning – but that’s a very vague statement. Some examples of mistakes might help make the message clearer. Further, since I am not well versed with the ITP literature it’s not possible for me to judge how valuable is this dataset. From the references, it seems like it’s drawn from a set of benchmark conjectures/proofs used in the ITP community – so its possibly a good dataset. ----------------My current rating is a weak reject, but if the authors address my concerns I would change to an accept.","The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.    As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no ""overlap"" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?",Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a, Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not . This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines . Many parts of how,". ------------I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. The paper is quite opaque. Many parts of how",0.24444444444444446,0.0449438202247191,0.14444444444444443,0.8523927927017212,0.8077110052108765,0.8294506072998047
https://openreview.net/forum?id=ryuxYmvel,"The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.","The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.    As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no ""overlap"" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?","The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification"," The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving . The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification","for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The authors present a dataset extraction method, dataset and first interesting results for machine-",0.25842696629213485,0.0909090909090909,0.20224719101123595,0.8708664178848267,0.8221465349197388,0.8458054661750793
https://openreview.net/forum?id=ryuxYmvel,"The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.----------------It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.","The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.    As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no ""overlap"" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?","The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data is",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data is"," The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems . The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data is",. The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The authors describe a dataset of proof steps in higher order logic derived from a set of,0.26136363636363635,0.022988505747126433,0.1590909090909091,0.856920063495636,0.8197863101959229,0.8379420042037964
https://openreview.net/forum?id=ByOK0rwlx,"This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights.","The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.",This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results., This paper addresses to reduce test-time computational load of DNNs . Another factorization approach is proposed and shows good results .,This paper addresses to reduce test-time computational load of DNNs. A factorization approach is proposed and shows good results.,0.1754385964912281,0.0,0.14035087719298245,0.8904715776443481,0.8682918548583984,0.8792418837547302
https://openreview.net/forum?id=ByOK0rwlx,"I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations.","The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.",I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. Some of this results are feasible with float representation but probably imposible for restricted representations.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. Some of this results are feasible with float representation but probably imposible for restricted representations., i would like to see the results when the compression is applied to state of the art nets where the float representation is important . For instance a network with 0.5% - 0.8% in MNIST. A Imagenet,"the results in clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation",0.26666666666666666,0.0,0.16,0.8430312275886536,0.8486694097518921,0.8458408713340759
https://openreview.net/forum?id=ByOK0rwlx,"This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.----------------My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the given method - other then possibly reducing the time it takes to compress the network - is thus unclear. In particular, taking a pre-trained network as a starting point for a quantized model that is subsequently fine-tuned might not take much longer to process then the method given here (but maybe the authors can quantify this?). Finally, much of the speed-up and memory reduction in the VGG-model seems to arise from the three fully-connected layers, in particular the last one. The speed-up in the convolutional layers is comparably small, making me wonder how well the method would work in all-convolutional networks such as the Inception architecture.----------------[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, https://arxiv.org/abs/1510.00149--------[2] Compressing Deep Convolutional Networks using Vector Quantization, https://arxiv.org/abs/1412.6115--------[3] XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, https://arxiv.org/abs/1603.05279","The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance.",This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x., This paper explores a new quantization method for both the weights and the activations that does not need re-training . In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x .,"-----------[1] Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding, https://arxiv.org/abs/151",0.40540540540540543,0.08333333333333333,0.2702702702702703,0.8700576424598694,0.8930675983428955,0.8814125061035156
https://openreview.net/forum?id=SyCSsUDee,"The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.----------------The derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.----------------Is \sigma in Equation 8 trainable parameter or hyperparameter? If it is trainable how it is trained? If it is not, how are they set? Does j correspond to one of the class? The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons. That being said, the proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers. In addition, there is a missing reference (DisturbLabel: Regularizing CNN on the Loss Layer, CVPR 2016) that applied synthetic noise process on the loss layer.----------------Experiments should be done for multiple times with different random subsets and authors should provide mean and standard error. Overall, I believe the proposed method is not very well justified and has limited novelty. ","The reviewers all expressed concerns with the technical quality of this work. In particular, the reviewers are concerned that ignoring certain entropy terms in the objective is problematic and would require significantly more justification theoretically and empirically. The reviewers believe that the authors had to resort to unjustified tricks such as adding noise in order to compensate for the missing terms in the objective. Some of the reviewers also had concerns with the choice of experiments, expressing that the authors did not choose the right baseline comparisons to compare to (e.g. convolutional networks vs. fully connected networks on MNIST). Hopefully the thorough feedback and lengthly discussion, along with the authors' responses (both in the text and additions to the paper and appendix), will lead to a stronger submission to a future conference.",The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training, The paper introduces supervised deep learning with layer-wise reconstruction loss and class-conditional semantic additive noise . The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons .,"is not really well justified.----------------The proposed method is not different from gaussian dropout (Wang and Manning, ICML 2013) but applied on different layers.-------------",0.17751479289940827,0.04790419161676646,0.1301775147928994,0.8447359204292297,0.8219485282897949,0.8331864476203918
https://openreview.net/forum?id=SyCSsUDee,"The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is achieved by defining a measure of total correlation between these variables and decomposing it in terms of entropies and conditional entropies.----------------Authors explain that they do not actually maximize the total correlation, but a lower-bound of it that ignores simple entropy terms, and only considers conditional entropies. It is not clearly explained what is the rationale for discarding these entropy terms.----------------Entropies measures are applying to probability distributions (i.e. this implies that the variables in the model should be random). The link between the conditional entropy formulation and the reconstruction error is not made explicit. In order to link these two views, I would have expected, for example, a noise model for the units of the network.----------------Later in the paper, it is claimed that the original ladder network is not suitable for supervised learning with small samples, and some empirical results seek to demonstrate this. But a more theoretical explanation why it is the case would have been welcome.----------------The MNIST results are shown for a particular convolutional neural network architecture, however, most ladder network results for this dataset have been produced on standard fully-connected architectures. Results for such neural network architecture would have been desirable for more comparability with original ladder neural network results.","The reviewers all expressed concerns with the technical quality of this work. In particular, the reviewers are concerned that ignoring certain entropy terms in the objective is problematic and would require significantly more justification theoretically and empirically. The reviewers believe that the authors had to resort to unjustified tricks such as adding noise in order to compensate for the missing terms in the objective. Some of the reviewers also had concerns with the choice of experiments, expressing that the authors did not choose the right baseline comparisons to compare to (e.g. convolutional networks vs. fully connected networks on MNIST). Hopefully the thorough feedback and lengthly discussion, along with the authors' responses (both in the text and additions to the paper and appendix), will lead to a stronger submission to a future conference.","The paper presents a new regularization technique for neural networks. It seeks to maximize correlation between input variables, latent variables and outputs. The link between the conditional entropy formulation and the reconstruction error is not made explicit.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents a new regularization technique for neural networks. It seeks to maximize correlation between input variables, latent variables and outputs. The link between the conditional entropy formulation and the reconstruction error is not made explicit."," The paper presents a new regularization technique for neural networks . It seeks to maximize correlation between input variables, latent variables and outputs . It is not clearly explained what is the rationale for discarding these entropy terms .","for a particular convolutional neural network architecture.---------------The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is",0.16470588235294117,0.011904761904761904,0.09411764705882353,0.8564904928207397,0.8249374032020569,0.8404178023338318
https://openreview.net/forum?id=SyCSsUDee,"This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.----------------Technical issues:----------------The move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.----------------In (3) arbitrary balancing paramters lamda_1 and lambda_2 are introduced ex-nihilo - they were not there in (2). This is not ever justified.----------------Then in (5), a further choice is made by simply adding L_{NLL} to the objective. But in the supervised case, the targets are known and so turn up in H(Z|Y). Hence now H(Z|Y) should be conditioned on the targets. However instead another objective is added again without justification, and the conditional entropy of Z is left disconnected from the data it is to be conditioned on. One might argue the C(X,Y,Z) simply acts as a prior on the networks (and hence implicitly on the weights) that we consider, which is then combined with a likelihood term, but this case is not made. In fact there is no explicit probabilistic or information theoretic motivation for the chosen objective.----------------Given these issues, it is then not too surprising that some further things need to be done, such as semantic noise addition to actually get things working properly. It may be the form of noise addition is a good idea, but given the troublesome objective being used in the first place, it is very hard to draw conclusions.----------------In summary, substantially better theoretical justification of the chosen model is needed, before any reasonable conclusion on the semantic noise modelling can be made.","The reviewers all expressed concerns with the technical quality of this work. In particular, the reviewers are concerned that ignoring certain entropy terms in the objective is problematic and would require significantly more justification theoretically and empirically. The reviewers believe that the authors had to resort to unjustified tricks such as adding noise in order to compensate for the missing terms in the objective. Some of the reviewers also had concerns with the choice of experiments, expressing that the authors did not choose the right baseline comparisons to compare to (e.g. convolutional networks vs. fully connected networks on MNIST). Hopefully the thorough feedback and lengthly discussion, along with the authors' responses (both in the text and additions to the paper and appendix), will lead to a stronger submission to a future conference.","This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. The move from (1) to (2) is problematic. There is no explicit probabilistic or information theoretic motivation for the",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. The move from (1) to (2) is problematic. There is no explicit probabilistic or information theoretic motivation for the"," This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations . It may be a good idea, but given the troublesome objective being used in the first place, it is hard to draw conclusions .","is not justified. This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.-------------Technical issues:-------------The move from (1) to",0.15476190476190477,0.024096385542168676,0.08333333333333333,0.8471050262451172,0.8295502066612244,0.8382357358932495
https://openreview.net/forum?id=HkwoSDPgg,"This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing.","The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.","This paper addresses the problem of achieving differential privacy in a very general scenario. I found the approach altogether plausible and very clearly explained by the authors. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper addresses the problem of achieving differential privacy in a very general scenario. I found the approach altogether plausible and very clearly explained by the authors. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes", A paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data . A simple idea of adding perturbation error to the counts is nicely re-,"of Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much",0.2826086956521739,0.0,0.1956521739130435,0.8648118376731873,0.848554253578186,0.8566059470176697
https://openreview.net/forum?id=HkwoSDPgg,"This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. ----------------The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.----------------The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. ","The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.",This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model. The probabilistic bound has quite a number of  empirical parameters.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model. The probabilistic bound has quite a number of  empirical parameters., This paper discusses how to guarantee privacy for training data . The proposed approach may be mostly useful for sensitive data like medical histories .,". This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as teachers'' model, which will train a student'",0.23529411764705882,0.0,0.1411764705882353,0.8579415082931519,0.845413088798523,0.851631224155426
https://openreview.net/forum?id=HkwoSDPgg,"Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.----------------One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and/or clarify what can be generalized about the observed learning performance.----------------Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in order to have a meaningful comparison to the epsilon-delta values reported in related work.----------------Finally, as has been acknowledged in the paper, the present approach may not work on other natural data types. Experiments on other data sets is strongly encouraged. Also, please cite the data sets used.----------------Other comments:----------------Discussion of certain parts of the related work are thorough. However, please add some survey/discussion of the related work on differentially-private semi-supervised learning. For example, in the context of random forests, the following paper also proposed differentially-private semi-supervised learning via a teacher-learner approach (although not denoted as “teacher-learner”). The only time the private labeled data is used is when learning the “primary ensemble.”  A ""secondary ensemble"" is then learned only from the unlabeled (non-private) data, with pseudo-labels generated by the primary ensemble.----------------G. Jagannathan, C. Monteleoni, and K. Pillaipakkamnatt: A Semi-Supervised Learning Approach to Differential Privacy. Proc. 2013 IEEE International Conference on Data Mining Workshops, IEEE Workshop on Privacy Aspects of Data Mining (PADM), 2013.----------------Section C. does a nice comparison of approaches. Please make sure the quantitative results here constitute an apples-to-apples comparison with the GAN results. ----------------The paper is extremely well-written, for the most part. Some places needing clarification include:--------- Last paragraph of 3.1. “all teachers….get the same training data….” This should be rephrased to make it clear that it is not the same w.r.t. all the teachers, but w.r.t. the same teacher on the neighboring database.--------- 4.1: The authors state: “The number n of teachers is limited by a trade-off between the classification task’s complexity and the available data.” However, since this tradeoff is not formalized, the statement is imprecise. In particular, if the analysis is done in the i.i.d. setting, the tradeoff would also likely depend on the relation of the target hypothesis to the data distribution.--------- Discussion of figure 3 was rather unclear in the text and caption and should be revised for clarity. In the text section, at first the explanation seems to imply that a larger gap is better (as is also indicated in the caption). However later it is stated that the gap stays under 20%. These sentences seem contradictory, which is likely not what was intended.","The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written.","The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. Although the approach is intended to be general, no theoretical guarantees are provided about the learning performance.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough. Although the approach is intended to be general, no theoretical guarantees are provided about the learning performance."," The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough . No theoretical guarantees are provided about the learning performance . Future experimentation is encouraged, even using the same data sets",".------------The paper advances the state of the art on differentially-private semi-supervised learning, is quite well-written, and relatively thorough.---------The paper is extremely well-written, for the most part",0.4367816091954023,0.07058823529411765,0.20689655172413796,0.8798025250434875,0.8553800582885742,0.8674194812774658
https://openreview.net/forum?id=HyAbMKwxe,"The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. --------Consequently, the optimized upper bound (log-loss) gets looser. ----------------As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. ----------------I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. ----------------There is a large body of work on directly optimizing task losses[2][3] and the references therein. These should also be discussed and related particularly to section 3 (optimizing the ROC curve).----------------[1] Training Highly Multiclass Classifiers, Gupta et al. 2014.--------[2] Direct Loss Minimization for Structured Prediction, McAllester et al. --------[3] Generalization Bounds and Consistency for Latent Structural Probit and Ramp Loss, McAllester and Keshet.----------------Final comment:--------I believe the material presented in this paper is of interest to a wide audience at ICLR.--------The problem studied is interesting and the proposed approach is sound. --------I recommend to accept the paper and increase my score (from 7 to 8). ","Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail.","The paper analyses the misclassification error of discriminators. It highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper analyses the misclassification error of discriminators. It highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly."," The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values","based on recomputing the bound is proposed. The paper analyses the misclassification error of discriminators and highlights the fact that uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from",0.14666666666666664,0.013513513513513513,0.12000000000000001,0.8218223452568054,0.8046190142631531,0.8131296634674072
https://openreview.net/forum?id=HyAbMKwxe,"The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.----------------The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an ""uncertain"" label. ----------------While the paper is easy to read and well-written overall, in a second read I found it difficult to fully understand because two problems are somewhat mixed together (here considering only binary classification for simplicity): --------(a) the optimization of the classification error of a *randomized* classifier, which predicts 1 with probability P(1|x, theta), and --------(b) the optimization of the deterministic classifier, which predicts sign(P(1|x, theta) - 0.5), in a way that is robust to outliers/underfitting. ----------------The reason why I am confused is that ""The standard approach to supervised classification"", as is mentioned in the abstract, is to use deterministic classifiers at test time, and the log-loss (up to constants) is an upper bound on the classification error of the deterministic classifier. However, the bounds discussed in the paper only concern the randomized classifier.----------------=== question:--------In the experiments, what kind of classifier is used? The randomized one (as would the sentence in the first page suggest ""Assuming the class is chosen according to p(y|X, θ)""), or the more standard deterministic classifier argmax_y P(y|x, theta) ?----------------As far as I can see, there are two cases: either (i) the paper deals with learning randomized classifiers, in which case it should compare the performances with the deterministic counterparts that people use in practice, or (ii) the paper makes sense as soon as we accept that the optimization of criterion (a) is a good surrogate for (b). In both cases,  I think the write-up should be made clearer (because in case (ii) the algorithm does not minimize an upper bound on the classification error, and in case (i) what is done does not correspond to what is usually done in binary classification). ----------------=== comments:--------- The section ""allowing uncertainty in the decision"" may be improved by adding some references, e.g. Bartlett & Wegkamp (2008) ""Classification with a Reject Option using a Hinge Loss"" or Sayedi et al. (2010) ""Trading off Mistakes and Don’t Know Predictions"".----------------- there seems to be a ""-"" sign missing in the P(1|x, theta) in L(theta, lambda) in Section 3.----------------- The idea presented in the paper is interesting and original. While I give a relatively low score for now, I am willing to increase this score if the clarifications are made.----------------Final comments:--------I think the paper is clear enough in its current form, even though there should still be improvement in the justification of why and to what extent the error of the randomized classifier is a good surrogate for the error of the true classifier. While the ""smoothed"" version of the 0/1 loss is an acceptable explanation in the standard classification setup, it is less clear in the section dealing with an additional ""uncertain"" label. I increase my score from 5 to 6.","Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail.",The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function. Experiments are performed to confirm the therotical intuition and motivation.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function. Experiments are performed to confirm the therotical intuition and motivation., The paper proposes new bounds on the misclassification error . The bounds lead to training classifiers with an adaptive loss function . The bound improves on standard log-likelihood when outliers/underfitting prevents the learning algorithm to properly optimize,"The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function. The bounds lead to training classifiers with an adaptive loss function, and the algorithm",0.12244897959183675,0.0,0.0816326530612245,0.819085419178009,0.8011726140975952,0.810029923915863
https://openreview.net/forum?id=HyAbMKwxe,"The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.----------------An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend for identifying the different curves in the figures, which makes difficult to appreciate the results.","Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail.",The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the, The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers . Extensions of this idea are developed for regularized losses and a weak form of policy learning . The idea looks fine and some of the results seem,. The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional max. log likelihood is an upper bound of the Bayes error which becomes lousy,0.16455696202531644,0.0,0.11392405063291139,0.8274471759796143,0.805966317653656,0.8165654540061951
https://openreview.net/forum?id=SJGPL9Dex,"This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below.----------------It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. ----------------Minor comments:----------------- E(z_k) in (3) and (4) are not defined.----------------- E_x in (19) is not defined.----------------- Forward referencing (“Equation (20) defines…”) in the paragraph above Theorem 2.2. needs to be corrected.","The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems.","This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. Based on the results, the authors proposed a reparametrization approach for the neural network architecture. My comments are listed below.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. Based on the results, the authors proposed a reparametrization approach for the neural network architecture. My comments are listed below."," This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks . The neural networks are generated by unfolding the ISTA/FISTA iterations . Based on the results, the authors proposed a reparametrization",.-------------Minor comments:---------------- E(z_k) in (3) and (4) are not defined.-------------- E_x in (19) is not defined.-,0.27956989247311825,0.021978021978021976,0.12903225806451615,0.8658431768417358,0.8607814908027649,0.8633049130439758
https://openreview.net/forum?id=SJGPL9Dex,"This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The authors here propose a solid analysis of the acceleration performance of LISTA, using a specific matrix factorisation of the dictionary. ----------------The analysis is well structured, and provides interesting insights. It would have been good to tie more closely these insights to specific properties of data or input distributions.----------------The learned dictionary results in Section 3.3 are not very clear: is the dictionary learned with a sort of alternating minimisation strategy that would include LISTA as sparse coding step? Or is it only the sparse coding that is studied, with a dictionary that has been learned a priori?----------------Overall, the paper does not propose a new algorithm and representation, but provides key insights on a well-known and interesting acceleration method on sparse coding. This is quite a nice work. The title seems however a bit confusing as 'neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding' - basically, it is not immediate to understand what 'neural sparse coding' means...","The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems.","This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The analysis is well structured, and provides interesting insights.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The analysis is well structured, and provides interesting insights."," This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem . It would have been good to tie more closely these insights to specific properties of data or input distributions","'neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding' - basically, it is not immediate to understand what '",0.3777777777777777,0.06818181818181818,0.2222222222222222,0.8885999917984009,0.8707970380783081,0.8796084523200989
https://openreview.net/forum?id=SJGPL9Dex,"This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).----------------FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.----------------Overall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. ----------------Minor comments/typos:--------- p. 6: ""memory taps"" -> tapes?--------- sec 3.2: ""a gap appears has the number of iterations increases"" -> as?--------- sec. 4: ""numerical experiments of 3"" -> of sec 3","The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems.","FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. It is not clear what is the advantage of using FacNet instead of LisTA.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. It is not clear what is the advantage of using FacNet instead of LisTA."," This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010) It is not clear what is the advantage of using FacNet instead of FacNet, up to some optimization errors .",a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). This paper propose a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010,0.2247191011235955,0.0,0.15730337078651685,0.8540265560150146,0.8492580056190491,0.8516356348991394
https://openreview.net/forum?id=SJDaqqveg,"This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. --------In particular, experiments are shown in a synthetic denoising task as well as in machine translation. ----------------I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different?--------If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I'd like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline.----------------The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. ----------------While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update \phi' with interpolation, penalize the variance, reducing the value of rare actions, etc. --------Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. --------It seems that the authors need more work to make the model work without so many heuristics.----------------The authors also mentioned several optimization difficulties (some of which are non-intuitive), --------1) why does the critic assign very high value to actions with very low probability according to the actor?--------2) why is a lower square error on Q resulting in much worst performance?----------------The paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework.--------Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. ----------------The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this?----------------The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. ----------------I'll revise my score if the authors address my questions.----------------In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.","Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work.","This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. While the idea makes sense, the authors needed to use many heuristics to make the model to work.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. While the idea makes sense, the authors needed to use many heuristics to make the model to work."," This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing . I like the idea of the paper, however, the experimental evaluation is not convincing .",". --------------I like the idea of the paper, however, the experimental evaluation is not convincing.----------The paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in",0.29677419354838713,0.16993464052287582,0.27096774193548384,0.9022518396377563,0.8291321992874146,0.8641480207443237
https://openreview.net/forum?id=SJDaqqveg,"This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation.  While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs.  Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens.----------------The motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified. There are a number of details that are necessary for successful training, that are discussed well.  While the full Actor-Critic model does not show strong improvements over REINFORCE with critic, the critic-based models still outperform other baselines.  It would be nice to include more discussion of the bias-variance tradeoff and future advantages of Actor-Critic (from the pre-review question response) in the paper.  The paper is solid and deserves acceptance","Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work.","This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation. The paper is solid and deserves acceptance.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation. The paper is solid and deserves acceptance."," This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation . The paper is solid and deserves acceptance .","for sequence prediction. This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation. This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction",0.15384615384615385,0.0425531914893617,0.11188811188811192,0.8889745473861694,0.8151196837425232,0.8504467010498047
https://openreview.net/forum?id=SJDaqqveg,"The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. ----------------The interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be quite useful. ----------------The pros include reasonable improvement over prior attempts at using RL to fine-tune sequence models. One possible con is that the actor-critic is likely more unstable than simpler prior methods, thus requiring a number of tricks to alleviate, and it would be nice to see discussion on stability and hyper-parameter sensitivity. Another possible con is that this is an application paper, but it explores a non-traditional approach in a widely applicable field. ","Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work.","The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence."," The paper presents a nice application of actor-critic method for conditional sequence prediction . The critic is trained conditional to target sequence output, while the actor is conditional on input sequence . Letting the critic having access to information only available at","for applying RL in sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a nice application of actor-critic method for conditional sequence prediction.",0.28378378378378377,0.0821917808219178,0.18918918918918923,0.8943984508514404,0.8153916001319885,0.8530696630477905
https://openreview.net/forum?id=r1y1aawlg,"This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.----------------The paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several epochs.----------------This paper does not convince me on these points.  Indeed, unsurprisingly, the authors note that ""the probability of correctly labelling a word as a mistake remains low (62%)"" - this admittedly beats a random-chance baseline, but is not compared to something more meaningful, such as simply contrasting the existing system with a more powerful convolutional model and labelling all discrepancies as mistakes.  The oracle experiments are rather meaningless - they just serve to confirm that improving a translation is very easy when the existing mistakes have been identified, but much harder when they are not. ----------------Although I do like the paper on the whole, to really convince me that main objective -- ie. that **iterative** improvement is beneficial -- has been satifactorily demonstrated it would be necessary to include stronger baselines - and in particular, to show that an iterative refinement scheme can really improve over a system closely matched to the attention-based model, both when used in isolation and when used in system combination with a PBMT system, and to demonstrate that the PBMT system is not simply acting as a regulariser for the attention-based model.----------------Minor comments:----------------I find the notation excessively fiddly at times - eg F^i = (F^{i,1}, F^{i,|F^i|}) - why use |F^i| here when F is a matrix, so surely the length of the slice is not dependent on i?----------------In the discussion in section 4 - it seems that this still creates a mismatch between the training and test conditions - could anything be done about this?------------------------------------------------------------------------ ","The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.",This paper proposes a method for improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution. It is motivated by the method in which (it is assumed) human translators operate.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a method for improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution. It is motivated by the method in which (it is assumed) human translators operate.," This paper proposes a method for improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution . The authors note that ""the probability of correctly labelling a word as a mistake remains low (62%)""","a machine learning method is used to identify and correct predictions of another method, or itself.-------------------This paper proposes a method for iteratively improving the output of an existing machine translation ",0.24615384615384614,0.031746031746031744,0.15384615384615385,0.8606604337692261,0.8718082904815674,0.8661985397338867
https://openreview.net/forum?id=r1y1aawlg,"This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on “right context”, and potentially enabling more rapid and/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective. In this analysis, the model squarely back in the context of traditional discriminative translation models which used “undirected” features, and the decoding algorithm then looks more like a standard greedy hill-climbing algorithm (albeit with an extra heuristic model for selecting which variable to update), which is also nothing unfamiliar.----------------My second criticism the limitations of the model are not well discussed. For example, the proposed editing procedure cannot obviously remove or insert a word from a translation. While I think this is a reasonable assumption than can be made for the sake of tractability, it is very unfortunate since missing or extra words (esp. function words) are a common problem in the baseline models that are being used. Second, the standard objections to absolute positional models (vs. relative positional models) seem particularly crucial to bring up in this work, especially since they might make some of the design decisions a bit more justifiable.----------------Overall, this is an initial step in an interesting direction, but it needs more thorough analysis to demonstrate its value. A more thorough analysis will also likely suggest some important model variants (for example: is a global translation model really the goal? or is a post-editing model that fixes outputs with more complex operations more ideal?)----------------Related work:--------I think that more could be done to put this work in the context of what has come before and what is currently going on in other parts of ML. The idea of iterative refinement has been proposed in other problems that have complex output spaces, for example the DRAW model of Gregor et al. and the conditional adversarial network models used to refine images proposed recently by Isola et al. In NLP, there have been several (stochastic) hill climbing approaches that have been proposed, such as the work on parsing by Zhang and Lei et al. (2014) who use random initial guesses and then do greedy hill climbing using a series of local refinements, the structured prediction cascades of Weiss and Taskar (2009) (not to mention general coarse-to-fine modeling strategies). Finally, in MT, Arun et al. (2009) who use a Gibbs sampler to refine an initial guess to do decoding with a more complex model. The use of an explicit error model is rather novel in the context of correction, but I would point out that although the proposed architecture is different, the discriminative word lexicon models of Mauser et al. (2009) and the neural version of the same by Ha et al. (2014) are similar in spirit. There have also been a number of papers on “automatic post editing”, including the shared task at WMT2016, and there are not only standard test sets and baselines, but also datasets that could actually be used to train a post editing model with human-generated data. Minimally using the techniques they described could be a useful foil for the models presented in this paper.----------------“the target sentence is also embedded in distributional space via a lookup table” I think “distributional space” is a bit unclear. Maybe “the target sentence is represented in terms of distributed word representations via a lookup table” or something like that. “distributional” suggests that the representations are derived from how the words are distributed in the corpus, whereas you are learning these representations on this task which isn’t modeling their distribution except only very indirectly.----------------Section 3 Model: In Section 3, the model computes the distribution over target word types at an absolute position i in the output sentence, given the target language context and the source language context. It is introduced as the model that is used to refine an existing hypothesis, but it is not immediately clear that the training data for this model (at least in this section) are the gold standard translations- “training set” could be interpreted in variety of ways. This becomes clearer when reading later in the paper, but it’s a bit less clear when reading from the beginning for the first time.----------------The use of a fixed sized window for representing the target word in context also seems to make something like a model 1 assumption since only the lexical features (and not any “alignment” or “positional” features) determine the attention. This should be clarified since it will make the assumptions of the model more transparent (and also suggest possible refinements to the model, e.g., including (representations) of i and j as components of S^j and T^i, which would allow model 2/3-like responses to be learned- although by leaving them out, the model might behave a bit more like a relative positional model than an absolute positional model, which is probably attractive).----------------Finally, some discussion for why a fixed window is used to represent the target sentence is worth including (since a global context is apparently used to represent the source sentence).----------------The relationship between this training objective and pseudo likelihood (PL; Besag, 1975) might be worth mentioning. Since I believe this is just a PL objective for a certain global model, this suggests alternative decoding algorithms, or certainly a different analysis of the proposed decoding objective.----------------The section 4 model conditions on the true context of a position in the true target, the current target guess, and the source. I don’t completely understand the rationale for this model since at test time only two of these variables are available, and the replacement of y_ref with y_g seems hard to justify.","The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.",This is an important idea that is not currently playing much of a role in neural net models. The lack of in depth analysis suggests this paper is not quite ready for a final publication version. The limitations of the model are not well discussed,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This is an important idea that is not currently playing much of a role in neural net models. The lack of in depth analysis suggests this paper is not quite ready for a final publication version. The limitations of the model are not well discussed," This paper proposes a model for iteratively refining translation hypotheses . It could be interpreted as a globally normalized, undirected (~CRF) translation model trained using a pseudo-likelihood objective .",".-------------This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but",0.29729729729729726,0.08333333333333331,0.2162162162162162,0.8780285120010376,0.8941304683685303,0.8860062956809998
https://openreview.net/forum?id=r1y1aawlg,"Disclosure: I am not an expert in machine translation algorithms.----------------Summary: A human translator does not come up with the final translation right--------away. Instead, (s)he uses an iterative process, starting with a rough draft--------which is corrected little by little. The idea behind this paper is to--------implement a similar framework for an automated system. ----------------This paper is generally well written. ----------------It is my opinion however that drawings illustrating the architectures would help--------understanding how the different algorithms relate to one another.----------------I like a lot that you report on a preliminary experiment to give an--------intuition of how difficult the task is. You should highlight the links--------between the task of finding the errors in a guess translation and the task--------of iterative refinement. Could you use post-edited text to have a more--------solid ground-truth?----------------My main concern with this paper is that in the experimental section the --------iterative approach tries to improve upon only one type of machine translation. --------Which immediately prompts these questions:--------- why did they choose that approach to improve on?--------- what is the part of the improvement that comes from the choice of the--------  initial draft (maybe it was a very bad draft)? ----------------Here are some minor typos:--------- p.2: ... a lookup table that replace*S* each word... ?--------- p.3: I might be mistanken but it seems to me that j is used for two--------  different things. It is confusing.--------- p.3: ...takes as input these representation*S* and outputs... ?","The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.","A human translator does not come up with the final translation right away. Instead, they use an iterative process, starting with a rough draft. The idea behind this paper is to implement a similar framework for an automated system.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","A human translator does not come up with the final translation right away. Instead, they use an iterative process, starting with a rough draft. The idea behind this paper is to implement a similar framework for an automated system."," A human translator does not come up with the final translation right away . Instead, (s)he uses an iterative process, starting with a rough draft . The idea behind this paper is to implement a similar framework for an automated system","-----------I am not an expert in machine translation algorithms.-------------Summary: A human translator does not come up with the final translation right--------away. Instead, he uses",0.20588235294117646,0.030303030303030304,0.14705882352941177,0.8583620190620422,0.8638033866882324,0.8610741496086121
https://openreview.net/forum?id=r1y1aawlg,"This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.----------------Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change ""I went to the fridge even though I was not hungry"" to ""Although I was not hungry, I went to the fridge""). The fact that only 0.6 words are edited on average supports this. ----------------Specific comments:--------- It would be interesting to see what the improvements are if the baseline model is a neural system.--------- It seems strange (to me at least) that T^i and L(y^{-i|k}) only look at a window of 2k words. It means that when making the decision to change the i-th word, the model does not know what was generated outside of the window? --------- Relatedly, the idea of changing individual words based on local (i.e. word-level) scores seems counterintuitive. Given that we have the full generated sentence, don't we want a global score? Scoring at the sentence-level could also make room for non-greedy search strategies, which could potentially facilitate richer edits.--------- How does the approach compare to a model that simply re-ranks the k-best output?--------- Instead of editing, did you consider learning an encoder-decoder that takes in x, y_g, and generates y_ref? When decoding you can attend to both x and y_g.----------------Minor comments:--------- Iteratively improving a generated text was also explored in https://arxiv.org/pdf/1510.09202v1.pdf from a reinforcement learning angle.--------- I don't understand footnote 1.","The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work.","This work proposes to improve a sentence that has been generated from another MT system. The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word. During testing, the gold",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This work proposes to improve a sentence that has been generated from another MT system. The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word. During testing, the gold"," This work proposes to improve a sentence that has been generated from another MT system . The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word . During testing, the gold",:--------- It seems strange (to me at least) that Ti and L(y-i|k) only look at a window of 2k words. This work proposes to iterative,0.19718309859154928,0.028985507246376812,0.11267605633802817,0.8328607082366943,0.8588960766792297,0.8456780910491943
https://openreview.net/forum?id=ryUPiRvge,"Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.                                                                                                                                                  --------                                                                                                                                                                                                          --------Pros                                                                                                                                                                                                      --------- The approach is rather simple and hence can be applied to existing methods. The major difference is incorporating functions with 2 or more inputs which was done successfully in the paper. --------            --------- It seems that MLP, even though it is good for interpolation, it fails to extrapolate data to model the correct function. It was a great idea to use basis functions like sine, cosine to make the approach more explicit.                                                                                                                                                                                        --------                                                                                                                                                                                                          --------Cons                                                                                                                                                                                                      --------- Page 8, the claim that x2 cos(ax1 + b) ~ 1.21(cos(-ax1 + π + b + 0.41x2) + sin(ax1 + b + 0.41x2)) for y in [-2,2] is not entirely correct. There should be some restrictions on 'a' and 'b' as well as the approximate equality doesn't hold for all real values of 'a' and 'b'. Although, for a=2*pi and b=pi/4, the claim is correct so the model is predicting a correct solution within certain limits.      --------                                                                                                                                                                                                          --------- Most of the experiments involve up to 4 variables. It would be interesting to see how the neural approach models hundreds of variables.                                                                 --------                                                                                                                                                                                                          --------- Another way of looking at the model is that the non-linearities like sine, cosine, multiplication act as basis functions. If the data is a linear combination of such functions, the model will be able to learn the weights. As division is not one of the non-linearities, predicting expressions in Equation 13 seems unlikely. Hence, I was wondering, is it possible to make sure that this architecture is a universal approximator.                                                                                                                                                                                --------                                                                                                                                                                                                          --------Suggested Edits                                                                                                                                                                                           --------- Page 8, It seems that there is a typographical error in the expression 1.21(cos(ax1 + π + b + 0.41x2) + sin(ax1 + b + 0.41x2)). When compared with the predicted formula in Figure 4(b), it should be 1.21(cos(-ax1 + π + b + 0.41x2) + sin(ax1 + b + 0.41x2)). ","This paper proposes using functions such as sin and cos as basis functions, then training a neural network with L1 regularization to obtain a simple estimate of functions that can extrapolate under some circumstances.    Pros:  - the paper has a wide-ranging discussion connecting extrapolation in regression problems to adjacent fields of system identification and causal learning.  - the method is sensible enough, and should probably be a baseline in the time-series literature. It also seems like an advance on the hard-to-optimize Eureqa method.    Cons:  I agree with the authors that Reviewer 5's comments aren't very helpful, but this paper really does ignore or dismiss a lot of recent progress and related methods. Specifically:  - The authors claim that cross-validation can't be used to choose the model, since it wouldn't encourage extrapolation - but why not partition the data in contiguous chunks, as is done in time-series methods?  - The authors introduce an annealing trick to help with the L1 objective, but there is a rich literature on gradient-based optimization methods with L1 regularization that address exactly this problem.  - The authors mostly consider toy data, limiting the potential impact of their method.  - The authors don't compare against closely related methods developed to address the exact same setting. Namely, Schmit + Lipson's Eureqa method, and the Gaussian process methods of Duvenaud, Lloyd, Grosse, Tenenbaum and Ghahramani.  - The authors invent their own ad-hoc model-selection procedure, again ignoring a massive literature.    Given the many ""cons"", it is recommended that this paper not be presented at the conference track, but be featured at the workshop track.","Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc."," This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. It seems that MLP, even though it is good for interpolation, it fails","to model the correct function. -------- --------Cons ------ Page 8, The claim that x2 cos(ax1 + b)  1.21(cos(-ax1 +",0.11111111111111109,0.013157894736842103,0.0784313725490196,0.8531172871589661,0.7897947430610657,0.8202356696128845
https://openreview.net/forum?id=ryUPiRvge,"The authors attempt to extract analytical equations governing physical systems from observations - an important task. Being able to capture succinct and interpretable rules which a physical system follows is of great importance. However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field. ----------------The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence: --------""Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t. the weights are sparse (L1)"".--------The learnt network weights with its fixed structure are then presented as the learnt equation. ----------------This research uses tools from literature from the '90s (I haven't seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then. I would encourage the authors to review modern literature and continue working on this important task.","This paper proposes using functions such as sin and cos as basis functions, then training a neural network with L1 regularization to obtain a simple estimate of functions that can extrapolate under some circumstances.    Pros:  - the paper has a wide-ranging discussion connecting extrapolation in regression problems to adjacent fields of system identification and causal learning.  - the method is sensible enough, and should probably be a baseline in the time-series literature. It also seems like an advance on the hard-to-optimize Eureqa method.    Cons:  I agree with the authors that Reviewer 5's comments aren't very helpful, but this paper really does ignore or dismiss a lot of recent progress and related methods. Specifically:  - The authors claim that cross-validation can't be used to choose the model, since it wouldn't encourage extrapolation - but why not partition the data in contiguous chunks, as is done in time-series methods?  - The authors introduce an annealing trick to help with the L1 objective, but there is a rich literature on gradient-based optimization methods with L1 regularization that address exactly this problem.  - The authors mostly consider toy data, limiting the potential impact of their method.  - The authors don't compare against closely related methods developed to address the exact same setting. Namely, Schmit + Lipson's Eureqa method, and the Gaussian process methods of Duvenaud, Lloyd, Grosse, Tenenbaum and Ghahramani.  - The authors invent their own ad-hoc model-selection procedure, again ignoring a massive literature.    Given the many ""cons"", it is recommended that this paper not be presented at the conference track, but be featured at the workshop track.",The authors attempt to extract analytical equations governing physical systems from observations. This research uses tools from literature from the '90s and does not build on modern techniques.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors attempt to extract analytical equations governing physical systems from observations. This research uses tools from literature from the '90s and does not build on modern techniques.," The authors attempt to extract analytical equations governing physical systems from observations - an important task . However, they do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field .",The authors attempt to extract analytical equations governing physical systems from observations - an important task. The authors attempt to extract analytical equations governing physical systems from observations - an important task. The authors attempt to extract analytical equation,0.08053691275167785,0.006756756756756757,0.060402684563758385,0.8586208820343018,0.7840855121612549,0.8196622133255005
https://openreview.net/forum?id=ryUPiRvge,"Thank you for an interesting read. ----------------To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as ""extrapolation""). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.----------------Quesions and comments:----------------1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future?----------------2. Fitting an EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial. Assume we know the underlying dynamics we want to learn can be represented by a polynomial. Then what's the difference between fitting a polynomial (with model selection techniques to determine the degree) and fitting an EQL (with model selection techniques to determine the number of layers)? Also your experiments showed that the selection of basis functions (specific to the underlying dynamics you want to learn) is crucial for the performance. This means you need to have some prior knowledge on the form of the equation anyway!----------------3. Ben-David et al. 2010 has presented some error bounds for the hypothesis that is trained on source data but tested on the target data. I wonder if your EQL model can achieve better error bounds?----------------4. Can you comment on the comparison of your method to those who modelled the extrapolation data with **uncertainty**?","This paper proposes using functions such as sin and cos as basis functions, then training a neural network with L1 regularization to obtain a simple estimate of functions that can extrapolate under some circumstances.    Pros:  - the paper has a wide-ranging discussion connecting extrapolation in regression problems to adjacent fields of system identification and causal learning.  - the method is sensible enough, and should probably be a baseline in the time-series literature. It also seems like an advance on the hard-to-optimize Eureqa method.    Cons:  I agree with the authors that Reviewer 5's comments aren't very helpful, but this paper really does ignore or dismiss a lot of recent progress and related methods. Specifically:  - The authors claim that cross-validation can't be used to choose the model, since it wouldn't encourage extrapolation - but why not partition the data in contiguous chunks, as is done in time-series methods?  - The authors introduce an annealing trick to help with the L1 objective, but there is a rich literature on gradient-based optimization methods with L1 regularization that address exactly this problem.  - The authors mostly consider toy data, limiting the potential impact of their method.  - The authors don't compare against closely related methods developed to address the exact same setting. Namely, Schmit + Lipson's Eureqa method, and the Gaussian process methods of Duvenaud, Lloyd, Grosse, Tenenbaum and Ghahramani.  - The authors invent their own ad-hoc model-selection procedure, again ignoring a massive literature.    Given the many ""cons"", it is recommended that this paper not be presented at the conference track, but be featured at the workshop track.","The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains."," EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains .",EQL vs. fitting a polynomial. It seems to me that the number of layers in EQL has some connections to the degree of the polynomial.-------------4.,0.10561056105610561,0.0,0.05940594059405941,0.863728940486908,0.7918564081192017,0.8262326121330261
https://openreview.net/forum?id=r1kGbydxg,"The paper is straightforward, easy to read, and has clear results. ----------------Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?----------------Would we get the same result if there was no reference-pose cost, only a locomotion cost?----------------Would we get the same result if the task was to spin a top? My guess is no. ----------------This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.----------------The video is nice.","After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.    The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the ""default"" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.    But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.    I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.","The paper is straightforward, easy to read, and has clear results. This work is interesting, but not likely to generalise to other scenarios.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper is straightforward, easy to read, and has clear results. This work is interesting, but not likely to generalise to other scenarios."," The paper is straightforward, easy to read, and has clear results . The work is interesting, but not likely to generalise to other scenarios .","if there was no reference-pose cost, only a locomotion cost?----------------Would we get the same result if the task was to spin a top?----------",0.10674157303370785,0.01694915254237288,0.06741573033707865,0.8932492733001709,0.8033446073532104,0.8459148406982422
https://openreview.net/forum?id=r1kGbydxg,"Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. ----------------> Significance & Originality:----------------The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. ----------------> Clarity:----------------The paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. ----------------> Experiments:----------------Experimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.","After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.    The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the ""default"" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.    But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.    I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.","Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities)",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities)"," Paper studies deep reinforcement learning paradigm for controlling high dimensional characters . Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance","izations compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on reinforcement learning and optimized control policies. Evaluated are",0.08241758241758242,0.016574585635359115,0.049450549450549455,0.8339022994041443,0.7874987125396729,0.8100364208221436
https://openreview.net/forum?id=r1kGbydxg,"This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.----------------My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. --------The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  --------My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term ""DeepRL"" seems arbitrary.----------------On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. --------It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.","After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.    The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the ""default"" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.    But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.    I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples.",This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment. My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment. My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments., This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment . Authors only consider a single neural network architecture and a single reward function .,"is definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this",0.12466124661246612,0.01634877384196185,0.08130081300813008,0.8600046634674072,0.7990462779998779,0.8284055590629578
https://openreview.net/forum?id=SypU81Ole,"This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons / quantitative evaluations of the techniques proposed. Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work.","This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.","This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. The techniques proposed are simple, well explained, and of immediate use to those working on generative model",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. The techniques proposed are simple, well explained, and of immediate use to those working on generative model"," This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models . This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc",". This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper proposes a variety of techniques for visualizing learned generative models.",0.24390243902439024,0.075,0.24390243902439024,0.8701131939888,0.8740189075469971,0.8720616698265076
https://openreview.net/forum?id=SypU81Ole,"This paper proposed a set of different things under the name of ""sampling generative models"", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.----------------The spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances to the origin.  The author's justification for this comes from the well known fact that in high dimensional space, even with a uniform distribution most points lie on a thin shell in the unit cube.  This is true because in high-dimensional space, the outer shell takes up most of the volume in space, and the inner part takes only a very small fraction of the space, in terms of volume.  This does not mean the density of data in the outer shell is greater than the inner part, though.  In a uniform distribution, the data density should be equal everywhere, a point on the outer shell is not more likely than a point in the inner part.  Under a Gaussian model, the data density is on the other hand higher in the center and much lower on the out side.  If we have a good model of data, then sampling the most likely points from the model should give us plausible looking samples.  In this sense, spherical interpolation should do no better than the normally used linear interpolation.  From the questions and answers it seems that the author does not recognize this distinction.  The results shown in this paper seem to indicate that spherical interpolation is better visually, but it is rather hard to make any concrete conclusions from three pairs of examples.  If this is really the case then there must be something else wrong about our understanding of the learned model.----------------Aside from these, the J-diagram and the nearest neighbor latent space traversal both seems to be good ways to explore the latent space of a learned model.  The attribute vector section on transforming images to new ones with desired attributes is also interesting, and it provides a few new ways to make the GAN latent space more interpretable.----------------Overall I feel most of the techniques proposed in this paper are nice visualization tools.  The contributions however, are mostly on the design of the visualizations, and not much on the technical and model side.  The spherical interpolation provides the only mathematical equation in the paper, yet the correctness of the technique is arguable.  For the visualization tools, there are also no quantitative evaluation, maybe these results are more art than science.","This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.","This paper proposed a set of different things under the name of ""sampling generative models"" It focuses on analyzing the learned latent space and synthesizing desirable output images. While this paper has some interesting ideas, it also has a number",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposed a set of different things under the name of ""sampling generative models"" It focuses on analyzing the learned latent space and synthesizing desirable output images. While this paper has some interesting ideas, it also has a number"," This paper does not have one clear message or idea, but proposed a set of techniques that seem to produce visually good looking results . The spherical interpolation idea is interesting, but after a second thought this does not make much sense . The",". This paper proposed a set of different things under the name of ""sampling generative models"" This paper proposed a set of different things under the name of ""sampling generative models"", focusing on analyzing",0.3255813953488372,0.09523809523809523,0.1627906976744186,0.8732883334159851,0.8715716004371643,0.8724291324615479
https://openreview.net/forum?id=SypU81Ole,"In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.----------------I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it’s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does not have the scientific quality and depth I’ve seen in many other ICLR submissions. But it does more than just describing useful “tricks”. And all things considered I think this paper deserves a wider audience (but  I'm not convinced that ICLR is the right venue)","This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation.",In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. They highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. They highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional, In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs . The authors highlight the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper,", and it’s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide (theoretical/empirical) insight and it does",0.29885057471264365,0.023529411764705882,0.20689655172413796,0.8431786894798279,0.8561991453170776,0.8496391177177429
https://openreview.net/forum?id=ryb-q1Olg,"The paper presents a repurposing of rectified factor networks proposed--------earlier by the same authors to biclustering. The method seems--------potentially quite interesting but the paper has serious problems in--------the presentation.------------------------Quality:----------------The method relies mainly on techniques presented in a NIPS 2015 paper--------by (mostly) the same authors. The experimental procedure should be--------clarified further. The results (especially Table 2) seem to depend--------critically upon the sparsity of the reported clusters, but the authors--------do not explain in sufficient detail how the sparsity hyperparameter is--------determined.------------------------Clarity:----------------The style of writing is terrible and completely unacceptable as a--------scientific publication. The text looks more like an industry white--------paper or advertisement, not an objective scientific paper. A complete--------rewrite would be needed before the paper can be considered for--------publication. Specifically, all references to companies using your--------methods must be deleted.----------------Additionally, Table 1 is essentially unreadable. I would recommend--------using a figure or cleaning up the table by removing all engineering--------notation and reporting numbers per 1000 so that e.g. ""0.475 +/- 9e-4""--------would become ""475 +/- 0.9"". In general figures would be preferred as a--------primary means for presenting the results in text while tables can be--------included as supplementary information.------------------------Originality:----------------The novelty of the work appears limited: the method is mostly based on--------a NIPS 2015 paper by the same authors. The experimental evaluation--------appears at least partially novel, but for example the IBD detection is--------very similar to Hochreiter (2013) but without any comparison.------------------------Significance:----------------The authors' strongest claim is based on strong empirical performance--------in their own benchmark problems. It is however unclear how useful this--------would be to others as there is no code available and the details of--------the implementation are less than complete. Furthermore, the method--------depends on many specific tuning parameters whose tuning method is not--------fully defined, leaving it unclear how to guarantee the generalisation--------of the good performance.","The reviewers pointed out several issues with the paper, and all recommended rejection.",The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The method relies mainly on techniques presented in a NIPS 2015 paper. The style of writing is terrible and completely unacceptable,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The method relies mainly on techniques presented in a NIPS 2015 paper. The style of writing is terrible and completely unacceptable, The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering . The method seems potentially quite interesting but the paper has serious problems in the presentation .,------------------------------------------------------------------------------------------------------,0.1568627450980392,0.04081632653061225,0.1568627450980392,0.8287956118583679,0.8690214157104492,0.8484319448471069
https://openreview.net/forum?id=ryb-q1Olg,"Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.----------------Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. ----------------Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)----------------Quality: The experiments are high-quality. ----------------Comments:--------1) The introduction claims that this method is much faster than FABIA because the use of rectified units allow it to be run on GPUs. It is not clear to me how this works. How many biclusters can be supported with this method? It looks like the number of biclusters used for this method in the experiments is only 3-5?--------2) The introduction claims that using dropout during training increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis, but this claim should be supported with a better argument or experiments.--------3) How is the model deep? The model isn't deep just because it uses a relu and dropout.","The reviewers pointed out several issues with the paper, and all recommended rejection.","The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. The introduction claims that this method is much faster than FABIA because the use of rectified",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. The introduction claims that this method is much faster than FABIA because the use of rectified"," The proposed algorithm appears to be a useful tool for unsupervised data modelling . The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used .","the RFN model (including the ReLU non-linearity and dropout training) increases sparsity in the bicluster assignments. This seems like a reasonable hypothesis, but this claim should be supported with a better",0.11320754716981132,0.0,0.07547169811320756,0.8395901918411255,0.8748902082443237,0.856876790523529
https://openreview.net/forum?id=ryb-q1Olg,"This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. ----------------This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of  and .  is a hidden unit, but what is ? I could not find any definition. Furthermore, I could not know how  is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. ----------------Totally, I am not sure that this paper is suitable for publication. ----------------Prons:--------Empirical performance is good.----------------Cons:--------Novelty of the proposed method--------Some description in the paper is unclear.","The reviewers pointed out several issues with the paper, and all recommended rejection.",This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 bic Lustering methods. The novelty of the proposed method seems small.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 bic Lustering methods. The novelty of the proposed method seems small., The proposed method performs best among 14 biclustering methods . The novelty of the proposed method seems small . The authors replied to the same question which another reviewer gave .,"method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave",0.13953488372093023,0.0,0.09302325581395349,0.8291112780570984,0.873979389667511,0.8509542942047119
https://openreview.net/forum?id=SJNDWNOlg,"The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.----------------Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.----------------While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval""). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact, they also indicate that image retrieval is more difficult than image classification -- this is because it is performed by using features originally trained for classification. I can partially accept this argument. However, given the results in recent papers, it is clear than end-to-end training is far superior in practice and it is not clear the analysis developed by the authors in this work would transfer or be useful for that case as well.","The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification.     An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval"". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. ""Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach"".     A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.","The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.", The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval . The authors focus on testing various architectural choices but do not propose or compare to end-to-end learning frameworks .,"focuses on comparing different baseline architectures for CNN-based image retrieval. The authors conduct a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do",0.3023255813953489,0.22352941176470584,0.2674418604651163,0.9159902334213257,0.839044451713562,0.8758305907249451
https://openreview.net/forum?id=SJNDWNOlg,"Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:----------------I don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used ""as large as possible"" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).----------------The setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network. ----------------Furthermore, I don't think it's sufficient to just try one network and claim these are the best practices for using CNNs for instance retrieval - what about ResNet, what about Inception, I don't know how to apply any of these conclusions for those networks, and would these conclusions even hold for them. Furthermore the parameter tweaking was done on Oxford, I really can't tell what conclusions would we get if we tuned on UKB for example. So a more appropriate paper title would be ""What are the best parameter values for VGG-19 on Oxford/Paris benchmarks?"" - I don't think this is sufficiently novel nor interesting for the community.","The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification.     An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval"". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. ""Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach"".     A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.",Authors investigate how to use pretrained CNNs for retrieval. For detailed comments on everything see the questions I posted earlier. The summary is here:,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Authors investigate how to use pretrained CNNs for retrieval. For detailed comments on everything see the questions I posted earlier. The summary is here:, Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters . For detailed comments on everything see the questions I posted earlier .,", and performs some more parameter tweaking to achieve the state-of-the-art results. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the",0.13580246913580246,0.0,0.08641975308641976,0.8634366393089294,0.8119337558746338,0.8368935585021973
https://openreview.net/forum?id=SJNDWNOlg,"This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as ""traditional wisdom"".----------------Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. ----------------While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it ""ignores"" 2 major recent works that are in direct contradictions with many claims of the paper ([a] ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by  Gordo et al. and [b] ""CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples"" by Radenović et al., both ECCV'16 papers). These works have shown that training for retrieval can be achieved with a siamese architectures and have demonstrated outstanding performance. As a result, many claims and findings of the paper are either outdated, questionable or just wrong.----------------Here are some of the misleading claims: ----------------  - ""Features aggregated from these feature maps have been exploited for image retrieval tasks and achieved state-of-the-art performances in recent years.""--------  Until [a] (not cited), the state-of-the-art was still largely dominated by sparse invariant features based methods (see last Table in [a]).--------  --------  - ""the proposed method [...] outperforms the state-of-the-art methods on four typical datasets""--------  That is not true, for the same reasons than above, and also because the state-of-the-art is now dominated by [a] and [b].--------  --------  - ""Also in situations where a large numbers of training samples are not available, instance retrieval using unsupervised method is still preferable and may be the only option."".--------  This is a questionable opinion. The method exposed in ""End-to-end Learning of Deep Visual Representations for Image Retrieval"" by Gordo et al. outperforms the state-of-the-art on the UKB dataset (3.84 without QE or DBA) whereas it was trained for landmarks retrieval and not objects, i.e. in a different retrieval context. This demonstrates that in spite of insufficient training data, training is still possible and beneficial.----------------  - Finally, most findings are not even new or surprising (e.g. aggregate several regions in a multi-scale manner was already achieved by Tolias at al, etc.). So the interest of the paper is limited overall.----------------In addition, there are some problems in the experiments. For instance, the tuning experiments are only conducted on the Oxford dataset and using a single network (VGG-19), whereas it is not clear whether these conditions are well representative of all datasets and all networks (it is well known that the Oxford dataset behaves very differently than the Holidays dataset, for instance). In addition, tuning is performed very aggressively, making it look like the authors are tuning on the test set (e.g. see Table 3). ----------------To conclude, the paper is one year too late with respect to recent developments in the state of the art.","The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification.     An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval"". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. ""Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach"".     A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue.","This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval", This paper explores different strategies for instance-level image retrieval with deep CNNs . The approach consists of extracting features from a network pre-trained for image classification . Post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted,"whereas it was trained for landmarks retrieval (e.g. VGG), and post-process them for image retrieval.---------------- This is a questionable opinion.-------------",0.25142857142857145,0.09248554913294797,0.19428571428571428,0.8782227635383606,0.8335922360420227,0.8553256988525391
https://openreview.net/forum?id=r1osyr_xg,"This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.----------------On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.----------------Detailed/minor points below:----------------1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very difficult to understand due to the unconventional sentence structure.--------2) The tables need better and more descriptive labels.--------3) The results are somewhat inconclusive. Particularly in the analogy task in Table 4 it is surprising that CBOW does better on the semantic aspect of the task than your embeddings which are specifically tailored to be good at this?--------4) Why was ""Enriched CBOW"" not included in the analogy task?--------5) In the related work section several papers are mentioned that learn embeddings from a combination of lexica and corpora, yet it is repeatedly said that this was the first work of such a kind / that there hasn't been enough work on this. That feels a little misleading.",The reviewers agree that the paper's clarity and experimental evaluation can be improved.,This paper proposes a method for estimating the context sensitivity of paraphrases. It uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a method for estimating the context sensitivity of paraphrases. It uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration., This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model . The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration .,is the most obvious model formulation to try. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The main weaknesses,0.2641509433962264,0.0784313725490196,0.22641509433962267,0.8636846542358398,0.8901063799858093,0.8766964673995972
https://openreview.net/forum?id=r1osyr_xg,"This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.----------------I think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.----------------In addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).----------------Finally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations / lexicons / corpus-based methods in general?",The reviewers agree that the paper's clarity and experimental evaluation can be improved.,This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations. It tries to determine which potential paraphrase is the most appropriate in a particular context.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations. It tries to determine which potential paraphrase is the most appropriate in a particular context., This paper tries to leverage an external lexicon to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context . I think this paper is a bit lost in translation,".-------------Finally, this work does not provide any qualitative result or motivation.-------------Finally, this work does not provide qualitative result or motivation.----------Finally,",0.13333333333333333,0.0,0.08888888888888889,0.8461652398109436,0.8737481832504272,0.8597355484962463
https://openreview.net/forum?id=r1osyr_xg,"This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.----------------The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.----------------Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij) is optimal. It would have been nice to see some experiments investigating different choices, in particular some baselines where the effect of f is diminished (so that it reduces to f=1 in the limit) would have been interesting. I also feel like there would be a lot to gain from having f be a function of the nearby word embeddings, though this would obvious incur a significant slowdown. (See for example 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should probably be cited.) As it stands, the experimental results do not clearly distinguish the fuzzy paraphrase approach from prior work, i.e. tables 3 and 4 do not show major trends one way or the other.----------------Regarding the second question, it is hard to draw many conclusions from analogy tasks alone, especially when effects unrelated to good/bad paraphrasing such as corpus size/content, window size, vocabulary size, etc., can have an outsize effect on performance. ----------------Overall, I think this is a good paper presenting a sensible idea, but I am not convinced by the experiments that the specific approach is achieving its goal. With some improved experiments and analysis, I would wholeheartedly recommend this paper for acceptance; as it stands, I am on the fence.",The reviewers agree that the paper's clarity and experimental evaluation can be improved.,This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. The main idea is to introduce a function that essentially judges the context-sensitivity,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. The main idea is to introduce a function that essentially judges the context-sensitivity, This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology . The main idea is to introduce a function that essentially judges the context-sensitivity,of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology.-------------The main argumentation leading to the model selection is,0.14814814814814817,0.0,0.1111111111111111,0.8215816020965576,0.8601840734481812,0.8404397964477539
https://openreview.net/forum?id=BJwFrvOeg,"This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.----------------The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.----------------At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.----------------However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. --------- I cannot find where  f_{symbkey} / f_{voca} / f_{copy} are defined--------- w^v, w^s are confusing.--------- e_k seems to be the average of all previous fact embeddings? It is necessary to make it clear enough.--------- (h_t, c_t) = f_LSTM(x_{t−1}, h_{t−1})  c_t is not used?--------- The notion of “fact embeddings” is also not that clear (I understand that they are taken as the concatenation of relation and entity (object) entities in the end).  For the anchor / “topic-itself” facts, do you learn the embedding for the special relations and use the entity embeddings from TransE?----------------On generating words from KB entities (fact description), it sounds a bit strange to me to generate a symbol position first.  Most entities are multiple words, and it is necessary to keep that order. Also it might be helpful to incorporate some prior information, for example, it is common to only mention “Obama” for the entity “Barack Obama”?","This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper ""different and more interesting"". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.    Pros:  - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments.     Mixed:  - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks.     Cons:  - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.  - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions",The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts. The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts. The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words., This paper proposes to incorporate knowledge base facts into language modeling . The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts .,"/ f_voca / f_copy are defined--------- wv, ws are confusing.---------- The notion of “fact embeddings”",0.15204678362573099,0.011834319526627219,0.10526315789473684,0.8442069292068481,0.8018203377723694,0.8224678635597229
https://openreview.net/forum?id=BJwFrvOeg,"The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.----------------The writing could be improved. The beginning of Section 3 in particular is hard to parse.----------------There have been similar efforts recently (like ""Pointer Sentinel Mixture Models"" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is nice.----------------The architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating. ------------------------Other comments:--------* How to cope with the dependency on the KB? Freebase is not updated anymore so it is likely that a lot of the new unseen words in the making are not going to be in Freebase.--------* What is the performance on standard benchmarks like Penn Tree Bank?--------* How long is it to train compare to a standard RNNLM?--------* What is the importance of the knowledge context ?--------* How is initialized the fact embedding  for the first word?--------* When a word from a fact description has been chosen as prediction (copied), how is it encoded in the generation history for following predictions if it has no embedding (unknown word)? In other words, what happens if ""Michelle"" in the example of Section 3.1 is not in the embedding dictionary, when one wants to predict the next word?","This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper ""different and more interesting"". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.    Pros:  - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments.     Mixed:  - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks.     Cons:  - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.  - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions",The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate, The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words . It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate,of RNNLMs with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words.----,0.17679558011049723,0.0111731843575419,0.11049723756906078,0.8359817862510681,0.79658442735672,0.8158077597618103
https://openreview.net/forum?id=BJwFrvOeg,"This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf’s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.----------------The model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.----------------Overall the paper could use some rewriting especially the notations in section 3. The experiments are well executed and they definitely get good results. The heat maps at the end are very insightful. ----------------Comments----------------This contributions of this paper would be much stronger if it showed improvements in a practical applications such as Question Answering (although the paper clearly mentions that this technique could be applied to improve QA)--------In section 3, it is unclear why the authors refer the entity as a ‘topic'. This makes the text a little confusing since a topic can also be associated with something abstract, but in this case the topic is always a freebase entity. --------Is it really necessary to predict a fact at every step before generating a word. In other words, how many distinct facts on average does the model choose to generate a sentence. Intuitively a natural language sentence would be describe few facts about an entity. If the fact generation step could be avoided (by adding a latent variable which decides if the fact should be generated or not), the model will also be faster.--------In equation 2, the model has to make a hard decision to choose the fact. For this to be end to end trained, every word needs to be annotated with a corresponding fact which might not be always a realistic scenario. For e.g., in domains such as social media text.--------Learning position embeddings for copying knowledge words seems a little counter-intuitive. Does the sequence of knowledge words follow any particular structure like word O_2 is always the last name (e.g. Obama).--------It would also be nice to compare to char-level LM's which inherently solves the unknown token problem. ","This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper ""different and more interesting"". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.    Pros:  - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments.     Mixed:  - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks.     Cons:  - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.  - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions","This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf’s law, most approaches limit the vocabulary. Rare words are especially important in context of applications such as",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf’s law, most approaches limit the vocabulary. Rare words are especially important in context of applications such as", This paper addresses the practical problem of generating rare or unseen words in the context of language modeling . Rare words are especially important in context of applications such as question answering .,based on the previously generated words and facts. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from ,0.12290502793296088,0.0,0.0893854748603352,0.8361715078353882,0.8018931746482849,0.8186737298965454
https://openreview.net/forum?id=HyEeMu_xx,"This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results.----------------Pros:--------- The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections]--------- Good visualisations.----------------Cons:--------- No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps.--------- Not clear how the query is fed into the localisation networks of baselines.--------- The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different. Why is this? There is no significant performance gain on any standard datasets.--------- No real novelty.","The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.    The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art.     Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger.","This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before. The element-wise multiplicative gates are very similar to convolution",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before. The element-wise multiplicative gates are very similar to convolution"," This paper proposes an attention mechanism which is essentially a gating on every spatial feature . The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through","is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections]------ Good visualisations.---------Cons:--------- No progressive baseline",0.11538461538461538,0.0,0.06410256410256411,0.8376538753509521,0.8173474073410034,0.8273760676383972
https://openreview.net/forum?id=HyEeMu_xx,"This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.----------------Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.  ----------------An important missing citation is Graves 2013 which had an early version of the attention model. ----------------Minor typo:--------""It confins possible attributes.."" -> It confines..--------""ImageNet (Deng et al., 2009), is used, and three additional"" -> "".., are used,""","The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.    The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art.     Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger.",This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this, This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet . The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset .,that uses multiple layers of soft attention in a convnet. This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic,0.23899371069182387,0.06369426751592357,0.17610062893081763,0.8551978468894958,0.8312886953353882,0.8430737853050232
https://openreview.net/forum?id=HyEeMu_xx,"The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention.----------------1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.--------2. Qualitative results: --------2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the “number” dataset and more on the Visual Genome dataset.--------2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.--------3. Qualitative results:--------3.1. The paper presents results on two datasets, one simulated dataset as well as Visual Genome. On both it shows moderate but significant improvements over related approaches.--------3.2. For the visual genome dataset, it would be interesting to include a quantitative evaluation how good the localization performance is of the attention approach.--------3.3. It would be interesting to get a more detailed understanding of the model by providing results for different CNN layers where the attention is applied.--------4. It would be interesting to see results on more established tasks, e.g. VQA, where the model should similarly apply. In fact, the task on the numbers seems to be identical to the VQA task (input/output), so most/all state-of-the-art VQA approaches should be applicable.------------------------Other (minor/discussion points)--------- Something seems wrong in the last two columns in Figure 11: the query “7” is blue not green. Either the query or the answer seem wrong.--------- Section 3: “In each layer, the each attended feature map” -> “In each layer, each attended feature map”--------- I think Appendix A would be clearer if it would be stated that is the attention mechanism used in SAN and which work it is based on.------------------------Summary:--------While the experimental evaluation could be improved with more detailed evaluation, comparisons, and qualitative results, the presented evaluation is sufficient to validate the approach. The approach itself is novel and interesting to my knowledge and speaks for acceptance.","The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.    The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art.     Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger.","The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only"," The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN . In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only",": The query “7” is blue not green. Either the query or the answer seem wrong.------ Section 3: “In each layer, the each attended feature map” -> “In each layer, each attended feature map",0.22784810126582278,0.05128205128205129,0.1518987341772152,0.837622880935669,0.8189042806625366,0.8281577825546265
https://openreview.net/forum?id=Byj72udxe,"This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.----------------The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words in the recent context). The context (query) vector for the pointing network is also different, but this is also a direct consequence of having a different application.----------------While the paper describes the differences between the proposed approach and Gulcehre et al.’s approach, I find some of the claims either wrong or not that significant. For example, quoting from Section 1:--------“Rather than relying on the RNN hidden state to decide when to use the pointer, as in the recent work of Gulcehre et al. (2016), we allow the pointer component itself to decide when to use the softmax vocabulary through a sentinel.”--------As far as I can tell, your model also uses the recent hidden state to form a query vector,  which is matched by the pointer network to previous words. Can you please clarify what you mean here?----------------In addition, quoting from section 3 which describes the model of Gulcehre et al.:--------“Rather than constructing a mixture model as in our work, they use a switching network to decide which component to use”--------This is not correct. The model of Gulcehre is also a mixture model, where an MLP with sigmoid output (switching network) is used to form a mixture between softmax prediction and locations of the input text.----------------Finally, in the following quote, also from section 3: --------“The pointer network is not used as a source of information for the switching network as in our model.” --------It is not clear what the authors mean by “source of information” here. Is it the fact that the switching probability is part of the pointer softmax? I am wondering how significant this difference is.----------------With regards to the proposed dataset, there are also other datasets typically used for language modelling, including The Hutter Prize Wikipedia (enwik8) dataset (Hutter, 2012) and e Text8 dataset (Mahoney, 2009). Can you please comment on the differences between your dataset and those as well?----------------I would be happy to discuss with the authors the points I raised, and I am open to changing my vote if there is any misunderstanding on my part.","The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly.","This paper proposes augmenting RNN-based language models with a pointer network. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes augmenting RNN-based language models with a pointer network. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the", This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words . The proposed model is a direct application of the previous work Gulcehre et al. which follows a similar approach,"as in our work, they use a switching network to decide which component to use”----------This is not correct.----------The main differences I find is that Gulcehre et al.",0.16748768472906403,0.03980099502487562,0.11822660098522167,0.8408523201942444,0.8133502006530762,0.8268726468086243
https://openreview.net/forum?id=Byj72udxe,"This work is basically a combined pointer network applied on language modelling. --------The smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences. --------Hence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words. ----------------Generally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate. --------This paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling. --------I would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results. ----------------In addition, the new WikiText language modelling dataset is very interesting. --------It probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset. ----------------Overall, this is a well-written paper. I recommend it to be accepted.","The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly.",This work is basically a combined pointer network applied on language modelling. The new WikiText language modelling dataset is very interesting.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This work is basically a combined pointer network applied on language modelling. The new WikiText language modelling dataset is very interesting.," This work is basically a combined pointer network applied on language modelling . It aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences .",". -----------This paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences. -----------This paper",0.13114754098360656,0.03314917127071823,0.1092896174863388,0.8640574812889099,0.808495044708252,0.8353533148765564
https://openreview.net/forum?id=Byj72udxe,"This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. --------The idea is appealing in general for context biasing and the specific approach appears quite simple.----------------The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,--------but not as a mixture model, as in this paper.----------------The paper is clearly written and the results seem promising.--------The new dataset the authors created (WikiText) also seems of high interest. ----------------A comment regarding notation:--------The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i) --------This is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words.--------It would be helpful to use different symbol for the two objects.","The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly.","This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. The new dataset the authors created (WikiText) also seems of high interest.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. The new dataset the authors created (WikiText) also seems of high interest.", This work is an extension of previous works on pointer models that mixes its outputs with standard softmax outputs . The idea is appealing in general for context biasing and the specific approach appears quite simple .,p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_,0.15544041450777202,0.06282722513089005,0.13471502590673576,0.8602460622787476,0.8208192586898804,0.8400703072547913
https://openreview.net/forum?id=BJh6Ztuxl,"The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.----------------Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in ""Improving Distributional Similarity with Lessons Learned from Word Embeddings"" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will be even more important.----------------While this paper makes a valuable contribution in setting out and exploring a methodology for evaluating sentence embeddings, the evaluations themselves are quite simple and do not necessarily correlate with real-world desiderata for sentence embeddings (as the authors note in other comments, performance on these tasks is not a normative measure of embedding quality). For example, as the authors note, the ability of the averaged vector to encode sentence length is trivially to be expected given the central limit theorem (or more accurately, concentration inequalities like Hoeffding's inequality).----------------The word-order experiments were interesting. A relevant citation for this sort of conditional ordering procedure is ""Generating Text with Recurrent Neural Networks"" by Sutskever, Martens, and Hinton, who refer to the conversion of a bag of words into a sentence as ""debagging.""----------------Although this is just a first step in better understanding of sentence embeddings, it is an important one and I recommend this paper for publication.","The area chair agrees with the reviewers and think this paper would be of interest to the ICLR audience. There is clearly more to be done in this area, but the authors do a good job shedding some light on what sentence embeddings can encode. We need more work like this that helps us understand what neural networks can model.","The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into"," The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order . The experiments are thorough and provide interesting insights into the representational power of common",of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent.-------------------The word-order experiments were interesting,0.16842105263157894,0.04301075268817204,0.10526315789473685,0.8523994088172913,0.8481559157371521,0.8502724170684814
https://openreview.net/forum?id=BJh6Ztuxl,"This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW. (There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).----------------There are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them. However, I found the discussion of the word-order experiment rather unsatisfying. It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper bound which can be deduced from natural language statistics?' This is investigated from one angle in Section 7, but I would have preferred to the effect of natural language statistics discussed up front rather than presented as the explanation to a 'surprising' observation. I had a similar reaction to the word-order experiments.----------------Most of the interesting results, in my opinion, are about the ED model. It is fascinating that the LSTM encoder does not seem to rely on natural-language ordering statistics -- it seems like doing so should be a big win in terms of per-parameter expressivity. I also think that it's strange that word content accuracy begins to drop for high-dimensional embeddings. I suppose this could be investigated by handicapping the decoder.----------------Overall, this is a very nice paper investigating some aspects of the information content stored in various types of sentence embeddings. I recommend acceptance.","The area chair agrees with the reviewers and think this paper would be of interest to the ICLR audience. There is clearly more to be done in this area, but the authors do a good job shedding some light on what sentence embeddings can encode. We need more work like this that helps us understand what neural networks can model.",This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invari,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invari, This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks . The authors aim to assess how much and what type of information is captured by the different embedding models .,"of the word-order experiment, I found the discussion of the word-order experiment rather unsatisfying. I found the discussion of the word-order experiment rather unsatisfying.-------------",0.25531914893617025,0.04347826086956522,0.1702127659574468,0.8175492882728577,0.8358909487724304,0.8266183733940125
https://openreview.net/forum?id=BJh6Ztuxl,"This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.----------------Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.----------------One minor red flag: --------- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation. Can you be absolutely certain that those results would appear with a different codebase and different random seed implementing the same model? Fortunately, this point is largely orthogonal to the major results of the paper.----------------Two writing comments:--------- I agree that the results with word order and CBOW are surprising, but I think it's slightly misleading to say that CBOW is predictive of word order. It doesn't represent word order at all, but it's possible to probabilistically reconstruct word order from the information that it does encode.--------- Saying that ""LSTM auto-encoders are more effective at encoding word order than word content"" doesn't really make sense. These two quantities aren't comparable. ","The area chair agrees with the reviewers and think this paper would be of interest to the ICLR audience. There is clearly more to be done in this area, but the authors do a good job shedding some light on what sentence embeddings can encode. We need more work like this that helps us understand what neural networks can model.","This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. Some of the results seem somewhat strange, but I",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. Some of the results seem somewhat strange, but I", This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning . The results are non-trivial and somewhat surprising .,"of the paper.------------One minor red flag:--------- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious further investigation",0.2626262626262626,0.020618556701030927,0.12121212121212123,0.8555880188941956,0.8487750291824341,0.8521679043769836
https://openreview.net/forum?id=ByQPVFull,"This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with “privileged information” in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional “background suppression” term.------------------------Pros:----------------Proposes a “group-wise model diversity” loss term which is novel, to my knowledge.----------------The use of foreground segmentation masks to improve image classification is also novel.----------------The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.------------------------Cons:----------------The evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The use of the background suppression term is also confusing to me -- it seems redundant, as the group orthogonality term should already serve to suppress the use of background features by the foreground feature extractor.----------------It would be nice to see the results with “Incomplete Privileged Information” on the full ImageNet dataset (rather than just 10% of it) with the privileged information included for the 10% of images where it’s available.  This would verify that the method and use of segmentation masks remains useful even in the regime of more labeled classification data.----------------The presentation overall is a bit confusing and difficult to follow, for me.  For example, Section 4.2 is titled “A Unified Architecture: GoCNN”, yet it is not an overview of the method as a whole, but a list of specific implementation details (even the very first sentence).----------------Minor: calling eq 3 a “regression loss” and writing “||0 - x||” rather than just “||x||” is not necessary and makes understanding more difficult -- I’ve never seen a norm regularization term written this way or described as a “regression to 0”.----------------Minor: in fig. 1 I think the FG and BG suppression labels are swapped: e.g., the “suppress foreground” mask has 1s in the FG and 0s in the BG (which would suppress the BG, not the FG).------------------------An additional question: why are the results in Table 4 with 100% privileged information different from those in Table 1-2?  Are these not the same setting?----------------The ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.  Besides additional experiments, the paper could also use some reorganization and revision for clarity.----------------===============----------------Edit (1/29/17): after considering the latest revisions -- particularly the full ImageNet evaluation results reported in Table 5 demonstrating that the background segmentation 'privileged information' is beneficial even with the full labeled ImageNet dataset -- I've upgraded my rating from 4 to 6.----------------(I'll reiterate a very minor point about Figure 1 though: I still think the ""0"" and ""1"" labels in the top part of the figures should be swapped to match the other labels.  e.g., the topmost path in figure 1a, with the text ""suppress foreground"", currently has 0 in the background and 1 in the foreground, when one would want the reverse of this to suppress the foreground.)","This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.",This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL V,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL V, This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group . The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL V,"the FG and BG suppression labels are swapped: e.g., the “suppress foreground” mask has 1s in the FG and 0s in the BG (which would suppress the",0.2558139534883721,0.02380952380952381,0.18604651162790697,0.8173906207084656,0.8423766493797302,0.8296955823898315
https://openreview.net/forum?id=ByQPVFull,"This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.----------------Pros:--------- The paper is clear and easy to follow--------- The experimental results seem to show some benefit from the proposed approach----------------Cons:--------(1) The paper proposes one core idea (group orthogonality w/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation--------(2) No comparison with an ensemble--------(3) Full experiments on ImageNet under the ""partial privileged information"" setting would be more impactful----------------This paper is promising and I would be willing to accept an improved version. However, the current version lacks focus and clean experiments.----------------First, the abstract and intro focus on the need to replace ensembles with a single model that has diverse (ensemble like) features. The hope is that such a model will have the same boost in accuracy, while requiring fewer FLOPs and less memory. Based on this introduction, I expect the rest of the paper to focus on this point. But it does not; there are no experimental results on ensembles and no experimental evidence that the proposed approach in able to avoid the speed and memory cost of ensembles while also retaining the accuracy benefit.----------------Second, the technical contribution of the paper is presented as group orthogonality (GO). However, in Sec 4.1 the idea of background feature suppression is introduced. While some motivation for it is given, the motivation does not tie into GO. GO does not require bg suppression and the introduction of it seems ad hoc. Moreover, the experiments never decouple GO and bg suppression, so we are unable to understand how GO works on its own. This is a critical experimental flaw in my reading.----------------Minor suggestions / comments:--------- The equation in definition 2 has an incorrect normalizing factor (1/c^(k)^2)--------- Figure 1 seems to have incorrect mask placements. The top mask is one that will mask out the background and only allow the fg to pass","This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.","This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign"," This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups . Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign",GO. This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups. This paper proposes a modification to ConvNet training so that the feature,0.20224719101123595,0.022988505747126433,0.11235955056179775,0.8289250135421753,0.8436906933784485,0.83624267578125
https://openreview.net/forum?id=ByQPVFull,"The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives ""complementary viewpoints"" of the input to the subsequent layers, which can be thought of as performing ensembling/expert combination within the model, rather than using an ensemble of networks. ----------------For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a ""foreground"" and a ""background"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. ----------------They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a ""vanilla"" baseline that does not use these losses.----------------I enjoyed reading the paper because the idea is simple, smart, and seems to be effective. --------But there are a few concerns;---------firstly, the way of doing this seems very particular to vision. In vision one knows that masking the features (during both training and testing) helps, e.g. https://arxiv.org/abs/1412.1283--------To be fair, this is not truly the same thing as what the authors are doing, because in the reference above the masking is computed  during both training and testing, while here it is used as a method of decorrelating neurons at training time.--------But I understand that to the broader iclr community this may seem as ""yet another vision-specific trick"", while to the vision community one would ask why not just use the mask during both training and testing, since one can compute it in the first place. ----------------More importantly, the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet for testing. They do get a substantial boost, but it is not clear if this will transfer to more data/layers. ----------------The authors could at least have also tried CIFAR-10/100. I would expect to see some more results during the rebuttal period. ","This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline.","The authors propose a method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers. They split intermediate neurons to a ""foreground"" and a ""background"" subset, and",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors propose a method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers. They split intermediate neurons to a ""foreground"" and a ""background"" subset, and"," The authors propose a method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers . They split intermediate neurons to a ""foreground"" and a ""background"" subset, and",".----------But I understand that this is not truly the same thing as what the authors are doing, because it is computed during both training and testing. ------------The authors propose a sensible method to",0.18823529411764706,0.0,0.1411764705882353,0.8260014057159424,0.8429011702537537,0.834365725517273
https://openreview.net/forum?id=r1X3g2_xl,"The authors propose to apply virtual adversarial training to semi-supervised classification.----------------It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.----------------In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).----------------Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).----------------Overall, the paper reads well and propose a semi-supervised learning algorithm which is shown to work in practice. Theoretical and experimental comparison with past work is missing.","This paper is concerned with extending adversarial and virtual adversarial training to text classification tasks. The main technical contribution is to apply perturbations to word embeddings rather than discrete input symbols. Excellent empirical performance is reported across a variety of tasks.     The reviewers were consensual in acknowledging the clarity and significance of the contribution, highlighting the quality of the numerical experiments. Moreover, the authors were responsive in the rebuttal phase and updated their paper with reviewers suggestions (such as the svm-related comparisons).     The AC thus recommends accepting this work as a poster.",The authors propose to apply virtual adversarial training to semi-supervised classification. Theoretical and experimental comparison with past work is missing.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors propose to apply virtual adversarial training to semi-supervised classification. Theoretical and experimental comparison with past work is missing., Authors propose to apply virtual adversarial training to semi-supervised classification . Theoretical and experimental comparison with past work is missing .,", the authors propose to apply virtual adversarial training to semi-supervised classification.----------------It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature",0.2456140350877193,0.08928571428571427,0.14035087719298245,0.8874533176422119,0.8250516653060913,0.8551155924797058
https://openreview.net/forum?id=r1X3g2_xl,"This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Extending the idea of the adversarial training to the text tasks is simple but non-trivial. Overall the paper is worth to publish. ----------------I only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze. ","This paper is concerned with extending adversarial and virtual adversarial training to text classification tasks. The main technical contribution is to apply perturbations to word embeddings rather than discrete input symbols. Excellent empirical performance is reported across a variety of tasks.     The reviewers were consensual in acknowledging the clarity and significance of the contribution, highlighting the quality of the numerical experiments. Moreover, the authors were responsive in the rebuttal phase and updated their paper with reviewers suggestions (such as the svm-related comparisons).     The AC thus recommends accepting this work as a poster.",This paper applies the idea of the adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Overall the paper is worth to publish.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper applies the idea of the adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Overall the paper is worth to publish., The paper is in general well written and easy to follow . Extending the idea of the adversarial training to the text tasks is simple but non-trivial . Overall the paper is worth to publish .,". Overall the paper is worth to publish. ---------------I only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a",0.34108527131782945,0.09448818897637795,0.18604651162790697,0.8859262466430664,0.8326921463012695,0.85848468542099
https://openreview.net/forum?id=r1X3g2_xl,"*** Paper Summary ***----------------This paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.----------------*** Review Summary ***----------------The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.----------------*** Detailed Review ***----------------The paper reads well. I have only a few comments regarding experiments and link to prior resarch:----------------Experiments:----------------- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?--------- As another baseline, did you consider dropping words, i.e. masking noise? It is generally better than dropout/gaussian noise for text application (e.g. denoising autoencoders)?--------- I am not sure I understand why virtual adversarial is worse than the baseline in Table 5. If you tune epsilon, in the worse case you would get the same performance as the baseline? Was it that validation was unreliable?----------------Related Work:----------------I think it would be interesting to point at SVM, transductive SVM who achieve something similar to adversarial training. When maximizing the margin in a (transductive) SVM, it is equivalent to move the example toward the decision boundary, i.e. moving them in the direction of increase of the loss gradient.----------------Also it would be interesting to draw a parallel between adversarial training and contrastive divergence. The adversarial samples are very close in nature to the one step Markov Chain samples from CD. See Bengio 2009. Related to this technique are also approaches that try to explicitely cancel the Jacobian at data points, e.g. Rifai et al 2011.----------------*** References ***----------------Marginalized Denoising Autoencoders for Domain Adaptation. Minmin Chen, K Weinberger.--------Stacked Denoising Autoencoders. Pascal Vincent. JMLR 2011.--------Learning invariant features through local space contraction, Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua Bengio and Pascal Vincent, 2011.--------Learning Deep Architectures for AI, Yoshua Bengio 2009--------Large Scale Transductive SVMs. Ronan Collobert et al 2006--------Optimization for Transductive SVM.  O Chapelle, V Sindhwani, SS Keerthi JMLR 2008","This paper is concerned with extending adversarial and virtual adversarial training to text classification tasks. The main technical contribution is to apply perturbations to word embeddings rather than discrete input symbols. Excellent empirical performance is reported across a variety of tasks.     The reviewers were consensual in acknowledging the clarity and significance of the contribution, highlighting the quality of the numerical experiments. Moreover, the authors were responsive in the rebuttal phase and updated their paper with reviewers suggestions (such as the svm-related comparisons).     The AC thus recommends accepting this work as a poster.",This paper applies adversarial and virtual adversarial training to LSTM for text classification. Extensive experiments are reported and demonstrate the advantage of these methods.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper applies adversarial and virtual adversarial training to LSTM for text classification. Extensive experiments are reported and demonstrate the advantage of these methods., The application of adversarial training to text data is a simple but not trivial extension . The proposed method is simple and effective and can be easily applied after reading the paper .,-------------------The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative,0.27350427350427353,0.12173913043478261,0.23931623931623933,0.9220284819602966,0.8345921635627747,0.8761342167854309
https://openreview.net/forum?id=Sy7m72Ogg,"The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:-----------------What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.---------Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.---------Since learning is a non-stationary process, while RL algorithms assume a stationary environment, why should we expect an RL algorithm to work for learning a learning rate?---------In figure 6, how does the proposed method compare to something like early stopping? It may be that the actor-critic method is overfitting less simply because it is worse at optimization.","The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify.",The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP. The method is presented against popular adaptive first-order methods for training deep networks.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP. The method is presented against popular adaptive first-order methods for training deep networks., Authors present a method for adaptingively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function . Method is presented against popular adaptive first-order methods for training,.------------What is the computational overhead of the actor-critic algorithm relative to other algorithms?------------What is the computational overhead of the actor-critic algorithm relative to other algorithms?---------------,0.2597402597402597,0.08,0.2077922077922078,0.878149151802063,0.8746500611305237,0.8763960599899292
https://openreview.net/forum?id=Sy7m72Ogg,"The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.----------------I have two main concerns. One is the lack of comparisons to similar recently proposed methods - ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?----------------My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in Table 2 and Table 3? These results suggest that the methods are not being tuned properly, which reinforces the need for comparisons on standard architectures with previously reported results. For example, if the baselines used a better architecture like a ResNet or, for simplicty, Network in Network from this list:--------http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html--------there wouldn't be a question of how well they are being tuned. Beating a previously published result on a well known architecture would be much more convincing.","The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify.","The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop."," The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning . The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIF","- ""Learning Step Size Controllers for Robust Neural Network Training"" by Daniel et al. and ""Learning to learn by gradient descent by gradient descent"" by Andrychowicz et al.",0.3582089552238806,0.12307692307692308,0.29850746268656714,0.8764156103134155,0.880170464515686,0.8782890439033508
https://openreview.net/forum?id=Sy7m72Ogg,"In the question response the authors mention and compare other works such as ""Learning to Learn by Gradient Descent by Gradient Descent"", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.--------The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.--------As discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.--------In summary, the general idea of having an actor-critic network as a meta-learner is an interesting idea. But the particular application proposed here does not seems to have any practical value and the reported results are very limited and it's hard to draw any conclusion about the effectiveness of the method. ","The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify.",The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. The extra cost of hyper-parameter optimizers are only justified if,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. The extra cost of hyper-parameter optimizers are only justified if," The idea of having an actor-critic network as a meta-learner is an interesting idea, but the particular application proposed here does not seems to have any practical value . The reported results are very limited and it's hard to",of hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.--------The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances,0.2531645569620253,0.025974025974025976,0.20253164556962025,0.8457239866256714,0.8568335175514221,0.8512424826622009
https://openreview.net/forum?id=HJ6idTdgg,"Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with--------batch normalization for pedestrian detection----------------Review summary: results do not cover enough datasets, the reported--------results do not improve over state of the art, writing is poor, and--------overall the work lacks novelty. This is a clear reject.----------------Pros:--------* Shows that using batch normalization does improve results----------------Cons:--------* Only results on ETH and INRIA. Should include Caltech or KITTI.--------* Reported results are fair, but not improving over state of the art--------* Overall idea of limited interest when considering works like S.--------Zhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang--------ECCV 2017 (Faster R-CNN for pedestrian detection)--------* Issues with the text quality--------* Limited takeaways----------------Quality: low--------Clarity: fair, but poor English--------Originality: low--------Significance: low----------------For acceptance at future conferences, this work would need more--------polish, improving over best known results on INRA, ETH, and Caltech or--------KITTI. And ideally, present additional new insights.----------------Minor comments:--------* The text lacks polish. E.g. influent -> influence, has maken ->--------made, is usually very important -> is important, achieve more--------excellent results -> achieve better results; etc. Please consider--------asking help from a native speaker for future submissions. There are--------also non-sense sentences such as “it is computational”.--------* Citations should be in parentheses--------* Some of the citations are incorrect because the family name is in--------the wrong position, e.g. Joseph Lim, Lawrence Zitnick, and Rodrigo Benenson.",Four knowledgable reviewers recommend rejection due to too weak of a contribution. The authors did not post a rebuttal. The AC agrees with the reviewers' recommendation.,"Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN withbatch normalization for pedestrian detection. Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor,",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN withbatch normalization for pedestrian detection. Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor,"," Paper proposed to use EdgeBoxes + Fast-RCNN with batch normalization for pedestrian detection . Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and the work lacks","-------------- Text lacks polish. E.g. influent -> influence, has maken ->-----made, is usually very important -> is important, achieve more--------excellent results",0.22950819672131148,0.03389830508474576,0.16393442622950818,0.8319058418273926,0.8543429970741272,0.8429751992225647
https://openreview.net/forum?id=HJ6idTdgg,"This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method. The proposed method is evaluated in INRIA and ETH dataset.----------------Pros:--------- The proposed method shows good performance(but not state-of-the-art).----------------Cons:--------- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, https://arxiv.org/abs/1506.01497; Faster RCNN with ResNet, https://arxiv.org/abs/1512.03385) are widely used in object detection tasks in many literatures. Meanwhile, batch normalization is a common practice to train / tune deep networks. It is not new to use batch normalization in CNNs for object / pedestrian detection. ----------------- The authors claim using EdgeBoxes for pedestrian proposal is another main contribution, and it is interesting that EdgeBoxes (93% recall) performs significantly better than Selective Search (23% recall) on INRIA dataset. It will be better to compare proposal generation method in more details (running time, recall, overlap statistics and number of proposals used), and preferably with more recent methods (RPN in https://arxiv.org/abs/1506.01497, YOLO in https://arxiv.org/abs/1506.02640, and SSD in https://arxiv.org/abs/1512.02325).  ----------------In summary, the contribution of this paper is minor. It cannot be accepted.",Four knowledgable reviewers recommend rejection due to too weak of a contribution. The authors did not post a rebuttal. The AC agrees with the reviewers' recommendation.,This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search. The proposed method is evaluated in INRIA and ETH dataset.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search. The proposed method is evaluated in INRIA and ETH dataset., This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization . EdgeBoxes is used to collect pedestrian proposals instead of selective search . The proposed method is evaluated in INRIA and ETH dataset .,".----------------Cons:--------- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, https://arxiv.org/abs/1506.0",0.16393442622950818,0.0,0.09836065573770493,0.8266216516494751,0.839931070804596,0.8332231640815735
https://openreview.net/forum?id=HJ6idTdgg,"The authors apply the commonly used Fast RCNN detection system to pedestrian detection. They use “EdgeBoxes” object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets. They are reasonable but not state-of-the-art. Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection. Perhaps more importantly, the paper has no novelty.----------------The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. EdgeBoxes has been used with Fast RCNN before. The authors don’t seem to be aware of more recent developments in object detection, including Faster RCNN (https://arxiv.org/abs/1506.01497), which uses a deep net to perform object proposals and performs much better than EdgeBoxes, or ResNets (https://arxiv.org/abs/1512.03385) which give a huge boost to Fast RCNN. The authors also propose that using batch norm inside of Fast RCNN is an innovation. It is not, the ResNet Faster RCNN system definitely uses batch norm (as really do most modern implementation of most variants of RCNN). ----------------While the system described in this paper is reasonable, it is quite dated at this point (judging by the fast progress in object detection). The authors should also see the paper “Is Faster R-CNN Doing Well for Pedestrian Detection?” (https://arxiv.org/abs/1607.07032) which shows a substantially improved result building on Faster RCNN for pedestrian detection. Finally, most recent research on pedestrian detection has been performed on Caltech Peds, that dataset is a must for publishing in this space.----------------The paper cannot be accepted.",Four knowledgable reviewers recommend rejection due to too weak of a contribution. The authors did not post a rebuttal. The AC agrees with the reviewers' recommendation.,"The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. Results are shown on the INR",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. Results are shown on the INR"," The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection . The implementation is not state-of-the-art, and there is no novelty in this work . Results are shown on the INR",is a standard application of Fast RCNN to pedestrian detection. They use “EdgeBoxes” object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets.,0.2153846153846154,0.0,0.15384615384615385,0.8393883109092712,0.8421372771263123,0.8407605886459351
https://openreview.net/forum?id=HJ6idTdgg,"This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.----------------Pros--------- The paper is clearly written and easy to follow----------------Cons--------- The paper's two contributions are too minor to merit publication--------- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset--------- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed----------------My rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing that other proposal methods (of which there are a great many in addition to Selective Search and Edge Boxes) are worse performing in some regard (speed, accuracy, memory, etc.). The second claimed contribution is the use of batch normalization (BN) in their network architecture. There is a case to be made that BN hasn't been explored in Fast R-CNN. However, if the goal of the paper was to thoroughly explore BN + Fast R-CNN, then why focus narrowly on pedestrian detection? Instead, it should focus more broadly on generic object category detection for which there are well established Fast R-CNN baselines on PASCAL VOC and COCO. The use of BN + Fast R-CNN only for pedestrian detection does not provide much signal about this choice. There are also potential technical issues that are not discussed. BN is typically avoided in Fast R-CNN because the batch size seen by most of the network is usually only one or two images. This is likely too few images for the naive application of BN.------------------------[a] ""Is Faster R-CNN Doing Well for Pedestrian Detection?"" Zhang et al.",Four knowledgable reviewers recommend rejection due to too weak of a contribution. The authors did not post a rebuttal. The AC agrees with the reviewers' recommendation.,This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH, This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians . It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization . The paper's two,for pedestrian detection. This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include,0.06779661016949153,0.0,0.06779661016949153,0.8078256249427795,0.8374043703079224,0.8223490715026855
https://openreview.net/forum?id=S1Bm3T_lg,"This paper proposes a new learning model ""Compositional Kernel Machines (CKMs)"" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets' success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper ""Deep Convolutional Networks are Hierarchical Kernel Machines"" by Anselmi, F. et al. seems to be interesting to the authors.--------Experiments seem to be preliminary in this paper. It's good to see promising results of CKMs on small NORB, but it is quite important to show competitive results on recent classification standard benchmarks, such as MNIST, CIFAR10/100 and even Imagenet, in order to establish a novel learning model. In NORB compositions, CKMs seem to be better than convnets at classifying images by their dominant objects. I suspect it is because the use of sparse ORB features. It will be great if this paper could show the accuracy of ORB features with matching kernel SVMs. Some details about this experiment need further clarification, such as what are the high and low probabilities of sampling from each collections and how many images are generated. In NORB Symmetries, CKMs show better performance than convnets with small data, but the convnets seem not converged yet. Could it be possible to show results with larger dataset?","There is consensus among the reviewers that the proposed method has potential merit, but that the experimental evaluation is too preliminary to warrant publication of the current manuscript. The paper also appears to make broad claims that are not fully supported by the results of the study. I encourage the authors to address the comments of the reviewers in future revisions of this work. Meanwhile, this paper would make a good contribution to the workshop track.",Compositional Kernel Machines (CKMs) extend the classic kernel machines by constructing compositional kernel functions using sum-product networks. CKMs seem to be better than convnets at classifying images by their dominant objects.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Compositional Kernel Machines (CKMs) extend the classic kernel machines by constructing compositional kernel functions using sum-product networks. CKMs seem to be better than convnets at classifying images by their dominant objects.," This paper considers convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature . In NORB compositions, CKMs seem to be better than convnets at classifying images by their dominant objects .",". This paper proposes a new learning model ""Compositional Kernel Machines (CKMs)"" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper consider",0.055555555555555546,0.0,0.055555555555555546,0.8003467321395874,0.8301748037338257,0.8149879574775696
https://openreview.net/forum?id=S1Bm3T_lg,"The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.----------------Pros:--------- Thoughtful methodology with sensible design choices--------- Potentially useful for smaller (n < 10000) datasets with a lot of statistical structure--------- Nice connections with sum-product literature----------------Cons:--------- Claims about scalability are very unclear--------- Generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.--------- Experiments are very preliminary ----------------The scalability claims are particularly unclear. The paper repeatedly mentions lack of scalability as a drawback for convnets, but it appears the proposed CKM is less scalable than a standard SVM, yet SVMs often handle much fewer training instances than deep neural networks. It appears the scalability advantages are mostly for training sets with roughly fewer than 10,000 instances -- and even if the method could scale to >> 10,000 training instances, it's unclear whether the predictive accuracy would be competitive with convnets in that domain. Moreover, the idea of doing 10^6 operations simply for creating virtual instances on 10^4 training points and 100 test points is still somewhat daunting. What if we had 10^6 training instances and 10^5 testing instances?  Because scalability (in the number of training instances) is one of the biggest drawbacks of using SVMs (e.g. with Gaussian kernels) on modern datasets, the scalability claims in this paper need to be significantly expanded and clarified. On a related note, the suggestion that convnets grow quadratically in computation with additional training instances in the introduction needs to be augmented with more detail, and is potentially misleading. Convnets typically scale linearly with additional training data. ----------------In general, the paper suffers greatly from a lack of clarity and issues of presentation. As above, the full story is not presented, with critical details often missing. Moreover, it would strengthen the paper to remove broad claims such as ""Just as support vector machines (SVMs) eclipsed multilayer perceptrons in the 1990s, CKMs could become a compelling alternative to convnets with reduced training time and sample complexity"", suggesting that CKMs could eclipse convolutional neural networks, and instead provide more helpful and precise information. Convnets are multilayer perceptrons used in the 1990s (as well as now) and they are not eclipsed by SVMs -- they have different relative advantages. And based on the information presented, broadly advertising scalability over convnets is misleading. Can CKMs scale to datasets with millions of training and test instances?  It seems as if the scalability advantages are limited to smaller datasets, and asymptotic scalability could be much worse in general. And even if CKMs could scale to such datasets would they have as good predictive accuracy as convnets on those applications? Being specific and with full disclosure about the precise strengths and limitations of the work would greatly improve this paper.----------------CKMs may be more robust to adversarial examples than standard convnets, due to the virtual instances. But there are many approaches to make deep nets more robust to adversarial examples. It would be useful to consider and compare to these. The ideas behind CKMs also are not inherently specific to kernel methods. Have you considered looking at using virtual instances in a similar way with deep networks? A full exploration might be its own paper, but the idea is worth at least brief discussion in the text. ----------------A big advantage of SVMs (with Gaussian kernels) over deep neural nets is that one can achieve quite good performance with very little human intervention (design choices). However, CKMs seem to require extensive intervention, in terms of architecture (as with a neural network), and in insuring that the virtual instances are created in a plausible manner for the particular application at hand. It's very unclear in general how one would want to create sensible virtual instances and this topic deserves further consideration. Moreover, unlike SVMs (with for example Gaussian or linear kernels) or standard convolutional networks, which are quite general models, CKMs as applied in this paper seem more like SVMs (or kernel methods) which have been highly tailored to a particular application -- in this case, the NORB dataset. There is certainly nothing wrong with the tailored approach, but it would help to be clear and detailed about where the presented ideas can be applied out of the box, or how one would go about making the relevant design choices for a range of different problems. And indeed, it would be good to avoid the potentially misleading suggestions early in the paper that the proposed method is a general alternative to convnets.----------------The experiments give some insights into the advantages of the proposed approach, but are very limited. To get a sense of the properties --the strengths and limitations -- of the proposed method, one needs a greater range of datasets with a much larger range of training and test sizes. The comparisons are also quite limited: why not an SVM with a Gaussian kernel?  What about an SVM using convnet features from the dataset at hand (light blue curve in figure 3) -- it should do at least as well as the light blue curve. There are also other works that could be considered which combine some of the advantages of kernel methods with deep networks. Also the claim that the approach helps with the curse of dimensionality is sensible but not particularly explored. It also seems the curse of dimensionality could affect the scalability of creating a useful set of virtual instances. And it's unclear how CKM would work without any ORB features. ----------------Even if the method can (be adapted to) scale to n >> 10000, it's unclear whether it will be more useful than convnets in that domain. Indeed, in the experiments here, convnets essentially match CKMs in performance after 12,000 examples, and would probably perform better than CKMs on larger datasets.  We can only speculate because the experiments don't consider larger problems.----------------The methodology largely takes inspiration from sum product networks, but its application in the context of a kernel approach is reasonably original, and worthy of exploration. It's reasonable to expect the approach to be significant, but its significance is not demonstrated.----------------The quality is high in the sense that the methods and insights are thoughtful, but suffers from broad claims and a lack of full and precise detail.----------------In short: I like the paper, but it needs more specific details, and a full disclosure of where the method should be most applicable, and its precise advantages and limitations.  Code would be helpful for reproducibility.","There is consensus among the reviewers that the proposed method has potential merit, but that the experimental evaluation is too preliminary to warrant publication of the current manuscript. The paper also appears to make broad claims that are not fully supported by the results of the study. I encourage the authors to address the comments of the reviewers in future revisions of this work. Meanwhile, this paper would make a good contribution to the workshop track.","The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims."," The authors propose a method to efficiently augment an SVM variant with many virtual instances . The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims . The scalability advantages are mostly for training sets with",". It seems the scalability advantages are mostly for training sets with roughly fewer than 10,000 instances -- and even if the method could scale to >> 10,000 training instances, it's unclear if the predictive accuracy would be competitive",0.247787610619469,0.036036036036036036,0.1238938053097345,0.8677047491073608,0.8594014644622803,0.8635331392288208
https://openreview.net/forum?id=S1Bm3T_lg,"Thank you for an interesting read. The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.----------------Pros--------- The approach seems to decrease the training time, which is of prime importance in deep learning. Although, that comes at a price of slightly more complex model.--------- There is a grounded theory for sum-product functions which is basis for the compositional architecture described in the paper. Theoretically, any semiring and kernel could be used for the model which decreases need for handcrafting the structure of the model, which is a big problem in existing convolutional neural networks.----------------Cons--------- The experiments are on very simple dataset NORB. Although, it is great to understand a model's dynamics on a simpler dataset, some analysis on complex datasets are important to act as empirical evidence. The compositional kernel approach is compared to convolutional neural networks, hence it is only fair to compare said results on large datasets such as Imagenet.----------------Minor--------- Section 3.4 claims that CKMs model symmetries of objects. It felt that ample justification was not provided for this claim","There is consensus among the reviewers that the proposed method has potential merit, but that the experimental evaluation is too preliminary to warrant publication of the current manuscript. The paper also appears to make broad claims that are not fully supported by the results of the study. I encourage the authors to address the comments of the reviewers in future revisions of this work. Meanwhile, this paper would make a good contribution to the workshop track.","The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence. The approach seems to decrease the training time, which is of prime importance in deep learning.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence. The approach seems to decrease the training time, which is of prime importance in deep learning."," The ideas presented have a good basis of being true, but the experiments are rather too simple . The approach seems to decrease the training time, which is of prime importance in deep learning .",.---------------Cons------------ The experiments are on very simple dataset NORB.------------------Cons------------ The compositional kernel approach is compared to convolutional neural networks,0.2711864406779661,0.017241379310344827,0.1694915254237288,0.8793308138847351,0.8607560992240906,0.8699442744255066
https://openreview.net/forum?id=S1Bm3T_lg,"This paper proposes a new learning framework called ""compositional kernel machines"" (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.----------------Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). ----------------Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do not see any advantage of the proposed framework over convnet. I will elaborate below.----------------1) One of the most important claims of this paper is that CKM is faster to learn than convnet. I am not clear why that is the case. Both CKM and convnet use gradient descent during learning, why would CKM be faster?----------------Also during inference, the running time of convnet only depends on its network structure. But for CKM, in addition to the network structure, it also depends on the size of training set. From this perspective, it does not seem CKM is very scalable when the training size is big. That is probably why this paper has to use all kinds of specialized data structures and tricks (even on a fairly simple dataset like NORB)----------------2) I am having a hard time understanding what the leaf kernel is capturing. For example, if the ""elements"" correspond to raw pixel intensities, a leaf kernel essentially compares the intensity value of a pixel in the query image with that in a training image. But in this case, wouldn't you end up comparing a lot of background pixels across these two images (which does not help with recognition)?----------------I think it probably helps to explain Sec 3.1 a bit better. In its current form, this part is very dense and hard to understand.----------------3) It is also not entirely clear to me how you would design the architecture of the sum-product function. The example is Sec 3.1 seems to be fairly arbitrary.----------------4) The experiment section is probably the weakest part. NORB is a very small and toy-ish dataset by today's standard. Even on this small dataset, the proposed method is only slighly better than SVM (it is not clear whether ""SVM"" in Table 2 is linear SVM or kernel SVM. If it is linear SVM, I suspect the performance of ""SVM"" will be even higher when you use kernel SVM), and far worse than convnet. The proposed method only shows improvement over convnet on synthetic datasets (NORB compositions, NORM symmetries)----------------Overall, I think this paper has some interesting ideas. But in its current form, it is a bit too preliminary and more work is needed to show its advantage. Having said that, I acknowledge that in the machine learning history, many important ideas seem pre-mature when they were first proposed, and it took time for these ideas to develop. ","There is consensus among the reviewers that the proposed method has potential merit, but that the experimental evaluation is too preliminary to warrant publication of the current manuscript. The paper also appears to make broad claims that are not fully supported by the results of the study. I encourage the authors to address the comments of the reviewers in future revisions of this work. Meanwhile, this paper would make a good contribution to the workshop track.","This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines two ideas: kernel methods and sum-product network (SPN) CKM first defines leaf kernels on elements of the query",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines two ideas: kernel methods and sum-product network (SPN) CKM first defines leaf kernels on elements of the query"," This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines kernel methods and sum-product network (SPN) CKM first defines leaf kernels on elements of the query and training examples","of this paper is interesting, but this paper is clearly very preliminary. This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines two ideas: kernel methods and sum-product",0.12844036697247707,0.037383177570093455,0.07339449541284404,0.8032189011573792,0.8338841795921326,0.8182643055915833
https://openreview.net/forum?id=BJxhLAuxg,"This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.----------------Pros:--------- Paper is well written and easy to follow.--------- Model is clear to understand.----------------Cons:--------- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.----------------This paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.","The authors have combined two known areas of research - frame prediction and reward prediction - and combined them in a feedforward network trained on sequences from Atari games. The fact that this should train well is unsurprising for this domain, and the research yields no other interesting results. Pros - the paper is clearly written and the experiments are sound. Cons - there is very little novelty or contribution.","This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames."," This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction . In Atari game playing scenarios, this model can successfully predict both reward and next frames .",model can successfully predict reward and next frames.----------------Pros:--------- Paper is well written and easy to follow.-------------Model is clear to understand.-------------Cons,0.29411764705882354,0.1,0.1764705882352941,0.8786424398422241,0.8527236580848694,0.8654890060424805
https://openreview.net/forum?id=BJxhLAuxg,"The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would be very interesting and impactful.","The authors have combined two known areas of research - frame prediction and reward prediction - and combined them in a feedforward network trained on sequences from Atari games. The fact that this should train well is unsurprising for this domain, and the research yields no other interesting results. Pros - the paper is clearly written and the experiments are sound. Cons - there is very little novelty or contribution.",The paper extends a recently proposed video frame prediction method with reward prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper extends a recently proposed video frame prediction method with reward prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps., The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps . The results are not really surprising given that the system state and the reward are linked deterministically in Atari games,. The paper extends a recently proposed video frame prediction method with reward prediction to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite,0.3300970873786408,0.07920792079207921,0.21359223300970873,0.8883023262023926,0.8555715084075928,0.8716297745704651
https://openreview.net/forum?id=BJxhLAuxg,"The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.----------------I’m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. ","The authors have combined two known areas of research - frame prediction and reward prediction - and combined them in a feedforward network trained on sequences from Atari games. The fact that this should train well is unsurprising for this domain, and the research yields no other interesting results. Pros - the paper is clearly written and the experiments are sound. Cons - there is very little novelty or contribution.","The topic of the paper, model-based RL with a learned model, is important and timely. I feel that the presented results are too incremental. Neither the methodology not the results are novel / surprising.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The topic of the paper, model-based RL with a learned model, is important and timely. I feel that the presented results are too incremental. Neither the methodology not the results are novel / surprising.", The paper is well written. I feel that the presented results are too incremental . Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do . Neither the methodology nor the results are novel /,"of model-based RL with a learned model is important and timely. The paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are",0.25999999999999995,0.020408163265306124,0.19999999999999998,0.8756020069122314,0.8507380485534668,0.8629909157752991
https://openreview.net/forum?id=rkKCdAdgx,"Description:----------------This paper aims at compressing binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings"".----------------The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.----------------Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs.----------------Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.----------------The paper simply applies this idea, testing it on seven data sets. Scores and training times are compared to the baseline networks without embeddings. Comparison embedding methods are mostly very traditional (hashing trick, error-correcting output codes, linear projection by canonical correlation analysis) but include one recent pairwise mutual information based approach.------------------------Evaluation:----------------It is hard to see a lot of novelty in this paper. The Bloom filters are an existing technique, which is applied very straightforwardly here. The back-projection step of equation 2 is also a straightforward continuous-valued variant of the Bloom-filter membership test.----------------The way of recovering outputs is heuristic, since the neural network inbetween the embedded inputs and outputs is not really aware that the outputs will be run through a further back-projection step.----------------In the comparisons of Table 3, only two embedding dimensionalities are used for each data set. This is insufficient, since it leaves open the question whether other methods could get improved performance for higher/lower embeddings, relative to the proposed method. (In appendix B, Figure 4, authors do compare their method to a variant of it for many different embedding dimensionalities; why not do this for all comparison methods too?)----------------Overall, this seems for the most part too close to off-the-shelf existing embedding to be acceptable.------------------------Minor points:----------------As the paper notes, dimensionality reduction of inputs by various techniques is common. The paper lists some simple embeddings such as SVD based ones, CCA etc. but a more thorough review of other approaches including the vast array of nonlinear dimensionality reduction solutions should be mentioned.----------------The experiments in the paper seem to have an underlying assumption that inputs and outputs need to have the same type of embedding dimension. This seems unnecessary.","The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline.","This paper aims at compressing binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings"" The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper aims at compressing binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings"" The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by"," A paper aims at compressing binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings"" The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by",".----------------This paper aims at compressing binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings""-------------------The paper simply applies this idea",0.3366336633663366,0.12121212121212119,0.1782178217821782,0.8578277230262756,0.8471779227256775,0.8524695634841919
https://openreview.net/forum?id=rkKCdAdgx,"The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) ""Hash Kernels for Structured Data"" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) ""Robust Bloom Filters for Large MultiLabel Classification Tasks"". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.----------------The main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/quantization). More recent methods are e.g.,----------------1) the model compression approach of https://arxiv.org/abs/1510.00149--------2) training with integer/binary weights https://arxiv.org/abs/1511.00363----------------Overall, the advantage of the Bloom filter approach is its simplicity and its wide applicability -- it could, as well, be used in conjunction with weight quantization and other compression methods. The main merits of the paper is to present extensive experiments on how well the vanilla Bloom filter approach can perform, but overall the novelty is fairly limited.","The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline.",The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. The main motivation of the paper is the reduction of model size for deep neural networks,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. The main motivation of the paper is the reduction of model size for deep neural networks, The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network . The advantage of the Bloom filter approach is its simplicity and its wide applicability .,encodings on the input features and the output layer of deep network to reduce the size of the network. The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to,0.35514018691588783,0.07619047619047618,0.2616822429906542,0.8766293525695801,0.8540109992027283,0.8651723265647888
https://openreview.net/forum?id=rkKCdAdgx,"The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.----------------Pros:----------------- Can be applied to practically any model, either at the input or hte output.--------- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. ----------------Cons:----------------- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.--------- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.----------------This seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental.","The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline.",The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. This reduces memory while introducing limited computational overhead. It is simple enough to implement that it can easily be added to any practitioner's toolbox,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. This reduces memory while introducing limited computational overhead. It is simple enough to implement that it can easily be added to any practitioner's toolbox," The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs . This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's tool",a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. This reduces memory while introducing limited computational overhead.--------- The novelty of the approach is a bit limited since,0.5142857142857143,0.4077669902912621,0.4571428571428571,0.9270750880241394,0.8969479203224182,0.9117627143859863
https://openreview.net/forum?id=r1fYuytex,"Experimental results look reasonable, validated on 3 tasks. --------References could be improved, for example I would rather see--------Rumelhart's paper cited for back-propagation than the Deep Learning book.","After discussion, the reviewers unanimously recommend accepting the paper.","Experimental results look reasonable, validated on 3 tasks. I would rather see Rumelhart's paper cited for back-propagation than the Deep Learning book.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Experimental results look reasonable, validated on 3 tasks. I would rather see Rumelhart's paper cited for back-propagation than the Deep Learning book."," Experimental results look reasonable, validated on 3 tasks .","results look reasonable, validated on 3 tasks. --------References could be improved, for example I would rather see--------Rumelhart's paper cited for back-propagation than the Deep Learning",0.12121212121212122,0.0,0.06060606060606061,0.8424068689346313,0.8719006180763245,0.8568999767303467
https://openreview.net/forum?id=r1fYuytex,"The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. --------The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.--------The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. --------This explanation would be clearer to me: In this paper, we propose sparsely-connected networks by randomly removing some of the connections in fully-connected networks. Random connection masks are generated by LFSR, which is also used in the VLSI implementation to disable the connections.--------Algorithm 1 is basically training a network with back-propogation where each layer has a binary mask that disables some of the connections. This explanation can be added to the text.--------Using random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf It was not used in fully connected layer before. The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up. Also the author's VLSI implementation does not speed up the network inference.--------How are the results of this work compared to Network in Network (NiN)? https://arxiv.org/abs/1312.4400 In NiN, the authors removed the fully connected layers completely and used a cheap pooling operation and also got improved performance. Are the results presented here better? It would be more convincing to see this method tested on ImageNet, which actually uses a big fully connected layer. ----------------Increased my rating from 5-6 after rebuttal.","After discussion, the reviewers unanimously recommend accepting the paper.",The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. The sparsity in fully connected layer decreases the computational burden but it is,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. The sparsity in fully connected layer decreases the computational burden but it is," The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks . It would be more convincing to see this method tested on ImageNet,","in fully connected layer before. The sparsity in fully connected layer decreases computational burden but it is difficult to speed up. --------This explanation would be clearer to me: In this paper, we propose sparse",0.11538461538461539,0.04,0.07692307692307693,0.8208327889442444,0.8709306120872498,0.8451399207115173
https://openreview.net/forum?id=r1fYuytex,"From my original comments:----------------The results looks good but the baselines proposed are quite bad.----------------For instance in the table 2 ""Misclassification rate for a 784-1024-1024-1024-10 "" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see ""significant"" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...----------------In CIFAR-10 experiments, i do not understand  why ""Sparsely-Connected 90% + Single-Precision Floating-Point"" is worse than ""Sparsely-Connected 90% + BinaryConnect"". So it is better to use binary than float. ----------------Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian noise or other regularizations). Therefore under my point of view the comparison between float and binary is not fair. This is a critic also for the original papers about binary and ternary precision. ----------------In fact with this convolutional network, floating (standard) precision we can get lower that 9% of error rate. Again bad baselines.------------------------------------The authors reply still does not convince me.----------------I still think that the same technique should be applied on more challenging scenarios.","After discussion, the reviewers unanimously recommend accepting the paper.","The results looks good but the baselines proposed are quite bad. I would like to see ""significant"" compression levels on state of the art results.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The results looks good but the baselines proposed are quite bad. I would like to see ""significant"" compression levels on state of the art results."," The results looks good but the baselines proposed are quite bad . I would like to see ""significant"" compression levels .","is better than ""Sparsely-Connected 90% + Single-Precision Floating-Point"" is worse than ""Sparsely-Connected 90% + BinaryConnect"". So it is better to",0.11764705882352941,0.0,0.11764705882352941,0.8507542610168457,0.8695942163467407,0.8600711226463318
https://openreview.net/forum?id=SJU4ayYgl,"The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. ----------------Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  ",The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.,The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups., The paper develops a simple and reasonable algorithm for graph node prediction/classification . It can easily leverage existing GPU speedups .,for graph node prediction/classification. The paper develops a simple and reasonable algorithm for graph node prediction/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage,0.3,0.0,0.16666666666666666,0.8667132258415222,0.8661225438117981,0.8664177656173706
https://openreview.net/forum?id=SJU4ayYgl,"The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.----------------The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and training time, with regularization based approaches, although they are mostly used in inductive settings. Below are some references for this family of methods.----------------The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer. They should comment on the interest of using more layers for graph classification.------------------------Some references on iterative classification Qing Lu and Lise Getoor. 2003. Link-based classification. In ICML, Vol. 3. 496–503.----------------Gideon S Mann and Andrew McCallum. 2010. Generalized expectation criteria for semi-supervised learning with weakly labeled data. The Journal of Machine Learning Research 11 (2010), 955–984.--------David Jensen, Jennifer Neville, and Brian Gallagher. 2004. Why collective inference improves relational classification. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 593–598.--------Joseph J Pfeiffer III, Jennifer Neville, and Paul N Bennett. 2015. Overcoming Relational Learning Biases--------to Accurately Predict Preferences in Large Scale Networks. In Proceedings of the 24th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 853–--------863.--------Stephane Peters, Ludovic Denoyer, and Patrick Gallinari. 2010. Iterative annotation of multi-relational social networks. In Advances in Social Networks Analysis and Mining (ASONAM), 2010 International Conference on. IEEE, 96–103.",The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.,The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large, The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation . The proposed algorithm has a limited complexity and it is shown to scale well on a large,.----------------The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has ,0.4411764705882353,0.060606060606060615,0.2058823529411765,0.87623530626297,0.8850813508033752,0.8806360960006714
https://openreview.net/forum?id=SJU4ayYgl,"This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.----------------This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.----------------It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2-hop neighborhood, and no longer range interactions can play any roles in this.  Since computation is quite efficient (sec. 6.3), I wonder if adding more layers helped anything or not.----------------Even though motivated from graph convolutions, when simplified as the paper suggests, the operations the model does are quite simple.  Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is simpler and does almost strictly less things.  So how would the proposed GCN compare against these methods?----------------Overall I think this model is simple, but the connection to graph convolutions is interesting, and the experiment results are quite good.  There are a few questions that still remain, but I feel this paper can be accepted.",The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.,"This model is used for semi-supervised learning on graphs. In the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin. Compared to Duvenaud et al. 2015 and Li et",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This model is used for semi-supervised learning on graphs. In the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin. Compared to Duvenaud et al. 2015 and Li et"," This paper proposes the graph convolutional networks, motivated from approximating graph convolutions . In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the",".----------------This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.-------------This paper proposes the graph convolutional networks, motivated from app",0.375,0.06451612903225808,0.1875,0.8403863906860352,0.8669461607933044,0.8534596562385559
https://openreview.net/forum?id=HJ7O61Yxe,"Because the authors did not respond to reviewer feedback, I am maintaining my original review score.-------------------------------------This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account. However, the model is restricted to using a linear decoder. Experimental results are positive but not convincing.----------------Strengths:--------- The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible neural net-like models.--------- The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states is clever. Combined with the Gaussian distribution over hidden states, the resulting regularization term is simple and differentiable.--------- This general approach -- focusing on writing down the problem as a neural network-like loss function -- seems robust and flexible and could be combined with other approaches, including variants of variational autoencoders.----------------Weaknesses:--------- The presentation is a muddled, especially the model definition in Sec. 3.3. The authors introduce four variants of their model with different combinations of decoder (with and without variance term) and linear vs. MLP transition function. It appears that the 2,2 variant is generally better but not on all metrics and often by small margins. This makes drawing a solid conclusions difficult: what each component of the loss contributes, whether and how the nonlinear transition function helps and how much, how in practice the model should be applied, etc. I would suggest two improvements to the manuscript: (1) focus on the main 2,2 variant in Sec. 3.3 (with the hypothesis that it should perform best) and make the simpler variants additional ""baselines"" described in a paragraph in Sec. 4.1; (2) perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.--------- The authors only allude to learning (with references to gradient descent and ADAM during model description) in this framework. Inference gets its one subsection but only one sentence that ends in an ellipsis (?).--------- It's unclear what is the purpose of introducing the inequality in Eq. 9.--------- Experimental results are not convincing: given the size of the data, the differences vs. the RNN and KF baselines is probably not significant, and these aren't particularly strong baselines (especially if it is in fact an RNN and not an LSTM or GRU).--------- The position of this paper is unclear with respect to variational autoencoders and related models. Recurrent variants of VAEs (e.g., Krishnan, et al., 2015) seem to achieve most of the same goals as far as uncertainty modeling is concerned. It seems like those could easily be extended to model relationships between time series using the simple regularization strategy used here. Same goes for Johnson, et al., 2016 (mentioned in separate question).----------------This is a valuable research direction with some intriguing ideas and interesting preliminary results. I would suggest that the authors restructure this manuscript a bit, striving for clarity of model description similar to the papers cited above and providing greater detail about learning and inference. They also need to perform more thorough experiments and present results that tell a clear story about the strengths and weaknesses of this approach.","The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.    I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue.","The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach. The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach. The idea of representing relationships between hidden states using KL divergence between their (distributions over) corresponding hidden states"," This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach . The authors target a worthwhile and challenging problem: incorporating the modeling of uncertainty over hidden states with the power of flexible","(with the hypothesis that it should perform best) and perform more thorough experiments with larger data sets to make a stronger case for the superiority of this approach.--------- The authors propose several elaborations, including ",0.08121827411167513,0.020512820512820513,0.06091370558375635,0.8098881244659424,0.8029413819313049,0.8063998222351074
https://openreview.net/forum?id=HJ7O61Yxe,"This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).----------------Modeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.----------------The formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1] for better uncertainty handling. Furthermore, the Gaussian co-variance collapses into a variance, which is the opposite of what one would want for modelling correlated time-series. There are approaches which take these correlations into account in the states, e.g. [2].----------------Moreover, the treatment of uncertainty only allows for linear decoding function f. This significantly reduces the power of the model. State of the art methods in timeseries modeling have moved beyond this constraint, especially in the Gaussian process community e.g. [2,3,4,5]. Comparing to a few of these methods, or at least discussing them would be useful.------------------------References:--------[1] Kingma and Welling. Auto-encoding Variational Bayes. arXiv:1312.6114--------[2] Damianou et al. Variational Gaussian process dynamical systems. NIPS 2011.--------[3] Mattos et al. Recurrent Gaussian processes. ICLR 2016.--------[4] Frigola. Bayesian Time Series Learning with Gaussian Processes, University of Cambridge, PhD Thesis, 2015. --------[5] Frigola et al. Variational Gaussian Process State-Space Models. NIPS 2014------------------------One innovation is that the prior structure of the correlation needs to be given. This is a potentially useful and also original structural component. However, it also constitutes a limitation in some sense, since it is unrealistic in many scenarios to have this prior information. Moreover, the particular regularizer that makes ""similar"" timeseries to have closeness in the state space seems problematic. Some timeseries groups might be more ""similar"" than others, and also the similarity might be of different nature across groups. These variations cannot be well captured/distilled by a simple indicator variable e_ij. Furthermore, these variables are in practice taken to be binary (by looking at the experiments), which would make it even harder to model rich correlations. ----------------The experiments show that the proposed method works, but they are not entirely convincing. Importantly, they do not shed enough light into the different properties of the model w.r.t its different parts. For example, the effect and sensitivity of the different regularizers. The authors state in a pre-review answer that they amended with some more results, but I can't see a revision in openreview (please let me know if I've missed it). From the performance point of view, the results are not particularly exciting, especially given the fact that it's not clear which loss is better (making it difficult to use the method in practice). ----------------It would also be very interesting to report the optimized values of the parameters \lambda, to get an idea of how the different losses behave.----------------Timeseries analysis is a very well-researched area. Given the above, it's not clear to me why one would prefer to use this model over other approaches. Methodology wise, there are no novel components that offer a proven advantage with respect to past methods. The uncertainty in the states and the correlation of the time-series are the aspects which could add an advantage, but are not adequately researched in this paper.","The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.    I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue.","This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconv",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconv"," Model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way . Methodology wise, there are no novel components that offer a proven advantage .",", but they are not entirely convincing. ------------------The experiments show that the proposed method works, but they are not entirely convincing.-------------------The results show that the proposed method works,",0.11940298507462686,0.0,0.07960199004975126,0.8299948573112488,0.8141100406646729,0.8219757080078125
https://openreview.net/forum?id=HJ7O61Yxe,"In absence of authors' response, the rating is maintained.-----------------------------------This paper introduces a nonlinear dynamical model for multiple related multivariate time series. It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable). The predictions/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix. Inference is forward only.----------------The model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.----------------The model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.----------------First of all, the paper should refer to variational auto-encoders / deep gaussian models, which also predict the mean and the variance during inference.----------------Secondly, the datasets are extremely small. For example, the WHO contains only 91 times series of 52*10 = 520 time points. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small and the high variance in the results indicates that further experiments, with longer time series, are required. The paper could also easily be extended with more information about the model (what is the architecture of the MLP) as well as time complexity comparison between the models (especially between DFGs and this model).----------------Minor remark:--------The footnote 2 on page 5 seems to refer to the structural regularization term, not to the dynamical term.","The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.    I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue.","The model is evaluated on four datasets, and compared to several baselines. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small and the high variance in the results indicates that",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The model is evaluated on four datasets, and compared to several baselines. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small and the high variance in the results indicates that"," Models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series . Models are Gaussian, because the model predicts both the mean and the","of the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable).-",0.22999999999999995,0.05050505050505051,0.12,0.8549135327339172,0.8207124471664429,0.837463915348053
https://openreview.net/forum?id=Sk36NgFeg,"This paper aims to characterize the perceptual ability of a neural network under different input conditions.  This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.  MSE and qualitative results are shown and compared for the different input conditions.----------------Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.  For example, at the end of sec 4.4, ""This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need...."".  But this hypothesis is left dangling:  What detailed regions are needed, and from where?  For what sort of tasks?----------------Secondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss?  A prime example is texture, which the autoencoder fails to recover.  But with a pixelwise loss, the network must predict high-frequency textures nearly pixel-for-pixel at training time; if this is impossible, then it will generate a pixelwise average of the training samples --- a flat region.  So then the network's inability to reconstruct textures is due to a problem generating them, specifically averaging from the training loss, not necessarily an issue in perceiving textures.  A network trained a different way (perhaps an adversarial network) may infer a texture is there, even if it wouldn't be able to generate it in a pixelwise l2 sense.----------------Similarly, the ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object/scene:  If there is an ambiguity, the network won't know which to ""choose"" (white flower or yellow flower?) and output an average, which is why there are so many sepia tones.  However, in its section on this, the paper only measures the reconstruction error for different amounts of color given, and does not drill very far into any hypotheses for why this behavior occurs.----------------There are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image, and the discussion on global features, which may start to get at a mechanism by which glimpses may propagate to an entire reconstruction.  But overall it's hard to know what to take away from this paper.  What are larger concrete conclusions that can be garnered from the details, and what mechanisms bring them about?  Can these be more thoroughly explored with more focus?","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.","This paper aims to characterize the perceptual ability of a neural network under different input conditions. It isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper aims to characterize the perceptual ability of a neural network under different input conditions. It isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and"," This paper aims to characterize the perceptual ability of a neural network under different input conditions . Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions .",", but this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.----------------This paper seems to lack focus, presenting a set of preliminary inspections with few",0.23157894736842105,0.021505376344086023,0.1894736842105263,0.821144700050354,0.836203932762146,0.8286058902740479
https://openreview.net/forum?id=Sk36NgFeg,"I like the idea the paper is exploring. Nevertheless I see some issues with the analysis:----------------- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.--------- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine this making a difference.--------- On page 4, you compare the performance of FOV-R (1% error) with that of DS-D (1.5%) and attribute this to information about the periphery that the autoencoder extracts from the fovea. While this might be the case, at least part of the reduced error will be due to the fact that the fovea is (hopefully) perfectly reconstructed. To answer the actual question ""how much additional information about the periphery can be extracted from the fovea"", you should consider calculating the error only in the periphery, i.e. the part of the image where DS-D and FOV-R got exactly the same input for. Then any decreased error is only due to the additional fovea information.----------------Other issues:--------- The images in Figure 2 (a) and (b) in the rows ""factor 2"", ""factor 4"", ""factor 8"" look very blurry. There seems some interpolation to be going on (although slighly different than the bilinear interpolation). This makes it hard to asses how much information is in these images. I think it would be much more insightfull to print them with ""nearest"" interpolation.--------- Figure 3 caption too vague. Maybe add something like footnote 2?--------- Often figures appear too early in paper which leads to lots of distance between text and figures.","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.","I like the idea the paper is exploring. Nevertheless I see some issues with the analysis. To get a better understanding of the quality of the results, I think some state-of-the-art comparisons should be included.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","I like the idea the paper is exploring. Nevertheless I see some issues with the analysis. To get a better understanding of the quality of the results, I think some state-of-the-art comparisons should be included."," SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone . However, there is an additional difference between the two conditions: According to the paper, part of the image",of FOV-R (1% error) with that of DS-D (1.5%) and FOV-R (1.5%). This makes it hard to asses how much information can be restored from the Fovea alone ,0.2417582417582418,0.02247191011235955,0.17582417582417584,0.8738255500793457,0.8591169118881226,0.8664088249206543
https://openreview.net/forum?id=Sk36NgFeg,"This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. ----------------I think the paper is well motivated. However, there are several concerns:--------1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as https://arxiv.org/abs/1609.04802--------2. Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks? I think it would be much better if the authors can provide a more general conclusion.","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger.",This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features. The baselines of the paper are too weak. Can the experiments based on AE support the idea that artificial neural networks can perceive,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features. The baselines of the paper are too weak. Can the experiments based on AE support the idea that artificial neural networks can perceive," The baselines of the paper are too weak . Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models .","to reconstruct the low fidelity of visual input. ---------------I think the paper is well motivated. There are several concerns:--------1. The baselines of the paper are too weak. Nearest neighbor,",0.18556701030927836,0.021052631578947368,0.12371134020618556,0.8311124444007874,0.8411229848861694,0.836087703704834
https://openreview.net/forum?id=BJ3filKll,"The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call ""monotonic chains of linear segments"", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.----------------While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very ""compatible"". In particular, I have three main concerns with respect to the results presented in this paper:----------------(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis of Zhang & Zha (2002), manifold charting by Brand (2002) and alignment of local models by Verbeek, Roweis, and Vlassis (2003). None of this work is referred to in related work, even though it seems highly relevant to the analysis presented here. For instance, it would be interesting to see how these old techniques compare to the deep network trained to produce the embedding of Figure 6. This may provide some insight into the inductive biases the deep net introduces: does it learn better representations that non-parametric techniques because it has better inductive biases, or does it learn worse representations because the loss being optimized is non-convex?----------------(2) It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous given the sparsity of data in high-dimensional space, or how it generalizes to deep network architectures that are not pure ReLU networks. For instance, most modern networks use a variant of batch normalization; this already appears to break the presented analyses.----------------(3) The error bound presented in Section 4 appears vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings). This is underlined by the analysis of the Swiss roll dataset, of which the authors state that the ""bound for this case is very loose"". The fact that the bound is already so loose for this arguably very simple manifold makes that the error analysis may tell us very little about the representational power of deep nets.----------------I would encourage the authors to address issue (1) in the revision of the paper. Issue (2) and (3) may be harder to address, but is essential that they are addressed for the line of work pioneered by this paper to have an impact on our understanding of deep learning.------------------------Minor comments: ----------------- In prior work, the authors only refer to fully supervised siamese network approaches. These approaches differ from that taken by the authors, as their approach is unsupervised. It should be noted that the authors are not the first to study unsupervised representation learners parametrized by deep networks: other important examples are deep autoencoders (Hinton & Salakhutdinov, 2006 and work on denoising autoencoders from Bengio's group) and parametric t-SNE (van der Maaten, 2009).--------- What loss do the authors use in their experiments? Using ""the difference between the ground truth distance ... and the distance computed by the network"" seems odd, because it encourages the network to produce infinitely large distances (to get a loss of minus infinity). Is the difference squared?","There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.","The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error"," The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds . The paper focuses on what the authors call ""monotonic chains of linear segments"", which are essentially sets","is vacuous for any practical setting, as the upper bound on the error is exponential in the total curvature (a quantity that will be quite large in most practical settings) The error bound presented in Section 4 appears vacuous",0.27586206896551724,0.052631578947368425,0.17241379310344826,0.8490929007530212,0.8326935768127441,0.8408132791519165
https://openreview.net/forum?id=BJ3filKll,"Summary:--------In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. --------They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.----------------They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. ----------------Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.----------------Comments:----------------The direction of investigation in the paper (looking at what happens to manifolds in a neural network), is very compelling, and I strongly encourage the authors to continue exploring this direction.----------------However, the current version of the paper could use some more work:----------------The experiments are all with a regression loss and a shallow network, and as part of the reason for interest in this question is the very large, high dimensional datasets we use now, which require a deeper network, it seems important to address this case.----------------It also seems important to confirm that embedding works well when *classification* loss is used, instead of regression----------------The theory sections could do with being more clearly written -- I’m not as familiar with the literature in this area, and while the proof method used is relatively elementary, it was difficult to understand what exactly was being proved -- e.g. formally stating what could be expected of an embedding that “accurately and efficiently” preserves a monotonic chain, etc.","There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.","The authors look at the ability of neural networks to represent low dimensional manifolds efficiently. They define a class of manifolds, monotonic chains, and give a construction to embed such a chain with a neural network.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors look at the ability of neural networks to represent low dimensional manifolds efficiently. They define a class of manifolds, monotonic chains, and give a construction to embed such a chain with a neural network.", The authors look at the ability of neural networks to represent low dimensional manifolds efficiently . They give a bound on the number of parameters required to do so . They also examine what happens when the manifold is noisy .,"s, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a bound on number of parameters required to embed the monotonic chain. This experiment supports the",0.16216216216216217,0.01834862385321101,0.12612612612612611,0.8409472703933716,0.8288109302520752,0.8348350524902344
https://openreview.net/forum?id=BJ3filKll,"SUMMARY --------This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. ----------------PROS --------Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. ----------------CONS --------The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). ----------------COMMENTS --------It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. --------Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. ----------------MINOR COMMENTS --------- Figure 1 could be referenced first in the text.  --------- ``Color coded'' where the color codes what? --------- Thank you for thinking about revising the points from my first questions. Note: Isometry on the manifold. --------- On page 5, mention how the orthogonal projection on S_k is realized in the network. --------- On page 6 ``divided into segments'' here `segments' is maybe not the best word. --------- On page 6 ``The mean relative error is 0.98'' what is the baseline here, or what does this number mean? ","There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference.",This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. The paper presents a construction illustrating certain structures that can be captured by a,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. The paper presents a construction illustrating certain structures that can be captured by a, This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers . It would be interesting to study the ramifications of the presented observations for the,-----------This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. ------------PRO,0.22222222222222224,0.06956521739130435,0.11965811965811965,0.8356478214263916,0.8306207656860352,0.8331267237663269
https://openreview.net/forum?id=H1oyRlYgg,"Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well","The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3.  Based on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. ","Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well.", The paper discusses why large batch gradient descent does not work too well .,"Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well.",0.08571428571428572,0.0,0.05714285714285714,0.8577637672424316,0.8113803267478943,0.8339275121688843
https://openreview.net/forum?id=H1oyRlYgg,"The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. ----------------Pros and Cons:--------Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. ----------------Significance:--------I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.----------------Comments:--------Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version.","The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3.  Based on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. ","The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatterminima have better generalization ability. There is little novelty in the paper, but",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatterminima have better generalization ability. There is little novelty in the paper, but"," The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima . The authors had some concern about the correctness of a claim made by the authors, which is resolved now .",": 1. SGD with smaller batch sizes converge to flatter minima, 2. flatter minima have better generalization ability. ---------Pros and Cons:--------Although there is little novelty in the paper",0.14457831325301204,0.0,0.09638554216867469,0.8427271842956543,0.8215398192405701,0.8319985866546631
https://openreview.net/forum?id=H1oyRlYgg,"I think that the paper is quite interesting and useful. --------It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime.","The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3.  Based on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. ","It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime."," It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime .","--------I think that the paper is quite interesting and useful. --------I think that the paper is quite interesting and useful. --------It might benefit from additional investigations, e.g., by adding some ",0.08571428571428572,0.0,0.05714285714285714,0.849078893661499,0.8108685612678528,0.8295339941978455
https://openreview.net/forum?id=ry54RWtxx,"This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.----------------One further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model need to, e.g., statically determine whether an `if` condition can ever evaluate to true in order to solve these tasks? Or is it just as simple as checking whether a variable appears on a LHS before it appears on a RHS later in the textual representation of the program?----------------Strengths:--------- Learning a static analyzer is an interesting concept, and I think there is good potential for this line of work--------- The ability to determine whether variables are defined before they are used is certainly a prerequisite for more complicated static analysis.--------- The experimental setup seems reasonable--------- The differentiable set seems like a useful (albeit simple) modelling tool----------------Weaknesses:--------- The setup is very toy, and it's not clear to me that this makes much progress towards the challenges that would arise if one were trying to learn a static analyzer --------- The models are mostly very simple. The one novelty on the modelling front (the differentiable set) provides a small win on this task, but it's not clear if it is a useful general construct or not.----------------Overall:--------I think it's an interesting start, and I'm eager to see how this line of work progresses. In my opinion, it's a bit too early to accept this work to ICLR, but I'd be excited about seeing what happens as the authors try to push the system to learn to analyze more properties of code, and as they push towards scenarios where the learned static analyzer would be useful, perhaps leveraging strengths of machine learning that are not available to standard programming languages analyses.","There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques).",This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables are defined before they are used.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables are defined before they are used., A paper takes a first step towards learning to learn to statically analyze source code . It develops a simple toy programming language that includes loops and branching . The aim is to determine whether all variables in the program are defined before they are used .,".---------- The experimental setup seems reasonable--------- The differentiable set seems like a useful (albeit simple) modelling tool----------I think it's an interesting start, and I'm eager to",0.15,0.0,0.1,0.847873866558075,0.8303192853927612,0.8390047550201416
https://openreview.net/forum?id=ry54RWtxx,"The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.----------------LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it.","There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques).","The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language. It's certainly possible that LSTMs",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language. It's certainly possible that LSTMs", The authors are trying to understand whether static analysis can be learned . LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems .,"can solve static analysis, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can",0.16867469879518074,0.0,0.14457831325301207,0.8450441956520081,0.8378523588180542,0.8414329290390015
https://openreview.net/forum?id=ry54RWtxx,"The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.----------------While the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these baselines is not ready for ICLR. Moreover, there is a risk that existing models already solve this problem very well, making the contribution unclear.","There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques).",The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem. The toy task presented in this paper is too simple to warrant an ICLR submission.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem. The toy task presented in this paper is too simple to warrant an ICLR submission., The authors explore the idea of deep-learning a static analyzer . They do it with a toy programming language and a very simplified analysis problem . The toy task presented in this paper is too simple to warrant an ICLR submission .,"is a toy algorihtmic task, similar to those solved in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer ",0.24691358024691357,0.05063291139240506,0.14814814814814814,0.8643833994865417,0.8448891639709473,0.854525089263916
https://openreview.net/forum?id=HJeqWztlg,"The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images.----------------The paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems.----------------Summary:--------———--------The paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me.----------------Quality: I think some of the techniques could be described more carefully to better convey the intuition.--------Clarity: Some of the derivations and intuitions could be explained in more detail.--------Originality: The suggested idea is reasonable but limited to binary data at this point in time.--------Significance: Since the experimental setup is somewhat limited according to my opinion, significance is hard to judge.----------------Details:--------———--------1. My main concern is related to the experimental evaluation. While the discussed approach is valuable, its application seems limited to binary images at this point in time. Can the authors comment?----------------2. There are existing techniques to extract representations of images which the authors may want to mention, e.g., work based on grammars.", The reviewer's opinions were clear for this paper. Mainly it seems that the fact that this work focuses on binary image patterns limited the ability of reviewers to assess the significance of this work based on the instantiation of the model explored in this work. It was also noted that the writing could have been clearer when describing the intuitions for the approach and that the derivations could have been explained in more detail.,The paper discusses a method to learn interpretable template representations from given data. The authors illustrate their approach on binary images. Unfortunately the experimental results are on smaller scale data.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper discusses a method to learn interpretable template representations from given data. The authors illustrate their approach on binary images. Unfortunately the experimental results are on smaller scale data., The paper discusses a method to learn interpretable hierarchical template representations from given data . The authors illustrate their approach on binary images . Unfortunately the experimental results are on smaller scale data .,based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate,0.19047619047619044,0.03883495145631068,0.1523809523809524,0.8629453182220459,0.8415144681930542,0.8520951867103577
https://openreview.net/forum?id=HJeqWztlg,"This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. --------In particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. ----------------Learning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. ----------------I have however, some concerns about the paper:----------------1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. --------I would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:----------------- Compositional hierarchies of Sanja Fidler--------- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects--------- AND-OR templates of Song-Chun Zhu's group at UCLA ----------------The claim that this paper is the first to discover such parts should be removed. ----------------2) The experimental evaluation is limited to very toy datasets. The papers I mentioned have been applied to real images (e.g., by using contours to binarize the images). --------I'll also like to see how good/bad the proposed approach is for classification in more well known benchmarks. --------A comparison to other generative models such as VAE, GANS, etc will also be useful.----------------3) I'll also like to see a discussion of the relation/differences/advantages of the proposed approach wrt to sum product networks and grammars.----------------Other comments:----------------- the paper claims that after learning inference is feed-forward, but since message passing is used, it should be a recurrent network. ----------------- the algorithm and tech discussion should be moved from the appendix to the main paper----------------- the introduction claims that compression is a prove for understanding. I disagree with this statement, and should be removed. ----------------- I'll also like to see a discussion relating the proposed approach to the Deep Rendering model. ----------------- It is not obvious how some of the constraints are satisfied during message passing. Also constraints are well known to be difficult to optimize with max product. How do you handle this?----------------- The learning and inference algorithms seems to be very heuristic (e.g., clipping to 1, heuristics on which messages are run). Could you analyze the choices you make?----------------- doing multiple steps of 5) 2) is not a single backward pass ----------------I'll reconsider my score in light of the answers", The reviewer's opinions were clear for this paper. Mainly it seems that the fact that this work focuses on binary image patterns limited the ability of reviewers to assess the significance of this work based on the instantiation of the model explored in this work. It was also noted that the writing could have been clearer when describing the intuitions for the approach and that the derivations could have been explained in more detail.,This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. Learning is performed by using approximated inference with MAX-product BP.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. Learning is performed by using approximated inference with MAX-product BP., This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images . Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary,"to learn interpretable parts. --------------I would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:--------------------------",0.13084112149532712,0.0380952380952381,0.07476635514018691,0.8453583717346191,0.8353590369224548,0.8403289318084717
https://openreview.net/forum?id=HJeqWztlg,"This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance/location variables is defined.----------------Overall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.--------Why would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning.----------------The current instantiation of the model is limited.  It models binary image patterns.  The experiments are done on synthetic data and MNIST digits.  The method recovers the structure and is effective at classification on synthetic data that are directly compositional.  On the MNIST data, the test errors are quite large, and worse than a CNN except when synthetic data corruption is added.  Further work to enhance the ability of the method to handle natural images or naturally occuring data variation would enhance the paper.", The reviewer's opinions were clear for this paper. Mainly it seems that the fact that this work focuses on binary image patterns limited the ability of reviewers to assess the significance of this work based on the instantiation of the model explored in this work. It was also noted that the writing could have been clearer when describing the intuitions for the approach and that the derivations could have been explained in more detail.,This paper presents a generative model for binary images. Images are composed by placing a set of binary features at locations in the image. These features are OR'd together to produce an image.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents a generative model for binary images. Images are composed by placing a set of binary features at locations in the image. These features are OR'd together to produce an image., This paper presents a generative model for binary images . Images are composed by placing a set of binary features at locations in the image . These features are OR'd together to produce an image . The experiments are done on synthetic data and MN,for binary images. Images are composed by placing a set of binary features at locations in the image. These features are OR'd together to produce an image. This paper presents a generative model for binary images. Images are,0.18348623853211007,0.03738317757009346,0.12844036697247707,0.8603605628013611,0.8366857171058655,0.8483579754829407
https://openreview.net/forum?id=S1Jhfftgx,"This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is interesting, and it tries to solve an important problem. However, it seems to me the paper is not ready to publish yet.----------------Comments:----------------- The first section of the paper is clear and well-motivated. ----------------- The authors should report test running time. The proposed approach changes the weight matrix. As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7). This is actually pretty expensive in comparison to running the beam search or other inference methods. Therefore, I'm not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD).   ----------------- If I understand correctly, Eq (7) is a noncontinuous function w.r.t W_\lambda and the simple SGD algorithm will not be able to find its minimum.----------------- For dependency parsing, there are standard splits of PTB. I would suggest the authors follow the same splits of train, dev, and test in order to compare with existing results. ------------------------Minor comments: several sentences are misleading and should be rewritten carefully. ----------------- Beginning of Section 3: ""A major advantage of neural network is that once trained, inference is extremely efficient."" This sentence is not generally right,  and I guess the authors mean if using greedy search as inference method, the inference is efficient. ----------------- The description in the end of section 2 is awkward. To me,  feed-forward and RNN  are general families that cover many specific types of neural networks, and the training procedures are not necessarily to aim to optimize Eq. (2). Therefore, the description here might not be true. In fact, I don't think there is a need to bring up feed-forward networks here; instead, the authors should provide more details the connection between RNN and Eq (2) here.----------------- The second paragraph of section 3 is related to [1], where it shows the search space of the inference can be represented as an imperative program. -------- ------------------------[1] Credit assignment compiler for joint prediction, NIPS 2016","The program committee appreciates the authors' response to the clarifying questions. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work.","This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is"," This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model . The main idea is to modify the weight of the neural network in order to find a feasible solution . Overall, the idea presented in the paper is",": ""A major advantage of neural network is that once trained, inference is extremely efficient"" This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the",0.2972972972972973,0.02777777777777778,0.16216216216216217,0.839870035648346,0.842941164970398,0.8414027690887451
https://openreview.net/forum?id=S1Jhfftgx,"This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems, while keeping inference unconstrained. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured prediction energy networks (SPENs), recently proposed by Belanger et al. ----------------Overall, I didn't find the approach very convincing and the paper has a few problems regarding the empirical evaluation. There's also some imprecisions throughout. The proposed approach (secs 6 and 7) looks more like a ""little hack"" to try to make it vaguely similar to Lagrangian relaxation methods than something that is theoretically well motivated.----------------Before eq. 6: ""an exponential number of dual variables"" -- why exponential? it's not one dual variable per output.----------------From the clarification questions:--------- The accuracy reported in Table 1 needs to be explained. --------- for the parsing experiments it would be good to report the usual F1 metric of parseval, and to compare with state of the art systems.--------- should use the standard training/dev/test splits of the Penn Treebank.--------The reported conversion rate in Table 1 does not tell us how many violations are left by the unconstrained decoder to start with. It would be good to know what happens in highly structured problems where these violations are frequent, since these are the problems where the proposed approach could be more beneficial.------------------------Minor comments/typos:--------- sec.1: ""there are"" -> there is?--------- sec 1: ""We find that out method is able to completely satisfy constraints on 81% of the outputs."" -> at this point, without specifying the problem, the model, and the constraints, this means very little. How many constrains does the unconstrained method satisfies?--------- sec 2 (last paragraph): ""For RNNs, each output depends on hidden states that are functions of previous output values"" -- this is not very accurate, as it doesn't hold for general RNNs, but only for those (e.g. RNN decoders in language modeling) where the outputs are fed back to the input in the next time frame. --------- sec 3: ""A major advantage of neural networks is that once trained, inference is extremely efficient."" -- advantage over what? also, this is not necessarily true, depends on the network and on its size.--------- sec 3: ""our goal is take advantage"" -> to take advantage--------- last paragraph of sec 6: ""the larger model affords us"" -> offers?","The program committee appreciates the authors' response to the clarifying questions. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work.",This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured, Paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems . Idea is to tweak the neural network parameters to make those output constraints hold . The underlying model is that of structured prediction energy,"The proposed approach (secs 6 and 7) looks more like a ""little hack"" to try to make it vaguely similar to Lagrangian relaxation methods.--------- sec 2 (last paragraph): ""Our goal is",0.23999999999999996,0.0,0.13333333333333333,0.8186204433441162,0.8336435556411743,0.8260636925697327
https://openreview.net/forum?id=S1Jhfftgx,"This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems.----------------Many things don't quite make sense to me:-------- 1. Most seq2seq models (such as those used for parsing) have substantially better performance when coupled with beam search than greedy search, and exact search is infeasible. This is because these models are trained to condition on discrete values of past outputs in each timestamp, and hence the problem of finding the highest-scoring total sequence of outputs is not solvable efficiently. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-of-the-art, specially when constraint-aware beam search is used. This comparison is specially interesting because both constrained beam search and this dual-decomposition-like approach require multiple computations of the model's score.-------- 2. It's unclear (to me at least) how to differentiate the constraint term g() in the objective function in the general case (though the particular example used here is understandable)-------- 3. The paper claims that ""Lagrangian relaxation methods for NLP have multipliers for each output variable that can be combined with linear models [...] . Since our non-linear functions and global constraints do not afford us the same ability"" but it is possible to add linear terms to the outputs of neural networks, possibly avoiding rerunning all the expensive inference terms.----------------Moreover, the justification for the particular method is hand-wavy at best, with inconvenient terms from equations ignored or changed at will. At this point it might be better to omit the attempted theoretical explanation and just present this method as a heuristic which is likely to achieve the desired result.----------------This, plus the concerns around lack of clear comparisons with baselines on benchmark problems lead me to recommend rejection. Further explanation of how this compares with beam search, how this relates to the state-of-the-art, and a better explanation for how to come up with differentiable constraint sets, are probably required for acceptance.","The program committee appreciates the authors' response to the clarifying questions. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work.",This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems. Many things don't quite make sense to me. It's unclear what kind of model this paper is using which allows for greedy decoding,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems. Many things don't quite make sense to me. It's unclear what kind of model this paper is using which allows for greedy decoding, This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems . Many things don't quite make sense to me: It's unclear what kind of model this paper is using which allows for greedy decoding,for enforcing constraints in neural network prediction systems.---------------------------------------------------------------------------------------------------------,0.13513513513513514,0.0,0.1081081081081081,0.8338356614112854,0.8453283905982971,0.8395427465438843
https://openreview.net/forum?id=rJRhzzKxl,"This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset.----------------The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.----------------I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).----------------I think this paper would make an interesting ICLR paper.","The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed",This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset., This paper studies the problem of transfer learning in the context of domain adaptation . The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning .,.----------------I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).-------------I think this,0.1935483870967742,0.04395604395604395,0.12903225806451613,0.8882436752319336,0.8092457056045532,0.8469064831733704
https://openreview.net/forum?id=rJRhzzKxl,"The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements. To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model. ----------------The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source  models, and performs comparable to the proposed method. --------Questions: --------1. Why did you choose to use different combination schemes in equation (3) and (5)? For example, in equation (5), what if minimizing H( (1-\lambda) y_teacher + \lambda P_t, P_s) instead? --------2. how will you extend the MCD definition to multi-class settings?  ","The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed","The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation."," The paper is well written and easy to follow . The idea is straight-forward, albeit fairly heuristic in several cases . It is not clear what is the advantage of the proposed method vs existing feature learning techniques .","to train the student model, learned on the source domain. --------------The paper is well written and easy to follow. The idea is straight-forward, although fairly heuristic in several cases. -----------",0.31775700934579443,0.1142857142857143,0.18691588785046728,0.8944846987724304,0.8400931358337402,0.8664361238479614
https://openreview.net/forum?id=rJRhzzKxl,"Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation. The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.----------------What exactly constitutes the learned representation h used in MCD measure? I assume those are the top level pre-softmax activations - is this the case? Those tend to be typically more task related, would not the intermediate ones work better?----------------One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework. As such, approach is rater constrained in scope.-------- --------Authors shall refer in the related-work to similar ideas proposed in the related field of acoustic modelling (and adaptation of acoustic models), in particular, works of Yu et al. [1] and the follow up work of Li et al. [2] which to an extent are a prior the work on knowledge distillation. Reasonably related is also work on deep relationship networks [3], where MT generative approach is proposed to avoid negative transfers, something of central role in this paper.----------------Minors: --------The student S similarly has an output probability -> models an output probability----------------[1] KL-Divergence Regularized Deep Neural Network Adaptation For Improved Large Vocabulary Speech Recognition, Dong Yu, Kaisheng Yao, Hang Su, Gang Li, Frank Seide--------[2] Learning Small-Size DNN with Output-Distribution-Based Criteria, Jinyu Li, Rui Zhao, Jui-Ting Huang and Yifan Gong--------[3] Learning Multiple Tasks with Deep Relationship Networks, Mingsheng Long, Jianmin Wang","The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed",Paper describes technique for leveraging multiple teachers in teacher-student framework. Central idea relies on using similarity measure between source and target domains. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Paper describes technique for leveraging multiple teachers in teacher-student framework. Central idea relies on using similarity measure between source and target domains. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis., Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation . Central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers .,: --------The student S similarly has an output probability -> models an output probability----------------[1] KL-Divergence Regularized Deep Neural Network Adaptation For Improved Large Vo,0.17475728155339806,0.05940594059405941,0.1553398058252427,0.8632329702377319,0.8104239106178284,0.8359953165054321
https://openreview.net/forum?id=BkVsEMYel,"This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)----------------In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. ----------------Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. ----------------This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. ----------------This interesting contribution gives an intuition about how deep network can capture the correlation between the input variables when its size is polynomial but and correlation is exponential.----------------It worth to note that although the authors tried to express their notation and definitions carefully where they were very successful, it would be helpful if they elaborate a bit more on their definitions, expressions, and conclusions in the sense to make them more accessible. ","The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.    The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.    The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?  https://arxiv.org/abs/1606.05340",This paper investigates the fact why deep networks perform well in practice. It shows how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper investigates the fact why deep networks perform well in practice. It shows how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank., This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially-sized deep network to provide a function with exponentially high separation rank . The general reasoning for this problem,"is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. ---------------This interesting contribution gives an intuition about how deep networks can capture the correlation between",0.25396825396825395,0.042780748663101595,0.12698412698412698,0.8821098208427429,0.8237435817718506,0.851928174495697
https://openreview.net/forum?id=BkVsEMYel,"The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).----------------While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.--------My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section 5.3), than the general deep case.----------------To summarize my understanding of the key theorem 1 result:--------- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer). So exponential network sizes are caused by this rank needing to grow exponentially, as required by the partition.--------- In the deep case, one also uses the case that the upper bound is linear in the size of the network (as measured by the last hidden layer), however, this situation is caused by the selection of a partition (I^low, J^high), and the maximal rank induced by this partition is only linear anyway, hence the network size can remain linear.----------------If tried my best to summarize the key point of this paper and still probably failed at it, which shows how complex is this notion of partition rank, and that its linear growth with network size can either be a good or bad thing depending on the setting. Hopefully, someone will come one day with an explanation that holds in a single slide.--------------------------------While this is worth publishing as conference paper in its present form, I have two suggestions that, IMHO, would make this work more significant:----------------On the theory side, we are still very far from the completeness of the PAC bound papers of the ""shallow era"". In particular, the non-probabilistic lower and upper bound in theorem 1 are probably loose, and there is no PAC-like theory to tell us which one to use and what is the predicted impact on performance (not just the intuition). Also, in the prediction of the inductive bias, the other half is missing. This paper attempts to predict the maximal representation capacity of a DNN under bounded network size constraints, but one of the reason why this size has to be bounded is overfitting (justified by PAC or VC-dim like bounds). If we consider the expected risk as  the sum of the empirical risk and the structural risk, this paper only seems to address fully the empirical risk minimization part, freezing the structural risk. ----------------On the practice side, an issue is that experiments in this paper mostly confirm what is obvious through intuition, or some simpler form of reasonings. For instance to use convolutions that join pixels which are symmetrical in images to detect symmetry. Basic hand-crafted pattern detectors, as they have been used in computer vision for decades, would just do the job. What would be a great motivation for using this framework is if it answered questions that simple human intuition cannot, and for which we are still in the dark: one example I could think of in the recent use of gated convolutions 'a trous' for 1D speech signal, popularized in Google WaveNet (https://deepmind.com/blog/wavenet-generative-model-raw-audio/). Note that 1D inputs would also simplify the notation!","The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.    The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.    The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?  https://arxiv.org/abs/1606.05340","The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. The ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. The ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match"," The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network . The ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match","theorem 1 result:--------- The upper bound of the separation rank is used to show that in the shallow case, this rank grows AT MOST linearly with the network size (as measured by the only hidden layer",0.20105820105820107,0.042780748663101595,0.1375661375661376,0.8652365207672119,0.8171902298927307,0.840527355670929
https://openreview.net/forum?id=BkVsEMYel,"This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.----------------The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is pretty terse and not easy to follow without knowledge of the authors’ prior work.----------------In some sense the comparison between deep and shallow networks is somewhat misleading, since the shallow networks lack a hierarchical pooling structure.  For example, a shallow convolutional network with RELU and max pooling does not really make sense, since the max occurs over the whole image.  So it seems that the paper is really more of an analysis of the effect of pooling vs. not having pooling.  For example, it is not clear that a deep CNN without pooling would be any more efficient than a shallow network, from this work.----------------It is not clear how much the theoretical results depend on the use of a model with product pooling, and how they might be extended to the more common max pooling.  Even if theoretical results are difficult to derive in this case, simple illustrative examples might be helpful.  In fact, if the authors prepare a longer version of the paper for a journal I think the results could be made more intuitive if they could add a simple toy example of a function that can be efficiently represented with a convolutional arithmetic circuit when the pooling structure fits the correlations, and perhaps showing also how this could be represented with a convolutional network with RELU and max pooling.----------------I would also appreciate a more explicit discussion of how the depth of a deep network affects the separability of functions that can be represented.  A shallow network doesn’t have local pooling, so the difference between deep and shallow if perhaps mostly one of pooling vs. not pooling.  However, practitioners find that very deep networks seem to be more effective than “deep” networks with only a few convolutional layers and pooling.  The paper does not explicitly discuss whether their results provide insight into this behavior.----------------Overall, I think that the paper attacks an important problem in an interesting way.  It is not so convincing that this really gets to the heart of why depth is so important, because of the theoretical limitation to arithmetic circuits, and because the comparison is to shallow networks that are without localized pooling. ","The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.    The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.    The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?  https://arxiv.org/abs/1606.05340","This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks. The basic intuition is convincing and fairly straightforward. Pooling operations bring together information.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks. The basic intuition is convincing and fairly straightforward. Pooling operations bring together information."," This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks . The basic intuition is convincing and fairly straightforward . Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently",".-------------The paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks. The basic intuition is convincing and fairly straightforward. Pooling operations bring together information. When information is ",0.17297297297297298,0.04371584699453551,0.12972972972972974,0.8842498064041138,0.8229893445968628,0.8525205254554749
https://openreview.net/forum?id=S1AG8zYeg,"This paper extends the ""order matters"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.----------------I think the problem is interesting and the results are promising. However, there are some problems about technical details:----------------- Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)?--------- Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function?--------- What is the connection between the encoder and decoder in the proposed model? How to combine them together? I read something relevant from the caption of Figure 1, but it is still not clear to me.","Sentence ordering is central to a number of NLP tasks (e.g., summarization), and this paper introduces a very sensible application of existing techniques, experimentations is also very solid. However, the technical contributions seems quite limited.    Positive:    -- An important NLP problem (I would disagree here with Reviewer1), a good match between the problem and the methods  -- Solid experiments   -- A well-written paper    Negative:    -- A straightforward application of existing methods    Unfortunately, the latter, I believe, is a problem. Though application papers are welcome, I believe they should go beyond direct applications and provide extra insights to the representation learning community.","This paper extends the ""order matters"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper extends the ""order matters"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination"," This paper extends the ""order matters"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level . Experiments in this paper show the capacity of the proposed model on both order discrimination","(Vinyals et al., 2015)?---------------I think the problem is interesting and the results are promising. But some notations in this paper are confusing. For example, what is the form",0.20437956204379562,0.014814814814814814,0.11678832116788322,0.8319875597953796,0.8011298179626465,0.8162670731544495
https://openreview.net/forum?id=S1AG8zYeg,"This paper presents an empirical study on sentence ordering using RNN variants.----------------I’m not sure how strong novelty this paper brings in terms of technical contributions. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks. Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence. Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.----------------That said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs.","Sentence ordering is central to a number of NLP tasks (e.g., summarization), and this paper introduces a very sensible application of existing techniques, experimentations is also very solid. However, the technical contributions seems quite limited.    Positive:    -- An important NLP problem (I would disagree here with Reviewer1), a good match between the problem and the methods  -- Solid experiments   -- A well-written paper    Negative:    -- A straightforward application of existing methods    Unfortunately, the latter, I believe, is a problem. Though application papers are welcome, I believe they should go beyond direct applications and provide extra insights to the representation learning community.","This paper presents an empirical study on sentence ordering using RNN variants. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015)",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents an empirical study on sentence ordering using RNN variants. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015)"," This paper presents an empirical study on sentence ordering using RNN variants . The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence","is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence.------------I’m not sure how strong novelty this paper brings in terms of",0.15748031496062992,0.032,0.11023622047244094,0.836660623550415,0.7989416122436523,0.8173661828041077
https://openreview.net/forum?id=S1AG8zYeg,"The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).----------------The first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a ""bag-of-sentences"", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.----------------The model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-decoder approach outperforms the other methods on the task of ordering a given set of sentences. The authors also show that the model learns relatively good sentence representations (e.g. compared to Skip-thought vectors).----------------This paper is relatively well written, and easy to follow. The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques to solve the task of sentence ordering. In particular, it uses an order-invariant encoder to get a representation of the set of sentences, and a pointer network to predict the ordering. While I am not entirely convinced by the task of sentence ordering, this approach seems promising to learn sentence representations.----------------Overall, this is a pretty solid and well executed paper.----------------pros:-------- - sound model for sentence ordering-------- - strong experimental results--------cons:-------- - might be a bit incremental-------- - usefulness of sentence ordering","Sentence ordering is central to a number of NLP tasks (e.g., summarization), and this paper introduces a very sensible application of existing techniques, experimentations is also very solid. However, the technical contributions seems quite limited.    Positive:    -- An important NLP problem (I would disagree here with Reviewer1), a good match between the problem and the methods  -- Solid experiments   -- A well-written paper    Negative:    -- A straightforward application of existing methods    Unfortunately, the latter, I believe, is a problem. Though application papers are welcome, I believe they should go beyond direct applications and provide extra insights to the representation learning community.","The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs"," The main contribution of this paper is to introduce a new neural network model for sentence ordering . This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs","to get a representation of the set of sentences, and a pointer network to predict the ordering.-------------This paper is relatively well written, and easy to follow. The model introduced in this paper is then ",0.21897810218978103,0.04444444444444444,0.1313868613138686,0.8312351703643799,0.79780113697052,0.8141750693321228
https://openreview.net/forum?id=SJRpRfKxx,"The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.----------------quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?----------------You claim that your ""results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported in Table 1. Apart from this, it is dangerous to compare AUC performance differences due to e.g. saturation issues.----------------clarity: the explanation for Table 3 is a bit confusing, also it is not clear why the CONV5 and the FC6 models differ in how the saliency map is used. At least one should also evaluate the CONV5 model when multiplying the input with the saliency map to see how much of the difference comes from the different ways to use the saliency map and how much from the different features.----------------Other issues:----------------You cite Kümmerer et. al 2015 as a model which ""learns ... indirectly rather than from explicit information of where humans look"", however the their model has been trained on fixation data using maximum-likelihood.----------------Apart from these issues, I think the paper make a very interesting contribution to spatio-temporal fixation prediction. If the evaluation issues given above are sorted out, I will happily improve my rating.",The paper describes a model for video saliency prediction using a combination of spatio-temporal ConvNet features and LSTM. The proposed method outperforms the state of the art on the saliency prediction task and is shown to improve the performance of a baseline action classification model.,The authors formulate a recurrent deep neural network to predict human fixation locations in videos. They train the model using maximum likelihood with actual fixation data. They combine the saliency predictions with the C3D features for action recognition.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors formulate a recurrent deep neural network to predict human fixation locations in videos. They train the model using maximum likelihood with actual fixation data. They combine the saliency predictions with the C3D features for action recognition., The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians . They train the model using maximum likelihood with actual fixation data .,in Table 1 have a performance worse than the center bias performance reported in Table 1. I am missing details on how central bias and human performance are modelled. All the state-of-the-art models reported in Table 2 have,0.380952380952381,0.04878048780487805,0.23809523809523808,0.8843476176261902,0.8743231296539307,0.8793067932128906
https://openreview.net/forum?id=SJRpRfKxx,"This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,--------thus helping to prune irrelevant information from the video and improve action recognition.----------------The work is interesting and has shown state-of-the-art results on predicting human attention on action videos.--------It has also shown promise for helping action clip classification.----------------The paper would benefit from a discussion on the role of context in attention.--------For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?----------------One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.--------The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.----------------Is there a way to combine the context and attention without concatenation?--------The rational for concatenating the features extracted from the original clip,--------and the features extracted from the saliency weighted clip seems to contradict the initial hypothesis that `eliminating or down-weighting pixels that are not important' will improve performance.----------------The authors should also mention the current state-of-the-art results in Table 4, for comparison.----------------# Other comments:--------# Abstract--------- Typo: `mixed with irrelevant ...'--------``Time consistency in videos ... expands the temporal domain from few frames to seconds'' - These two points are not clear, probably need a re-write.----------------# Contributions--------- 1) `The model can be trained without having to engineer spatiotemporal features' - you would need to collect training data from humans though.. ----------------# Section 3.1--------The number of fixation points is controlled to be fixed for each frame - how is this done?----------------In practice we freeze the layers of the C3D network to values pretrained by Tran etal.--------What happens when you allow gradients to flow back to the C3D layers?--------Is it not better to allow the features to be best tuned for the final task?----------------The precise way in which the features are concatenated needs to be clarified in section 3.4.----------------Minor typo:--------`we added them trained central bias'",The paper describes a model for video saliency prediction using a combination of spatio-temporal ConvNet features and LSTM. The proposed method outperforms the state of the art on the saliency prediction task and is shown to improve the performance of a baseline action classification model.,The work is interesting and has shown state-of-the-art results on predicting human attention on action videos. The paper would benefit from a discussion on the role of context in attention.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The work is interesting and has shown state-of-the-art results on predicting human attention on action videos. The paper would benefit from a discussion on the role of context in attention., This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns . It helps to prune irrelevant information from the video and improve action recognition .,"...'-----------Time consistency in videos... expands temporal domain from few frames to seconds'' - These two points are not clear, probably need a re-write.-------",0.43037974683544306,0.12987012987012989,0.25316455696202533,0.8925150036811829,0.8605814576148987,0.8762573599815369
https://openreview.net/forum?id=SJRpRfKxx,"This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.----------------Overall, the idea in this paper is reasonable and the paper is well written. RNN/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN/LSTM for problems where the output is continuous like in this paper.----------------The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e.g. C3D + SVM) on the action recognition task.----------------My only ""gripe"" of this paper is that this paper is missing some important baseline comparisons. In particular, it does not seem to show how the ""recurrent"" part help the overall performance. Although Table 2 shows RMDN outperforms other state-of-the-art, it might be due to the fact that it uses strong C3D features (while other methods in Table 2 use traditional handcrafted features). Since saliency prediction is essentially a dense image labeling problem (similar to semantic segmentation). For dense image labeling, there has been lots of methods proposed in the past two years, e.g. fully convolution neural network (FCN) or deconvnet. A straightforward baseline is to simply take FCN and apply it on each frame. If the proposed method still outperforms this baseline, we can know that the ""recurrent"" part really helps.",The paper describes a model for video saliency prediction using a combination of spatio-temporal ConvNet features and LSTM. The proposed method outperforms the state of the art on the saliency prediction task and is shown to improve the performance of a baseline action classification model.,"This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) The visual features are then passed to LSTM. The hidden state at each",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) The visual features are then passed to LSTM. The hidden state at each"," This paper proposes a new method for estimating visual attention in videos . The input clip is first processed by a convnet (in particular, C3D) to extract visual features . The visual features are then passed to LSTM .",". This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed",0.38554216867469876,0.024691358024691357,0.24096385542168675,0.8771817088127136,0.8748606443405151,0.8760196566581726
https://openreview.net/forum?id=rJEgeXFex,"This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:-----------------The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that leaves Table 1.  I recognize there is only one dataset, but this does not offer a vast amount of empirical evidence and analysis that one might expect out of a paper with no major algorithmic/theoretical advances.  To be clear I don't think this is disqualifying or deeply concerning; I simply found it a bit underwhelming.----------------- To be constructive, re the results I would recommend removing Fig 5 and replacing that with some more meaningful analysis of performance.  I found Fig 5 to be mostly uninformative, other than as a negative result, which I think can be stated in a sentence rather than in a large figure.----------------- There is a bit of jargon used and expertise required that may not be familiar to the typical ICLR reader.  I saw that another reviewer suggested perhaps ICLR is not the right venue for this work.  While I certainly see the reviewer's point that a medical or healthcare venue may be more suitable, I do want to cast my vote of keeping this paper here... our community benefits from more thoughtful and in depth applications. Instead I think this can be addressed by tightening up those points of jargon and making the results more easy to evaluate by an ICLR reader (that is, as it stands now researchers without medical experience have to take your results after Table 1 on faith, rather than getting to apply their well-trained quantitative eye). ----------------Overall, a nice paper.","This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.","This is a well written, organized, and presented paper that I enjoyed reading. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This is a well written, organized, and presented paper that I enjoyed reading. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing"," This is a well written, organized, and presented paper that I enjoyed reading . It did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes",". I found Fig 5 to be mostly uninformative, other than as a negative result (tSNE is only in some places interpretable), so that leaves Table 1. This is a well written, organized, and",0.3076923076923077,0.012987012987012986,0.15384615384615385,0.8747740983963013,0.843538224697113,0.8588722944259644
https://openreview.net/forum?id=rJEgeXFex,"In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.-------------------------------------This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and a random forest, as well as a common sense baseline. The differences in performance between the recurrent models and the MLP appear to be large enough to be significant, given the size of the test set.----------------Strengths:--------- Very important problem. As the authors point out, two the value propositions of EHRs -- which have been widely adopted throughout the US due to a combination of legislation and billions of dollars in incentives from the federal government -- included more accurate records and fewer medication mistakes. These two benefits have largely failed to materialize. This seems like a major opportunity for data mining and machine learning.--------- Paper is well-written with lucid introduction and motivation, thorough discussion of related work, clear description of experiments and metrics, and interesting qualitative analysis of results.--------- Empirical results are solid with a strong win for RNNs over convincing baselines. This is in contrast to some recent related papers, including Lipton & Kale et al, ICLR 2016, where the gap between the RNN and MLP was relatively small, and Choi et al, MLHC 2016, which omitted many obvious baselines.--------- Discussion is thorough and thoughtful. The authors are right about the kidney code embedding results: this is a very promising result.----------------Weaknesses:--------- The authors make several unintuitive decisions related to data preprocessing and experimental design, foremost among them the choice NOT to use full patient sequences but instead only truncated patient sequences that each ends at randomly chosen time point. This does not necessarily invalidate their results, but it is somewhat unnatural and the explanation is difficult to follow, reducing the paper's potential impact. It is also reduces the RNN's potential advantage.--------- The chosen metrics seem appropriate, but non-experts may have trouble interpreting the absolute and relative performances (beyond the superficial, e.g., RNN score 0.01 more than NN!). The authors should invest some space in explaining (1) what level of performance -- for each metric -- would be necessary for the model to be useful in a real clinical setting and (2) whether the gaps between the various models are ""significant"" (even in an informal sense).--------- The paper proposes nothing novel in terms of methods, which is a serious weakness for a methods conference like ICLR. I think it is strong enough empirically (and sufficiently interesting in application) to warrant acceptance regardless, but there may be things the authors can do to make it more competitive. For example, one potential hypothesis is that higher capacity models are more prone to overfitting noisy targets. Is there some way to investigate this, perhaps by looking at the kinds of errors each model makes?----------------I have a final comment: as a piece of clinical work, the paper has a huge weakness: the lack of ground truth labels for missing medications. Models are both trained and tested on data with noisy labels. For training, the authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional isn't too big of a problem). For testing, though, this seems like it could skew metrics. Further, the assumption that the label noise is not systemic seems very unlikely given that these data are recorded by human clinicians. The cases shown in Appendix C lend some credence to this assertion: for Case 1, 7/26 actual medications received probabilities < 0.5. My hunch is that clinical reviewers would view the paper with great skepticism. The authors will need to get creative about evaluation -- or invest a lot of time/money in labeling data -- to really prove that this works.----------------For what it is worth, I hope that this paper is accepted as I think it will be of great interest to the ICLR community. However, I am borderline about whether I'd be willing to fight for its acceptance. If the authors can address the reviewers' critiques -- and in particular, dive into the question of overfitting the imperfect labels and provide some insights -- I might be willing to raise my score and lobby for acceptance.","This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.",This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient based solely upon the sequence of ICD-9 codes assigned to the patient. The paper proposes to use standard LSTM and GRU architectures,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient based solely upon the sequence of ICD-9 codes assigned to the patient. The paper proposes to use standard LSTM and GRU architectures, This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period . In experiments using,".---------The paper has a huge weakness: the lack of ground truth labels for missing medications. The authors are right that this shouldn't be a huge problem, provided the label noise is random (even class conditional",0.1935483870967742,0.0130718954248366,0.12903225806451613,0.8503892421722412,0.8233091831207275,0.83663010597229
https://openreview.net/forum?id=rJEgeXFex,"This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.----------------The authors also did address the questions of the reviewers.----------------My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR.","This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound.","This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests.", This is a well-conducted and well-written study on the prediction of medication from diagnostic codes . My only negative point is that this work might be more relevant for data science or medical venue rather than at ICLR .,"on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs)",0.15492957746478872,0.028571428571428574,0.09859154929577466,0.8564306497573853,0.8259822726249695,0.8409309387207031
https://openreview.net/forum?id=SkBsEQYll,"This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.----------------Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in ""A Simple Word Embedding Model for Lexical Substitution"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.----------------In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: --------* http://veceval.com/ (Nayak et al., RepEval 2016).--------* Lexical Substitution in Context--------And many higher-level tasks where word similarity in context could be a game-changer:--------* Semantic Text Similarity--------* Recognizing Textual Entailment / Natural Language Inference--------I was disappointed that none of these were even brought up.","There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved.",This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. The novelty claim must be more accurate and position itself with respect to existing work.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. The novelty claim must be more accurate and position itself with respect to existing work., This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity . The novelty claim must be more accurate and position itself with respect to existing work .,"in context.----------------Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the",0.2903225806451613,0.03333333333333333,0.16129032258064516,0.8498133420944214,0.8755499720573425,0.8624897003173828
https://openreview.net/forum?id=SkBsEQYll,"this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. ----------------First, considering the related work [1,2] the proposed approach brings marginal novelty. Especially--------Context Encoders is just a small improvement over word2vec. ----------------Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].----------------[1] http://papers.nips.cc/paper/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions.pdf--------[2] http://deeplearning.cs.cmu.edu/pdfs/OJA.pca.pdf--------[3] http://www.anthology.aclweb.org/P/P10/P10-1040.pdf","There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved.",This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context., This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings . They also use the proposed idea to represent out-of-vocabulary words using the words in given context .,. proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. ---,0.20338983050847456,0.03508771929824561,0.13559322033898305,0.84029221534729,0.8697408437728882,0.8547629714012146
https://openreview.net/forum?id=SkBsEQYll,"This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.----------------While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.----------------Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.----------------The evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect ""the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.""----------------The argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.----------------Overall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance.","There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved.",This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The model is nothing more than the most vanilla neural network trained with SGD on similarity signals,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The model is nothing more than the most vanilla neural network trained with SGD on similarity signals," This paper introduces a similarity encoder based on a feed-forward neural network with the aim of generating similarity-preserving embeddings . As far as I can tell, the model is nothing more than the most vanilla neural network trained",based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. This paper introduces a similarity encoder based on a standard feed-forward neural network,0.18181818181818185,0.0,0.15151515151515152,0.8303361535072327,0.8656318187713623,0.8476167321205139
https://openreview.net/forum?id=SJAr0QFxe,I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.,"This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance.",I think the write-up can be improved. The results of the paper also might be somewhat misleading. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",I think the write-up can be improved. The results of the paper also might be somewhat misleading. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model., The behavior for when weights are 0 is not revealing of how the model works . I think the work also underestimates the effect of the nonlinearities on the learning dynamics .,is not revealing how the model works general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model.,0.1851851851851852,0.018867924528301886,0.12962962962962962,0.8732442855834961,0.8476001024246216,0.8602311015129089
https://openreview.net/forum?id=SJAr0QFxe,"This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. ----------------This result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:----------------1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?----------------2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \sigma^\prime at 0. Therefore, if we use tanh which has derivative 1 at zero, the condition number is the same for linear and tanh activations. But this probably is not enough to explain the bit difference in performance or optimization for linear and nonlinear networks, or how the situations evolve after learning the 0 point.----------------3. As for the success of ResNet (or convnets in general) in computer vision, I believe there are more types of nonlinearity such as pooling? Can the result here generalizes to pooling as well?----------------Minor: --------- sec 1 last paragraph, low approximation error typically means more powerful model class and better training error, but not necessarily better test error--------- sec 4.1 what do you mean by ""zero initialization with small random perturbations""? why not exactly zero initialization, how large is the random perturbation?","This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance.","This paper studies the optimization issue of linear ResNet. It shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. But I do not think the paper has fully resolved the linear",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper studies the optimization issue of linear ResNet. It shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. But I do not think the paper has fully resolved the linear"," This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth .","-------------1. This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth.---------",0.2882882882882883,0.03669724770642201,0.1981981981981982,0.8615903854370117,0.857149064540863,0.8593639731407166
https://openreview.net/forum?id=SJAr0QFxe,"ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:----------------1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission ""Singularity of Hessian in Deep Learning"") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.----------------2- Hessian at zero initial point: The explanation of why we should be interested in Hessain at zero initial point is not acceptable. The zero initial point is not interesting because it is a very particular point that cannot tell us about the Hessian during optimization. ","This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance.","ResNet and other architectures that use shortcuts have shown empirical success in several domains. studying the optimization for such architectures is very valuable. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","ResNet and other architectures that use shortcuts have shown empirical success in several domains. studying the optimization for such architectures is very valuable. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper", ResNet and other architectures that use shortcuts have shown empirical success in several domains . Some of the experiments in the paper are interesting but there are two main issues with the current paper .,s and other architectures that use shortcuts have shown empirical success in several domains. I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence,0.2807017543859649,0.07142857142857142,0.15789473684210525,0.8803614377975464,0.8512685298919678,0.865570604801178
https://openreview.net/forum?id=r1kQkVFgl,"This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by:----------------1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. --------2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.----------------Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.----------------While the paper still misses some critical information that I would like to see, including how the sparse pointer network performance chances with different size of K, and how computationally efficient it is for both training and inference time compared to LSTM w/ attention of various window size, and ablation experiments about how much (1) and (2) contribute respectively, it might be of interest to the ICLR community to see it accepted.","This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.  The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).",This paper presents an improved neural language models designed for selected long-term dependency. The improvements are obtained by replacing the fixed-widow attention with a pointer network. Such a model avoids the the need of large window size of the attention,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents an improved neural language models designed for selected long-term dependency. The improvements are obtained by replacing the fixed-widow attention with a pointer network. Such a model avoids the the need of large window size of the attention," This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python . The improvements are obtained by replacing the fixed-widow",":----------------1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history.-------",0.30000000000000004,0.05617977528089887,0.17777777777777778,0.8697507381439209,0.8204307556152344,0.8443711400032043
https://openreview.net/forum?id=r1kQkVFgl,"This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and/or pointers truly show an advantage in capturing long term dependencies.----------------The sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.----------------The construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future long term dependency work, it may be promising as an avenue for future task exploration.----------------Overall this paper and the dataset are likely an interesting contribution even though there are a few potential issues.","This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.  The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).",This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. The sparse pointer method does seem to provide better results than attention for similar window sizes.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. The sparse pointer method does seem to provide better results than attention for similar window sizes., This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages . The sparse pointer method does seem to provide better results than attention for similar window sizes .,compared a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board.----------------The sparse pointer method does seem,0.27586206896551724,0.09302325581395349,0.1724137931034483,0.878491222858429,0.825295090675354,0.8510627150535583
https://openreview.net/forum?id=r1kQkVFgl,"This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. ","This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.  The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc).","This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty."," A paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers . The idea of an attention policy takes advantage of expert knowledge is a","of an attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the",0.24719101123595502,0.022727272727272728,0.1348314606741573,0.8703936338424683,0.8198727369308472,0.8443781733512878
https://openreview.net/forum?id=Bk67W4Yxl,"The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. ----------------The paper’s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it’s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it’s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would’ve been more convincing to train both AlphaGo and this paper's architectures on an equal number of RL self-play iterations, then have them play each other. (Both would be pre-trained using supervised training, as per the AlphaGo paper).----------------It is not until section 3.3 that it is clearly stated this is strictly a supervised-learning paper. This should have been put front and center in the abstract and introduction.----------------Fully 3 pages are spent on giant but simple architecture diagrams. This is both extravagant and muddles the exposition. It seems better to show just the architectures used in the experiments, and spend at most half a page doing so, so that they may be seen alongside one another.----------------The results (Table 1, Figures 7 and 8) are hard to skim, as there is little information in the captions, and the graph axes are poorly labeled. For example, I assume “Examples” in figures 7 and 8 should be “Training examples”, and the number of training examples isn’t 0-50, but some large multiple thereof.----------------Results: ----------------The take-home seems to be that that deeper networks do better, and residual architectures and spatial batch normalization each improve the results in this domain, as they are known to do in others. Furthermore, we are asked to assume that the improvements in an RL setting will be similar to the improvements in SL shown here. These results seem too incremental to justify an ICLR publication. ","The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.","The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rot",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rot"," The paper tests various feedforward network architectures for supervised training to predict a human’s next move, given a board position . It trains on human play data taken from KGX, augmenting the data by considering all 8 rot",".----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training",0.19354838709677416,0.021978021978021976,0.15053763440860213,0.8609541654586792,0.8531273007392883,0.8570228219032288
https://openreview.net/forum?id=Bk67W4Yxl,"This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance/flop or performance/flop/energy.--------Overall a good paper.","The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.","This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency."," This paper reports new CNN architectures for playing Go . The results are better than previously reported, but there is no mention of computational time and efficiency .","This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance/flop or performance/flop/energy.--------",0.345679012345679,0.0,0.19753086419753085,0.899854302406311,0.8705011010169983,0.8849343657493591
https://openreview.net/forum?id=Bk67W4Yxl,"The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.----------------The paper looks like a follow up paper of the author’s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. ----------------The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?","The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.",The paper trains slightly different network architectures on Computer Go. It provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper trains slightly different network architectures on Computer Go. It provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.," The paper trains slightly different network architectures on Computer Go . It looks like a follow up paper of the author’s previous paper Cazenave (2016a) However, the contribution over the previous paper is not clear .",". ------------------The paper trains slightly different network architectures on Computer Go, and provides analysis of the accuracy as the number of training samples increases and the network architecture differs. It is not clear what are the improvements",0.23809523809523808,0.07317073170731708,0.19047619047619047,0.8720008730888367,0.8467032313346863,0.8591658473014832
https://openreview.net/forum?id=Bk67W4Yxl,"The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.----------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?--------2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?--------3) It is not at all surprising that ResNets do slightly better than vanilla CNNs on move prediction. I don't think this alone is enough for an ICLR paper. It would be good to see at least a comparison of several different variants of the network evaluated at actually playing Go, even if it's against other bots like GnuGo, Pachi, and Fuego.--------4) Are the differences between net_dark and your proposed networks after 20 iterations (Table 1) significant? ----------------Please also see my original questions.","The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled.",The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan, The paper investigates several different architectures for move prediction in computer Go . The main innovation seems to be the use of residual networks . The best proposed architecture outperforms previous results on KGS move prediction dataset .,with batch norm before ReLU?-------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(,0.3092783505154639,0.021052631578947368,0.22680412371134023,0.8681135177612305,0.8592172861099243,0.8636424541473389
https://openreview.net/forum?id=SyEiHNKxx,"I would definitely love to have this and use it for my research. A great tool.----------------However, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack of detail. However, replicability is a crucial part of science. Other publications proposing software (e.g. the tensorflow, theano and edward papers) come along with open source code. This is not the case here and therefore the picture is quite incomplete.----------------I am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more. Nevertheless, this is just an encouragement to the authors to interact with those communities.","Originality, significance:   The paper implements a physics-based simulator directly using Theano. This avoids the type of finite differentiation that physics engines such as MuJoCo use to compute derivatives. It is quite an interesting idea, and is demonstrated using learned control for several models.     Quality, clarity:   The original version was somewhat loosely written; the current version is improved.    Pros:  - The nice idea of implementing a physics engine in a language such as Theano, and showing that this is quite feasible.  - May inspire further work in this direction.    Cons:  - The speed is not systematically evaluated, as compared to finite-difference-based engines. It is thought to be ""in the same ballpark"" as other more full-featured engines. It is not clear that those using simulators will care whether it uses the true derivatives or finite differences.","I would definitely love to have this and use it for my research. A great tool. However, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","I would definitely love to have this and use it for my research. A great tool. However, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack"," The paper lacks detail and is impossible for someone to reimplement the research . However, replicability is a crucial part of science . ICLR is not the right venue: IROS and ICRA might appreciate it much more .",ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more.-------------I am not convinced that ICLR is the right venue: robotics conferences such as,0.19428571428571426,0.023121387283236993,0.13714285714285715,0.8474785089492798,0.7999593019485474,0.8230335712432861
https://openreview.net/forum?id=SyEiHNKxx,"A differentiable physics engine is indeed a wonderful thing to have. ----------------The key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Besides describing the engine's speed in rather creative units (e.g. ""model seconds per day""), the reader has no idea if this is fast or slow. Todorov's engine (my simulator of choice) computes a dynamics step and its derivatives wrt both states and controls (using finite-differences) in less than 1ms for a *full humanoid* model (his code is available here mujoco.org/book/programming.html#saDerivative). I think this actually faster than the engine described in this paper, but I can't be sure.----------------Because this engine is so limited in what it can collide (sphere/sphere and sphere/plane), it would be trivial to build the example models in several other popular engines (e.g. ODE, Bullet and MuJoCo) and simply compare the performance. Until this comparison is done I consider the paper to be incomplete.","Originality, significance:   The paper implements a physics-based simulator directly using Theano. This avoids the type of finite differentiation that physics engines such as MuJoCo use to compute derivatives. It is quite an interesting idea, and is demonstrated using learned control for several models.     Quality, clarity:   The original version was somewhat loosely written; the current version is improved.    Pros:  - The nice idea of implementing a physics engine in a language such as Theano, and showing that this is quite feasible.  - May inspire further work in this direction.    Cons:  - The speed is not systematically evaluated, as compared to finite-difference-based engines. It is thought to be ""in the same ballpark"" as other more full-featured engines. It is not clear that those using simulators will care whether it uses the true derivatives or finite differences.","The key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Todorov's engine computes a dynamics step and its derivatives wrt both states and controls (using finite-differences",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Todorov's engine computes a dynamics step and its derivatives wrt both states and controls (using finite-differences"," The key selling point of the proposed software is its speed, however there is no comparison to other physics engines . Todorov's engine computes a dynamics step and its derivatives wrt both states and controls (using finite-differences",". I think this actually faster than the engine described in this paper, but I can't be sure. ------------------The key selling point of the proposed software is its speed, however there is no comparison to other",0.23121387283236994,0.023391812865497075,0.13872832369942198,0.8458303213119507,0.8146129846572876,0.8299282193183899
https://openreview.net/forum?id=SyEiHNKxx,"This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.----------------My two key reservations with the paper are as follows:--------1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.--------2. They key novelty/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.----------------For the reader to fully understand the pros and cons of the approach, the paper should also include quantitative comparison between the speed of the proposed simulator, and that of standard simulation platforms.----------------Because the idea is interesting and novel, I think it lies above the acceptance threshold. However, it would be significantly improved with the aforementioned comparisons.----------------Lastly, the writing of the paper could be improved, as it is rather informal and/or imprecise in a number of places. Here are some examples:---------- “we model the use of a neural network as a general controller for a robot” - can be more concisely phrased as something like “we model the robot controller using a neural network” or “the robot controller is modeled using a neural network""---------- “In previous research, finding a gradient…” - This is a run-on sentence.---------- “We basically jam this entire equation into” - This sentence is informal and imprecise.---------- “deep learning neural network” - the word “learning” should be omitted---------- “one big RNN, where we unfold over time” - should be “…RNN, which we unfold over time” or “…RNN, unfolded over time”----------------The writing would also be improved by making it more concise and fitting the paper into 8 pages.","Originality, significance:   The paper implements a physics-based simulator directly using Theano. This avoids the type of finite differentiation that physics engines such as MuJoCo use to compute derivatives. It is quite an interesting idea, and is demonstrated using learned control for several models.     Quality, clarity:   The original version was somewhat loosely written; the current version is improved.    Pros:  - The nice idea of implementing a physics engine in a language such as Theano, and showing that this is quite feasible.  - May inspire further work in this direction.    Cons:  - The speed is not systematically evaluated, as compared to finite-difference-based engines. It is thought to be ""in the same ballpark"" as other more full-featured engines. It is not clear that those using simulators will care whether it uses the true derivatives or finite differences.","This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics. The",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics. The"," This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation . The proposed method is only useful for robotics if the learned policy can transfer the real world .","is based on learning policies for robotics. This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is",0.26285714285714284,0.046242774566473986,0.18285714285714286,0.8642228841781616,0.8056977987289429,0.8339347839355469
https://openreview.net/forum?id=BJm4T4Kgx,"This paper has two main contributions: --------(1) Applying adversarial training to imagenet, a larger dataset than previously considered --------(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.----------------This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.","This paper is an empirical study of adversarial training in a large-scale regime.   Its main contributions are to demonstrate that Imagenet models can be made robust to adversarial examples using relatively efficient constructions (so-called 'one-step' methods). Along the way, the authors also report the 'label leaking' effect, a flaw in previous adversarial constructions that limits the effectiveness of adversarial examples at regularizing training.     The reviewers were consensual that this contribution is useful to the field, and the label leaking phenomena is an important aspect that this paper helps address and mitigate. The authors responded promptly to reviewers questions. Despite limited novelty and lack of quantitative analysis, I recommend accept as a poster.","This paper has two main contributions: Applying adversarial training to imagenet, a larger dataset than previously considered and comparing different training approaches. The authors also uncover and explain the label leaking effect which is an important contribution.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper has two main contributions: Applying adversarial training to imagenet, a larger dataset than previously considered and comparing different training approaches. The authors also uncover and explain the label leaking effect which is an important contribution."," This paper has two main contributions: Applying adversarial training to imagenet, a larger dataset than previously considered . Comparing different training approaches, focusing importantly on the transferability of different methods .","paper has two main contributions: --------(1) Applying adversarial training to imagenet, a larger dataset than previously considered --------(2) Comparing different adversarial training approaches, focusing importantly on the transferability of",0.3026315789473684,0.13333333333333333,0.2105263157894737,0.8898645639419556,0.8455305695533752,0.867131233215332
https://openreview.net/forum?id=BJm4T4Kgx,"This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.--------The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.----------------Pros:--------For the first time in the literature, this paper proposed the concept of ‘label leaking’. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.--------Using the ratio of 'clean accuracy' over ‘adversarial accuracy’ as the measure of robust is more reasonable compared to the existing works in the literature. ----------------Cons:--------Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the ‘large-scale’ in the title and in the introduction seems improper. --------Basically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets. ","This paper is an empirical study of adversarial training in a large-scale regime.   Its main contributions are to demonstrate that Imagenet models can be made robust to adversarial examples using relatively efficient constructions (so-called 'one-step' methods). Along the way, the authors also report the 'label leaking' effect, a flaw in previous adversarial constructions that limits the effectiveness of adversarial examples at regularizing training.     The reviewers were consensual that this contribution is useful to the field, and the label leaking phenomena is an important aspect that this paper helps address and mitigate. The authors responded promptly to reviewers questions. Despite limited novelty and lack of quantitative analysis, I recommend accept as a poster.","The paper is well written and easy to follow. For the first time in the literature, this paper proposed the concept of ‘label leaking’ The title of the paper seems a little misleading.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper is well written and easy to follow. For the first time in the literature, this paper proposed the concept of ‘label leaking’ The title of the paper seems a little misleading.", This paper raises several noteworthy finding from its experiments . It proposed the concept of ‘label leaking’ for the first time in the literature . The title of the paper seems a little misleading .,"based on the experiments on ImageNet, the title of the paper seems a little misleading. --------Basically all the conclusions of the paper are made based on observing the experimental results. Without that, the conclusions of",0.24324324324324328,0.0410958904109589,0.17567567567567566,0.8688430786132812,0.8231438398361206,0.8453763127326965
https://openreview.net/forum?id=BJm4T4Kgx,"This paper is a well written paper. This paper can be divided into 2 parts:--------1.Adversary training on ImageNet --------2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity----------------For part [1], I don’t think training without clean example will not make reasonable ImageNet level model. Ian’s experiment in “Explaining and Harnessing Adversarial Examples” didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian’s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.----------------For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation ""FGSM examples are most transferable"".  ----------------In this part the authors raise many interesting problems or guess, but lack theoretical explanations. ----------------Overall I think these empirical observations are useful for future work. ","This paper is an empirical study of adversarial training in a large-scale regime.   Its main contributions are to demonstrate that Imagenet models can be made robust to adversarial examples using relatively efficient constructions (so-called 'one-step' methods). Along the way, the authors also report the 'label leaking' effect, a flaw in previous adversarial constructions that limits the effectiveness of adversarial examples at regularizing training.     The reviewers were consensual that this contribution is useful to the field, and the label leaking phenomena is an important aspect that this paper helps address and mitigate. The authors responded promptly to reviewers questions. Despite limited novelty and lack of quantitative analysis, I recommend accept as a poster.","This paper is a well written paper. It can be divided into 2 parts. The experiments cover most variables in adversary training, yet lack technical depth.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper is a well written paper. It can be divided into 2 parts. The experiments cover most variables in adversary training, yet lack technical depth."," This paper can be divided into two parts: Adversary training on ImageNet and study of label leak, single/multiple step attack and importance of model capacity .",". I suggest adding an experiment of training without clean samples.-------------For part [1], The experiments cover most variables in adversary training, yet lack technical depth.-------------For part [1], The",0.1702127659574468,0.05755395683453237,0.15602836879432624,0.8809469938278198,0.8230859637260437,0.8510341644287109
https://openreview.net/forum?id=SJx7Jrtgl,"This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). ----------------A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. ----------------Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the samples in (6).----------------A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength, namely \eta (eq. (3)) and \alpha (eq. 7). According to the authors adjusting these parameters is crucial, and there seems to be no principled way of adjusting them. It also seems that these two parameters interact, since they both regularize z in different ways. This makes the search space over them grow multiplicatively, since the tuning problem now becomes combinatorial. The authors mention that they tune the trade-off between these two regularizers, but I'd be interested in a comment concerning how this is done (what's the space of parameters to search on). In practical clustering applications, high sensitivity to tuned parameters is undesirable, since one also needs to cross-validate values of K at the same time.----------------I really liked the experiments section. It is not very exhaustive in terms of comparison, but it is very exploratory in terms of demonstrating the model components, strengths and weaknesses. This is much more useful than reporting unintuitive percentage improvements relative to arbitrarily selected baselines and datasets.------------------------Overall, I am a little concerned about the practicality of this approach, given the tuning it requires. However, I am in favor of accepting this paper because it makes its strong and weak points very clear through good explanation and demonstration. Therefore, I expect further research to be built on top of this paper, so that the aforementioned issues will hopefully be alleviated in the future. Finally, the theoretical intuitions given in the paper (and author comments) improve its usefulness as a scientific manuscript.","The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference.",This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has, This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario . First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has,", but I'd be interested in seeing a discussion on the variance of the samples in (6).----------------------The authors mention that they tune the trade-off between these two regularizers.",0.1592920353982301,0.0,0.12389380530973451,0.8369753956794739,0.8460036516189575,0.8414652347564697
https://openreview.net/forum?id=SJx7Jrtgl,"The authors posit a mixture of Gaussian prior for variational--------auto-encoders. They also consider a regularization term motivated--------from information theory.----------------The modeling extension is simple and the inference follows--------mechanically from what's already standard in the literature. Instead--------of using discrete latent variable samples they collapse the expected--------KL; this works for few mixture components and has been considered--------before in more general contexts, e.g., Titsias and Lazaro-Gredilla--------(2015). It will not scale to many mixture components.----------------I find the discussion in Section 3.2.2 difficult to parse and, if I--------understood it correctly, not necessarily correct. Many arguments are--------introduced and few fleshed out. First, there is a claim that a--------multinomial prior with equal class probabilities assigns the same--------number of data points to each class on average; this is true a priori--------but certainly not true given data. Second, they claim the KL--------regularizer forces the approximate posterior to be close to this--------uniform; this is only true for small data, certainly the energy term--------in the ELBO (expected log-likelihood) will overpower the regularizer;--------is this not the case in a mean-field approximation to a mixture of--------Gaussians model? Third, there is a claim that ""under the mean-field--------approximation, this constraint is enforced on each sample""; how does--------the mean-field approximation enforce a constraint on the effect of--------Monte Carlo sampling? Fourth, they argue Johnson et al. (2016) can--------overcome this issue partly due to SVI; how does data subsampling--------affect this behavior? Fifth, they derive the exact posterior in--------Equation 6; so to what extent are these arguments relevant?----------------The experiments are limited on toy data and only a few mixture--------components are considered (not enough where the collapsed approach--------will not scale).----------------+ Titsias, M. K., & Lázaro-Gredilla, M. (2015). Local Expectation Gradients for Black Box Variational Inference. In Neural Information Processing Systems.","The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference.",The authors posit a mixture of Gaussian prior for variational auto-encoders. They also consider a regularization term motivated from information theory. The experiments are limited on toy data and only a few mixture components are considered.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors posit a mixture of Gaussian prior for variational auto-encoders. They also consider a regularization term motivated from information theory. The experiments are limited on toy data and only a few mixture components are considered., The authors posit a mixture of Gaussian prior for variational auto-encoders . They also consider a regularization term motivated from information theory . The experiments are limited on toy data and only a few mixture components are considered .,------------------------.. Local Expectation Gradients for Black Box Variational Inference. In Neural Information Processing Systems.------------- (2015).,0.10526315789473685,0.0,0.07017543859649122,0.8415212631225586,0.8375624418258667,0.8395371437072754
https://openreview.net/forum?id=SJx7Jrtgl,"The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.--------The use of a mixture of VAE is an incremental idea if novel.--------I would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.--------The results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the ""Best Run"". Was the best run maximizing the validation performance or the test performance ? Moreover, the average performance (higher) and standard deviation (lower) of Adversarial Autoencoder makes me question the claim that ""we have advanced the state of the art in deep unsupervised clustering both in theory and practice"".--------The consistency violation regularization might be interesting. But the cluster degeneracy problem is, as far as I know, also a problem in plain mixture of gaussians models. So making an experiment on this simpler model on a synthetic dataset should also be done.--------In general, I would recommend running more experiments as to solidify your claims.","The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference.","The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVH",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVH", Authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization . GMVAE+ seems to outperform other methods,of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization.--------The results in Table 1 are questionable,0.12500000000000003,0.0,0.08928571428571427,0.8334401845932007,0.8367701172828674,0.8351017832756042
https://openreview.net/forum?id=BkLhzHtlg,"While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). ----------------The overall approach is interesting: all three of the key techniques (aux. tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.----------------I found some of the results hard to understand/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. ----------------It may be worthwhile very briefly mentioning the relationship of ""diagonal"" connections to other emerging terms for similar ideas (e.g. skip connections, etc). ""Skip"" seems to me to be accurate regardless of how you draw the network, whereas ""diagonal"" only makes sense for certain visual layouts.----------------In response to comment in the discussion below: ""leading to less over-segmentation of action bouts"" (and corresponding discussion in section 5.1 of the paper): I would be like to have a bit more about this in the paper. I have assumed that ""per-bout"" refers to ""per-action event"", but now I am not certain that I have understood this correctly (i.e. can a ""bout"" last for a few minutes?): given the readership, I think it would not be inappropriate to define some of these things explicitly.----------------In response to comment about fly behaviours that last minutes vs milliseconds: This is interesting, and I would be curious to know how classification accuracy relates to the time-scale of the behaviour (e.g. are most of the mistakes on long-term behaviours? i realize that this would only tell part of the story, e.g. if you have a behaviour that has both a long-term duration, but that also has very different short-term characteristics than many other behaviours, it should be easy to classify accuractely despite being ""long-term""). If easy to investigate this, I would add a comment about it; if this is hard to investigate, it's probably not worth it at this point, although it's something you might want to look at in future.----------------In response to comment about scaling to human behavior: I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try, but seriously: there is usually a pretty big gap between what ""should"" work and what actually works, as I am sure the authors are aware. (Indeed, I am sure the authors have a much more experiential and detailed understanding of the limitations of their work than I do). What I see presented is a nice system that has been demonstrated to handle spatiotemporal trajectories. The claims made should correspond to this. ----------------I would consider adjusting my rating to a 7 depending on future revisions.","Originality and Significance:   The paper develops a recurrently coupled discriminative / generative hierarchical model, as applied to fruit-fly behavior and online handwriting. Qualitative evaluation is provided by generating motions, in addition to quantitative results. Writer identity and fly gender are learned in an unsupervised fashion. While the individual components of the solution are not particularly novel, their combination together with the detailed experimental validation makes the method potentially interesting to a broad audience. Some reviewers have concerns about the broad claims that are implied by the title and abstract, and thus it is recommended that these be refined to be more specific about the method and the applications.    Quality and Clarity:   The paper is well written.    Pros:  - interesting problem and application domains; will be of broad interest  - useful ideas and architecture: shows that forcing the network to predictions about motion leads to improved classification  - well written paper backed up by good experiments    Cons:  - the individual architectural features have for the most part been proposed before","All three of the key techniques (aux tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense. I found some of the results hard to understand/interpret.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","All three of the key techniques (aux tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense. I found some of the results hard to understand/interpret."," All three of the key techniques (aux tasks, skip/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense . I found some of the results hard to understand/interpret .",": I agree that in principle, adding conv layers directly above the sensory input would be the right thing to try.-------------I would consider adjusting my rating to a 7 depending on future revisions.-",0.13793103448275862,0.029850746268656712,0.10837438423645321,0.838259220123291,0.7879012823104858,0.8123005628585815
https://openreview.net/forum?id=BkLhzHtlg,"The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.----------------The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.--------Motion is discretized and predicted using classification. The model is trained on classification loss combined with a loss on motion prediction. The goal is to leverage latter loss in a semi-supervised setting from parts of the data which do not contain action labels.----------------The idea of leveraging predictions to train feature representations for discrimination is not new. However, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.----------------My biggest concern is with the experimental evaluation. The experimental section contains a large amount of figures, which visualize what the model has learned in a qualitative way. However, quantitative evaluation is rarer.----------------- On the fly application, the authors compare the classification performance with another method previously published by the first author.--------- Again on the fly application, the performance gain on motion prediction in figure 5c looks small compared to the baseline. I am not sure it is significant.--------- I did not see any recognition results on the handwriting application. Has this part not been evaluated?----------------Figure 5a is difficult to understand and to interpret. The term ""BesNet"" is used here without any introduction.----------------Figure 4 seems to tell multiple and different stories. I'd suggest splitting it into at least two different figures.","Originality and Significance:   The paper develops a recurrently coupled discriminative / generative hierarchical model, as applied to fruit-fly behavior and online handwriting. Qualitative evaluation is provided by generating motions, in addition to quantitative results. Writer identity and fly gender are learned in an unsupervised fashion. While the individual components of the solution are not particularly novel, their combination together with the detailed experimental validation makes the method potentially interesting to a broad audience. Some reviewers have concerns about the broad claims that are implied by the title and abstract, and thus it is recommended that these be refined to be more specific about the method and the applications.    Quality and Clarity:   The paper is well written.    Pros:  - interesting problem and application domains; will be of broad interest  - useful ideas and architecture: shows that forcing the network to predictions about motion leads to improved classification  - well written paper backed up by good experiments    Cons:  - the individual architectural features have for the most part been proposed before","The paper presents a method for joint motion prediction and activity classification from sequences with two different applications. The method uses a classical encoder-decoder pipeline, with skip connections.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents a method for joint motion prediction and activity classification from sequences with two different applications. The method uses a classical encoder-decoder pipeline, with skip connections."," The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition . The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between",", which visualize what the model has learned in qualitative way.-------------The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting",0.15463917525773194,0.020833333333333336,0.09278350515463916,0.8561376929283142,0.7912614345550537,0.8224220871925354
https://openreview.net/forum?id=BkLhzHtlg,"This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.--------The paper is well written, clear in its presentation and backed up by good experiments.--------They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,--------allowing more accurate classification with less training data.--------They also show how the information learned by the network is interpretable and organised in a hierarchy.----------------Weaknesses:--------- a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.--------- moreover, a discussion on how this approach could scale to more challenging scenarios ""involving animals"" and visual input for instance and more general ""behaviours"" is also missing;--------The criticism here is pointed at the fact that the title/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,--------making the original claim a little bit far fetched unless its backed up by additional evidence.--------Using ""Insects"", or ""fruit flies"" would be more appropriate than ""animals"".","Originality and Significance:   The paper develops a recurrently coupled discriminative / generative hierarchical model, as applied to fruit-fly behavior and online handwriting. Qualitative evaluation is provided by generating motions, in addition to quantitative results. Writer identity and fly gender are learned in an unsupervised fashion. While the individual components of the solution are not particularly novel, their combination together with the detailed experimental validation makes the method potentially interesting to a broad audience. Some reviewers have concerns about the broad claims that are implied by the title and abstract, and thus it is recommended that these be refined to be more specific about the method and the applications.    Quality and Clarity:   The paper is well written.    Pros:  - interesting problem and application domains; will be of broad interest  - useful ideas and architecture: shows that forcing the network to predictions about motion leads to improved classification  - well written paper backed up by good experiments    Cons:  - the individual architectural features have for the most part been proposed before","This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents. The paper is well written, clear in its presentation and backed up by good experiments.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents. The paper is well written, clear in its presentation and backed up by good experiments."," This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents . The paper is well written, clear in its presentation and backed up by good experiments .","--------The paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.--------The paper is well written, clear in its presentation and backed up by good experiments.-",0.22564102564102564,0.09326424870466321,0.17435897435897435,0.8979711532592773,0.8032185435295105,0.8479560613632202
https://openreview.net/forum?id=Hk8TGSKlg,"First I would like to apologize for the delay in reviewing.----------------Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.--------The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.----------------In order to get a better sense of the reason for improvement it would be interesting to have a complexity and/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments.----------------The dataset description in section 2.2, should be moved to section 4 where the other datasets are described.","This paper proposes a memory-enhanced RNN in the vein of NTM, and a novel training method for this architecture of cloze-style QA. The results seem convincing, and the training method is decently novel according to reviewers, although the evaluation seemed somewhat incomplete according to reviewers and my own reading. For instance, it is questionable whether or not the advertised human performance on CNN/DM is accurate (based on 100 samples from a 300k+ dataset), so I'm not sure this warrants not evaluating or reporting performance on it. Overall this looks like an acceptable paper, although there is room for improvement.","The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one. The dataset description in section 2.2, should be moved to section 4.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one. The dataset description in section 2.2, should be moved to section 4.", This work introduces a novel memory based artificial neural network for reading comprehension . Experiments show improvement on state of the art .,: this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.-------------The originality of the approach seems to be on the implementation of,0.22377622377622375,0.0,0.1258741258741259,0.8487405776977539,0.8286082148551941,0.8385535478591919
https://openreview.net/forum?id=Hk8TGSKlg,"Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments:----------------1. Actually the model in the paper is not single model, it proposed two models. One consists of ""reading"", ""writing"", ""adaptive computation"" and "" Answer module 2"", the other one is ""reading"", ""composing"", ""writing"", ""gate querying"" and ""Answer module 1"". Based on the method section and the experiment, it seems the ""adaptive computation"" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine.----------------2. What is the MLP setting in the composing module? ----------------3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could you find 436? Is there any tricks helping you find those numbers?----------------4. It needs more ablation study about using different T such as T=1,2..----------------5. According to my understanding, for the adaptive computation,  it would stop when the P_T <0. So what is the distribution of T in the testing data?","This paper proposes a memory-enhanced RNN in the vein of NTM, and a novel training method for this architecture of cloze-style QA. The results seem convincing, and the training method is decently novel according to reviewers, although the evaluation seemed somewhat incomplete according to reviewers and my own reading. For instance, it is questionable whether or not the advertised human performance on CNN/DM is accurate (based on 100 samples from a 300k+ dataset), so I'm not sure this warrants not evaluating or reporting performance on it. Overall this looks like an acceptable paper, although there is room for improvement.","Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good."," Thie paper proposed an iterative memory updating model for cloze-style question-answering task . The approach is interesting, and result is good .",":(256, 368, 436, 512) This paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the",0.208,0.03252032520325203,0.15999999999999998,0.8840473890304565,0.8356752395629883,0.8591809868812561
https://openreview.net/forum?id=Hk8TGSKlg,"This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models/multi-hop reasoning in previous papers.----------------There are a couple places that this paper can improve. First, I would like to see the results from CNN/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query gating) to help understand better the query regression and if it is human interpretable.","This paper proposes a memory-enhanced RNN in the vein of NTM, and a novel training method for this architecture of cloze-style QA. The results seem convincing, and the training method is decently novel according to reviewers, although the evaluation seemed somewhat incomplete according to reviewers and my own reading. For instance, it is questionable whether or not the advertised human performance on CNN/DM is accurate (based on 100 samples from a 300k+ dataset), so I'm not sure this warrants not evaluating or reporting performance on it. Overall this looks like an acceptable paper, although there is room for improvement.","The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general. Another advantage",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read/compose/write operations seem to be more general. Another advantage", The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW) The concept of read/compose/write operations seem to be more general and can be,for cloze-style QA. This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-,0.2714285714285714,0.04347826086956522,0.1285714285714286,0.8606687784194946,0.8355042338371277,0.8478999137878418
https://openreview.net/forum?id=r1tHvHKge,"This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a ""danger model"". The paper is well written including some rather poetic language [*].----------------The heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).----------------[*] this reviewer's favourite quotes:--------""Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it’s undesirable.""--------""The child can learn to adjust its behaviour without actually having to stab someone.""--------""... the catastrophe lurking just past the optimal shave.""","This paper presents a few interesting ideas, namely the idea of keeping around a set of ""danger states"" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.    However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.    Pros:  + Interesting idea of keeping around danger states and injecting them into training    Cons:  - Algorithm doesn't seem that well motivated  - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.  - Experiments aren't that convincing (better after revisions, but still need work)","This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a ""danger model"" The heuristic is evaluated in two toy domains.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a ""danger model"" The heuristic is evaluated in two toy domains."," This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a ""danger model"" The paper is well written including some rather poetic language .",-----------------The heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari.,0.10480349344978167,0.02643171806167401,0.07860262008733623,0.8547777533531189,0.7997033596038818,0.8263238668441772
https://openreview.net/forum?id=r1tHvHKge,"- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. ----------------- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. ""Reinforcement learning with unsupervised auxiliary tasks."", Blundell, Charles, et al. ""Model-free episodic control."" , Narasimhan et al. ""Language understanding for text-based games using deep reinforcement learning""), which could perhaps reinforce the agent to avoid revisiting catastrophic states. ----------------- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? ","This paper presents a few interesting ideas, namely the idea of keeping around a set of ""danger states"" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.    However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.    Pros:  + Interesting idea of keeping around danger states and injecting them into training    Cons:  - Algorithm doesn't seem that well motivated  - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.  - Experiments aren't that convincing (better after revisions, but still need work)","- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the"," Authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper . Overall, the approach presented is not very principled .",- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. -------------------- There is also an increasing amount of literature of using reward replay buffers in,0.175,0.04201680672268908,0.13333333333333333,0.8629923462867737,0.8124949336051941,0.8369826674461365
https://openreview.net/forum?id=r1tHvHKge,"This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).----------------1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of “robust RL”. A good place to start might be with the work of Shie Mannor.----------------2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as “catastrophic” if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I’d want to see plots for “average reward” in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible “danger states”?----------------3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I’m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant “state-of-the-art” variant of DQN are you using?----------------The good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.----------------Given my comment a couple of weeks ago, and the prompt response (“we implemented expected SARSA”), I would have expected that the paper had been revised with the new results by now? In any case, I’m open to discussing all these points and revising my opinion based on an updated version of the paper.----------------Minor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. “Sergey et al. Levine”. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.","This paper presents a few interesting ideas, namely the idea of keeping around a set of ""danger states"" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.    However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.    Pros:  + Interesting idea of keeping around danger states and injecting them into training    Cons:  - Algorithm doesn't seem that well motivated  - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.  - Experiments aren't that convincing (better after revisions, but still need work)",This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea) A good place to start might be with the work of Shie Mannor.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea) A good place to start might be with the work of Shie Mannor., This paper addresses an important and timely topic in a creative way . I consider it to have three flaws (and one good idea) I'm open to discussing all these points and revising my opinion .,"ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and two additional replay memories.",0.15384615384615385,0.008620689655172414,0.11111111111111112,0.8473661541938782,0.7959516644477844,0.8208546042442322
https://openreview.net/forum?id=BymIbLKgl,"I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that ""if it's not worth doing, it's not worth doing well."" There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of ""why use this representation"" with the authors and they said their ""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network."" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use of smoothed/filtered representations coupled with nonlinearities, it's sensible to revisit this problem using convnets. I wouldn't mind seeing this paper accepted, since it's different from the mainstream, but I worry about there being too narrow an audience at ICLR that still cares about this type of shape representation.","This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.","There is no question that the MPEG-7 dataset/benchmark got saturated long ago. I brought up the question of ""why use this representation"" with the authors. They said their ""main purpose was to connect the theory of differential",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","There is no question that the MPEG-7 dataset/benchmark got saturated long ago. I brought up the question of ""why use this representation"" with the authors. They said their ""main purpose was to connect the theory of differential"," There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference . I wouldn't mind seeing this paper accepted, but I worry","and references to curvature scale space brought to mind the old saying that ""if it's not worth doing, it's not worth doing well"" There is no question that the MPEG-7 dataset/benchmark got saturated",0.2,0.033707865168539325,0.1,0.8354419469833374,0.8056872487068176,0.8202948570251465
https://openreview.net/forum?id=BymIbLKgl,"Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.----------------The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.----------------Furthermore the paper misses a more thorough quantitative evaluation and concentrates more on showing particular examples, instead of measuring more robust statistics over multiple curves (invariance to noise and sampling artifacts).----------------In general, the paper shows interesting first steps in this direction, however it is not clear whether the experimental section is strong and thorough enough for the ICLR conference. Also the novelty of the proposed idea is limited as Siamese networks are used for many years and this work only shows that they can be applied to a different task.","This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.","Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants"," Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves . With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants .",for a Siamese architecture can be used for learning representations for planar curves. Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for plan,0.31111111111111117,0.056179775280898875,0.13333333333333333,0.8948270082473755,0.8273921608924866,0.8597893118858337
https://openreview.net/forum?id=BymIbLKgl,"Pros : --------- New representation with nice properties that are derived and compared with a mathematical baseline and background--------- A simple algorithm to obtain the representation----------------Cons :--------- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first.","This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects.",A new representation with nice properties that are derived and compared with a mathematical baseline and background. A simple algorithm to obtain the representation.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",A new representation with nice properties that are derived and compared with a mathematical baseline and background. A simple algorithm to obtain the representation.," The paper sounds like an applied maths paper, but further analysis could be done, for instance, by understanding the nature of each layer, or at least the first .",: --------- New representation with nice properties that are derived and compared with a mathematical baseline and background------------ A simple algorithm to obtain the representation----------------Cons :------,0.1696969696969697,0.024539877300613494,0.1090909090909091,0.8697493672370911,0.805070698261261,0.8361611366271973
https://openreview.net/forum?id=HJpfMIFll,"On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold).----------------On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results).----------------For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (""MFS"") yields the highest F-measure of 63.5%. The paper's result with ""2 clusters"" (with an average of about 1.9) seems to be close to MFS. So I don't think we can use F-measure to compare.----------------The V-measure seems to be tilted towards systems that have high number of senses per word. UoY has V=15.7%, while the paper (with ""5 clusters"") has 14.4%. That isn't very convincing that the proposed method has captured the geometry of polysemy.----------------In general, I have often wondered why people work on pure unsupervised WSI and WSD. The assessment is very difficult (as described above). More importantly, some very weakly supervised systems (with minimal labels) can work pretty well to bootstrap. See, e.g., the classic paper [3].----------------If the authors used the Grassmannian idea to solve higher-level NLP problems directly (such as analogies), that would be very persuasive. However, that's a very different paper than what was submitted. For an example of application of Grassmannian manifolds to analogies, see [4].----------------References:--------1. Manandhar, Suresh, et al. ""SemEval-2010 task 14: Word sense induction & disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.--------2. Korkontzelos, Ioannis, and Suresh Manandhar. ""Uoy: Graphs of unambiguous vertices for word sense induction and disambiguation."" Proceedings of the 5th international workshop on semantic evaluation. Association for Computational Linguistics, 2010.--------3. Yarowsky, David. ""Unsupervised word sense disambiguation rivaling supervised methods."" Proceedings of the 33rd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1995.--------4. Mahadevan, Sridhar,  and Sarath Chandar Reasoning about Linguistic Regularities in Word Embeddings using Matrix Manifolds https://arxiv.org/abs/1507.07636 ",The paper considers an important problem largely ignored by continuous word representation learning: polysemy. The approach is mathematically grounded and interesting and well explored.,"On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold) The experiments in this paper done on SemEval-2010 are not very persuasive.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold) The experiments in this paper done on SemEval-2010 are not very persuasive."," The experiments in this paper done on SemEval-2010 are not very persuasive . On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold)",": Word sense induction & disambiguation.-------------On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a ",0.2033898305084746,0.035087719298245605,0.13559322033898305,0.8426293730735779,0.8644025921821594,0.8533771634101868
https://openreview.net/forum?id=HJpfMIFll,"This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.----------------The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose, making it hard to follow the description of the method. Some of the justification (""intersection hypothesis"", ""polysemy intersection hypothesis"") might feel a bit like hand waving.----------------Overall, the work presented in the paper looks solid.----------------Pros:-------- - I really liked the idea of representing a word in context by a subspace (as opposed to a weighted sum). Indeed, such representations captures much more information than a single vector.-------- - The proposed method also obtain very good results, compared to existing polysemous word embeddings.-------- - It can be used with any word vectors, making its application very easy.----------------Cons:-------- - I felt that the paper is sometimes a bit verbose and some justifications might be a bit hand waving.-------- - I am also wondering how much of the improvement over existing approaches is due to the quality of the word2vec embeddings, or due to the proposed approach. It would therefore be nice to have a comparison with a regular k-means approach, where context are represented as sum of word vectors using the same embeddings.",The paper considers an important problem largely ignored by continuous word representation learning: polysemy. The approach is mathematically grounded and interesting and well explored.,"This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of"," This paper describes a new method to capture word polysemy with word embeddings . In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of","to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the",0.2333333333333333,0.0,0.16666666666666669,0.8487710356712341,0.8701381683349609,0.8593217730522156
https://openreview.net/forum?id=HJpfMIFll,"This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks.----------------The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR.",The paper considers an important problem largely ignored by continuous word representation learning: polysemy. The approach is mathematically grounded and interesting and well explored.,"This paper presents a study of the spaces around existing word embeddings. The subspace picture allows for a little more information to be preserved. The paper is a little dense reading at times, but the perspective is original enough.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents a study of the spaces around existing word embeddings. The subspace picture allows for a little more information to be preserved. The paper is a little dense reading at times, but the perspective is original enough."," This paper presents a study of the spaces around existing word embeddings . Instead of representing a word token by a vector, represent it by the subspace spanned by embedded word types . These subspaces capture some notions of poly","of context word embeddings. I propose something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that",0.15873015873015872,0.03278688524590164,0.12698412698412698,0.8695932030677795,0.862443208694458,0.8660034537315369
https://openreview.net/forum?id=SkgSXUKxx,"Summary--------===--------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016. In that paper a regularizer was proposed which penalizes--------gradient magnitudes and it was shown to aid low-shot learning performance.--------This work shows that the previous regularizer is equivalent to a direct penalty--------on the magnitude of feature values weighted differently per example.----------------The analysis goes to to provide two examples where a feature penalty--------favors a better representation. The first example addresses the XOR--------problem, constructing a network where a feature penalty encourages--------a representation where XOR is linearly separable.--------The second example analyzes a 2 layer linear network, showing improved stability--------of a 2nd order optimizer when the feature penalty is added.--------One last bit of analysis shows how this regularizer can be interpreted as--------a Gaussian prior on both features and weights. Since the prior can be--------interpreted as having a soft whitening effect, the feature regularizer--------is like a soft version of Batch Normalization.----------------Experiments show small improvements on a synthetic XOR test set.--------On the Omniglot dataset feature regularization is better than most baselines,--------but is worse than Moment Matching Networks. An experiment on ImageNet similar--------to Hariharan and Girshick 2016 also shows effective low-shot learning.------------------------Strengths--------===----------------* The core proposal is a simple modification of Hariharan and Girshick 2016.----------------* The idea of feature regularization is analyzed from multiple angles--------both theoretically and empirically.----------------* The connection with Batch Normalization could have broader impact.------------------------Weaknesses--------===----------------* In section 2 the gradient regularizer of Hariharan and Girshick is introduced.--------While introducing the concept, some concern is expressed about the motivation:--------""And it is not very clear why small gradients on every sample produces--------good generalization experimentally."" This seems to be the central issue to me.--------The paper details some related analysis, it does not offer a clear answer to--------this problem.------------------------* The purpose and generality of section 2.1 is not clear.----------------The analysis provides a specific case (XOR with a non-standard architecture)--------where feature regularization intuitively helps learn a better representation.--------However, the intended take-away is not clear.----------------The take-away may be that since a feature penalty helps in this case it--------should help in other cases. I am hesitant to buy that argument because of the--------specific architecture used in this section. The result seems to rely on the--------choice of an x^2 non-linearity, which is not often encountered in recent neural--------net literature.----------------The point might also be to highlight the difference between a weight--------penalty and a feature penalty because the two seem to encourage--------different values of b in this case. However, there is no comparison to--------a weight penalty on b in section 2.1.------------------------* As far as I can tell, eq. 3 depends on either assuming an L2 or cross-entropy--------loss. A more general class of losses for which eq. 3 holds is not provided. This--------should be made clear before eq. 3 is presented.------------------------* The Omniglot and ImageNet experiments are performed with Batch Normalization,--------yet the paper points out that feature regularization may be similar in effect--------to Batch Norm. Since the ResNet CNN baseline includes Batch Norm and there are--------clear improvements over that baseline, the proposed regularizer has a clear--------additional positive effect. However, results should be provided without--------Batch Norm so a 1-1 comparison between the two methods can be performed.------------------------* The ImageNet experiment should be more like Hariharan and Girshick.--------In particular, the same split of classes should be used (provided in--------the appendix) and performance should be measured using n > 1 novel examples--------per class (using k nearest neighbors).------------------------Minor:----------------* A brief comparison to Matching Networks is provided in section 3.2, but the--------performance of Matching Networks should also be reported in Table 1.----------------* From the approach section: ""Intuitively when close to convergence, about half--------of the data-cases recommend to update a parameter to go left, while--------the other half recommend to go right.""----------------Could the intuition be clarified? There are many directions in high--------dimensional space and many ways to divide them into two groups.----------------* Is the SGM penalty of Hariharan and Girshick implemented for this paper--------or using their code? Either is acceptable, but clarification would be appreciated.----------------* Should the first equal sign in eq. 13 be proportional to, not equal to?----------------* The work is dense in nature, but I think the presentation could be improved.--------In particular, more detailed derivations could be provided in an appendix--------and some details could be removed from the main version in order to increase--------focus on the results (e.g., the derviation in section 2.2.1).------------------------Overall Evaluation--------===----------------This paper provides an interesting set of analyses, but their value is not clear.--------There is no clear reason why a gradient or feature regularizer should improve--------low-shot learning performance. Despite that, experiments support that conclusion,--------the analysis is interesting by itself, and the analysis may help lead to a--------clearer explanation.----------------The work is a somewhat novel extension and analysis of Hariharan and Girshick 2016.--------Some points are not completely clear, as mentioned above.","The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.",This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes. It was shown to aid low-shot learning performance.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes. It was shown to aid low-shot learning performance., A paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016 . The idea of feature regularization is analyzed from multiple angles . The connection with Batch Normalization could have broader impact .,a Gaussian prior on both features and weights.----------------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016.,0.45783132530120485,0.1728395061728395,0.2891566265060241,0.9190939664840698,0.8896090388298035,0.9041112065315247
https://openreview.net/forum?id=SkgSXUKxx,"This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.----------------First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about ""results can be derived for φ(x) with convex differentiable non-linear activation functions such as ReLU"", both via analysis and experimentation to measure numerical stability.----------------Second, the authors again show an interesting correspondance to batch normalization, but IMO fail to experimentally show its relevance.----------------Finally, I understand the appeal of the proposed method from a numerical stability point of view, but am not convinced that it has any effect on low-shot learning in the high dimensional spaces that deep networks are used for.----------------I commend the authors for contributing to the mathematical understanding of our field, but I think they have yet to demonstrate the large scale effectiveness of what they propose. At the same time, I feel like this paper does not have a clear and strong message. It makes various (interesting) claims about a number of things, but they seem more or less disparate, and only loosely related to low-shot learning.----------------notes:--------- ""an expectation taken with respect to the empirical distribution generated by the training set"", generally the training set is viewed as a ""montecarlo"" sample of the underlying, unknown data distribution \mathcal{D}.--------- ""we can see that our model learns meaningful representations"", it gets a 6.5% improvement on the baseline, but there is no analysis of the meaningfulness of the representations.--------- ""Table 13.2"" should be ""Table 2"".--------- please be mindful of formatting, some citations should be parenthesized and there are numerous extraneous and missing spacings between words and sentences.","The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.","This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm. The authors argue that it is helpful to low-shot learning, numerically stable, and a soft version of Batch Normalization.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm. The authors argue that it is helpful to low-shot learning, numerically stable, and a soft version of Batch Normalization."," This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm . Authors argue that it is helpful to low-shot learning, it is numerically stable, and it is a soft version of Batch","to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. I feel like the authors could write a full paper about ""results can be derived",0.38554216867469876,0.07407407407407406,0.26506024096385544,0.8623731732368469,0.8568788766860962,0.8596172332763672
https://openreview.net/forum?id=SkgSXUKxx,"The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.--------Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.--------The proposed approach relates to Batch Norm and weight decay.--------Experiments are given on ""low-shot"" settting.--------There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?--------Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?--------Overall, the idea is simple but feels like preliminary: while it is supposed to be a ""soft BN"", BN itself gets better performance than feature penalty, and both together give even better results. Is something still missing in the explanation?------------------ edits after revised version:----------------Thank you for adding more information to the paper. I feel it is still too long but hopefully you can reduce it to 9 pages as promised. However, I'm still not convinced the paper is ready to be accepted, mainly for the following reasons:--------- on Omniglot, the paper is still significantly far from the current state of the art.--------- the new experiments do not really confirm/infirm the relationship with BN.--------- you added an explanation of why FP works for low-shot setting, by showing it controls the VC dimension and hence is good to control overfitting with a small number of training examples, but this discussion is basic and does not really shed more light than the obvious.--------I'm pushing up your score from 4 to 5 for the improved version, but I still think it is below acceptance level.","The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance.","The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. I'm still not convinced the paper is ready to be accepted, mainly for the following reasons.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. I'm still not convinced the paper is ready to be accepted, mainly for the following reasons.", The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net . The proposed approach relates to Batch Norm and weight decay . The paper is still too long but hopefully you can reduce it,", but I'm still not convinced the paper is ready to be accepted.---------Overall, the idea is simple but feels like preliminary: while it is supposed to be a soft batch norm version, and low-shot",0.3448275862068966,0.07058823529411765,0.2528735632183908,0.8862014412879944,0.8573729395866394,0.8715488910675049
https://openreview.net/forum?id=SJQNqLFgl,"The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.----------------I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an ""introduction to training CNNs"" course / blog post. It also seems to be a bit skewed towards recent work that was fairly incremental (e.g. a lot of attention is given to the flurry of ResNet variants).----------------The paper states that ""it is universal in all convolutional neural networks that the activations are downsampled and the number of channels increased from the input to the final layer"", which is wrong. We already discussed this previously re: my question about design pattern 5, but I think the answer that was given (""the nature of design patterns is that they only apply some of the time"") does not excuse making such sweeping claims. This should probably be removed.----------------""We feel that normalization puts all the layer's input samples on more equal footing, which allows backprop to train more effectively"" (section 3.2, 2nd paragraph) is very vague language that has many possible interpretations and should probably be clarified. It also seems odd to start this sentence with ""we feel"", as this doesn't seem like the kind of thing one should have an opinion about. Such claims should be corroborated by experiments and measurements. There are several other instances of this issue across the paper.----------------The connection between Taylor series and the proposed Taylor Series Networks seems very tenuous and I don't think the name is appropriate. The resulting function is not even a polynomial as all the terms represent different functions -- f(x) + g(x)**2 + h(x)**3 + ... is not a particularly interesting object, it is just a nonlinear function of x.----------------Overall, the paper reads like a collection of thoughts and ideas that are not very well delineated, and the experimental results are unconvincing.",The authors agree with the reviewers that this manuscript is not yet ready.,The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-, The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems . It also proposes a few new architectural ideas inspired by these rules . These are experimentally evaluated on CIFAR-,.----------------The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. It also proposes a few new architectural ideas inspired by these rules,0.0392156862745098,0.0,0.0392156862745098,0.8244432210922241,0.857881486415863,0.8408300280570984
https://openreview.net/forum?id=SJQNqLFgl,"The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.----------------The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.----------------Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, ""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality"" is presented as one of 14 core design principles without any further justification. Similarly ""Design Pattern 6: Over-train includes any training method where the network is trained on a harder problem than necessary to improve generalization performance of inference"" is presented in the middle of a paragraph with no supporting references or further explanation.----------------The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles. In general, these approaches either are minor modifications of existing networks (different FractalNet pooling strategies) or are novel architectures that do not perform well. The exception being the Fractal-of-Fractal network which achieves slightly improved accuracy but also introduces many more network parameters (increased capacity) over the original FractalNet.------------------------Preliminary rating:--------It is a useful and perhaps noble task to collect and distill research from many sources to find patterns (and perhaps gaps) in the state of a field; however, some of the patterns presented do not seem well developed and include principles that are poorly explained. Furthermore, the innovative architectures motivated by the design principles either fall short or achieve slightly better accuracy by introducing many more parameters (Fractal-Of-Fractal networks). For a paper addressing the topic of higher level design trends, I would appreciate additional rigorous experimentation around each principle rather than novel architectures being presented.",The authors agree with the reviewers that this manuscript is not yet ready.,The authors have grouped recent work in convolutional neural network design to identify core design principles guiding the field at large. The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The authors have grouped recent work in convolutional neural network design to identify core design principles guiding the field at large. The experimental portion of this paper feels scattered with many different approaches being presented based on subsets of the design principles., The authors have grouped recent work in convolutional neural network design to identify core design principles guiding the field at large . The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an,based on 14 core design principles.----------------The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large,0.18181818181818182,0.037735849056603765,0.14545454545454545,0.8365765810012817,0.879432737827301,0.8574694991111755
https://openreview.net/forum?id=SJQNqLFgl,"The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. ----------------There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the ""community service"" aspect of helping someone who starts figure out the ""coordinate system"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.----------------However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. ----------------Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU, ReLU, ...) and I do not see how it stands on the same grounds as something as general as ""3 Strive for simplicity"".----------------Similarly some of the patterns are as vague as ""Increase symmetry"" and are backed up by statements such as ""we noted a special degree of elegance in the FractalNet"". I do not see how this leads to a design pattern that can be applied to a new architecture - or if it applies to anything other than the FractalNet. ----------------Some other patterns are phrased with weird names ""7 Cover the problem space"" - which I guess stands for dataset augmentation; or ""6 over-train"" which is not backed up by a single reference. Unless the authors relate it to regularization (text preceding ""overtrain""), which then has no connection to the description of ""over-train"" provided by the authors (""training a network on a harder problem to improve generalization""). If ""harder problem"" means one where one adds an additional term (i.e. the regularizer), the authors are doing harm to the unexperienced reader, confusing ""regularization"" with something that sounds like ""overfitting"" (i.e. the exact opposite).----------------Furthermore, the extensions proposed in Section 4 seem a bit off tune - in particular I could not figure out ---------how the Taylor Series networks stem from any of the design patterns proposed in the rest of the paper. ---------whether the text between 4.1 and 4.1.1 is another of the architecture innovations (and if yes, why it is not in the 4.1.2, or 4.1.0) ---------and, most importantly, how these design patterns would be deployed in practice to think of a new network. ----------------To be more concrete, the authors mention that they propose the ""freeze-drop-path"" variant from ""symmetry considerations"" to ""drop-path"". --------Is this an application of the ""increase symmetry"" pattern? How would ""freeze-drop-path"" be more symmetric that ""drop-path""?-------- Can this be expressed concretely, or is it some intuitive guess? If the second, it is not really part of applying a pattern, in my understanding. If the first, this is missing. ------------------------What I would have appreciated more (and would like to see in a revised version) would have been a table of ""design patterns"" on one axis, ""Deep network"" on another, and a breakdown of which network applies which design pattern. ----------------A big part of the previous work is also covered in cryptic language - some minimal explanation of what is taking place in the alternative works would be useful. ------------------------  ",The authors agree with the reviewers that this manuscript is not yet ready.,"The authors take on the task of figuring out a set of design patterns for current deep architectures. I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors take on the task of figuring out a set of design patterns for current deep architectures. I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do"," The authors take on the task of figuring out a set of design patterns for current deep architectures . However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet",", but I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. ------------First, I am not too sure how",0.25000000000000006,0.07407407407407407,0.25000000000000006,0.8524028062820435,0.8883665800094604,0.8700131773948669
https://openreview.net/forum?id=r10FA8Kxg,"Description.--------This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. ----------------Strong  points.--------- The experiments are carefully done with thorough selection of hyperparameters. --------- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).--------- The paper is well and clearly written.----------------Weak points:--------- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?----------------Originality:--------- This is mainly an experimental paper, but the question it asks is interesting and worth investigation. The experimental results are solid and provide new insights.----------------Quality:--------- The experiments are well done.----------------Clarity:--------- The paper is well written and clear.----------------Significance:--------- The results go against some of the conclusions from previous work, so should be published and discussed.----------------Overall:--------Experimental paper with interesting results. Well written. Solid experiments. ",The reviewers unanimously recommend accepting this paper.,This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset. The results show that similar accuracy on,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset. The results show that similar accuracy on, The paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy . The experiments are performed on the CIFAR 10 dataset . The results show that similar accuracy on,.----------This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep,0.125,0.04347826086956522,0.08333333333333334,0.8187601566314697,0.8601468205451965,0.8389434218406677
https://openreview.net/forum?id=r10FA8Kxg,"This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. --------To this end the authors conducted a series of experiments on the CIFAR10 dataset.--------They find that there is a significant performance gap between the two approaches, in favour of deep CNNs. --------The experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.--------They also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.----------------My take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The results are therefore perhaps not quite surprising, but not completely obvious either.----------------An interesting point on which the authors comment only very briefly is that among the non-convolutional architectures the ones using 2 or 3 hidden layers outperform those with 1, 4 or 5 hidden layers. Do you have an interpretation / hypothesis of why this is the case? It  would be interesting to discuss the point a bit more in the paper.----------------It was not quite clear to me why were the experiments were limited to use  30M parameters at most. None of the experiments in Figure 1 seem to be saturated. Although the performance gap between CNN and MLP is large, I think it would be worthwhile to push the experiment further for the final version of the paper.----------------The authors state in the last paragraph that they expect shallow nets to be relatively worse in an ImageNet classification experiment. --------Could the authors argue why they think this to be the case? --------One could argue that the much larger training dataset size could compensate for shallow and/or non-convolutional choices of the architecture. --------Since MLPs are universal function approximators, one could understand architecture choices as expressions of certain priors over the function space, and in a large-data regimes such priors could be expected to be of lesser importance.--------This issue could for example be examined on ImageNet when varying the amount of training data.--------Also, the much higher resolution of ImageNet images might have a non-trivial impact on the CNN-MLP comparison as compared to the results established on the CIFAR10 dataset.----------------Experiments on a second data set would also help to corroborate the findings, demonstrating to what extent such findings are variable across datasets.",The reviewers unanimously recommend accepting this paper.,This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification. To this end the authors conducted a series of experiments on the CIFAR10 dataset,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification. To this end the authors conducted a series of experiments on the CIFAR10 dataset, This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification . The authors conducted a series of experiments on the CIFAR10 dataset . They find,the experiments were limited to use 30M parameters at most. None of the experiments in Figure 1 seem to be saturated. -----------The authors conclude that shallow non-convolutional networks can be as affective as deep convolution,0.13636363636363638,0.04761904761904762,0.09090909090909091,0.8205289840698242,0.8659086227416992,0.8426082134246826
https://openreview.net/forum?id=ryMxXPFex,"Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). ----------------Overall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:----------------Graham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the 26th International Conference on Machine Learning (ICML), 1025–1032, 2009.----------------Discussion of this should certainly be added. ","The authors present a novel reparameterization framework for VAEs with discrete random variables. The idea is to carry out symmetric projections of the approximate posterior and the prior into a continuous space and evaluating the autoencoder term in that space by marginalizing out the discrete variables. They consider the KL divergence between the approximating posterior and the true prior in the original discrete space and show that due to the symmetry of the projection into the continuous space, it does not  contribute to the KL term.     One question that warrants further investigation is whether this framework can be extended to GANs and what empirical success they would have.    The reviewers have presented a strong case for the acceptance of the paper and I go with their recommendation.",Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds., Paper proposes novel Variational Encoder architecture that contains discrete variables . Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables),is a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual,0.16149068322981366,0.025157232704402514,0.12422360248447203,0.8547844886779785,0.8152716159820557,0.8345606327056885
https://openreview.net/forum?id=ryMxXPFex,"This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. ----------------The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!----------------The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)----------------I’m a bit concerned about the variance of the gradients in the general formulation of the algorithm. See my comment “variance of the derivatives of F^{-1}” below. I think the response is convincing, but the problem (as well as “engineering principles” for the smoothing distribution) are probably worth pointing out in the paper itself, since the problem seems likely to occur unless the user is aware of it. (E.g., my proposal of widely separated normals would be a natural distribution to consider until one actually works through the gradients — something not commonly done in the age of autodiff frameworks.)----------------Another concern is how many sequential operations are needed for inference in the RBM model. (Note: is this actually an RBM, or a general Boltzmann machine?) The q distribution takes the form of an autoregressive model where the variables are processed one at a time. Section 3 mentions the possibility of grouping together variables in the q distribution, and this is elaborated in detail in Appendix A. But the solution requires decomposing the joint into a product of conditionals and applying the CDFs sequentially. So either way, it seems like we’re stuck handling all the variables sequentially, which might get expensive. ----------------Minor: the second paragraph of Section 3 needs a reference to Appendix A.","The authors present a novel reparameterization framework for VAEs with discrete random variables. The idea is to carry out symmetric projections of the approximate posterior and the prior into a continuous space and evaluating the autoencoder term in that space by marginalizing out the discrete variables. They consider the KL divergence between the approximating posterior and the true prior in the original discrete space and show that due to the symmetry of the projection into the continuous space, it does not  contribute to the KL term.     One question that warrants further investigation is whether this framework can be extended to GANs and what empirical success they would have.    The reviewers have presented a strong case for the acceptance of the paper and I go with their recommendation.","This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the"," This paper presents a way of training deep generative models with discrete hidden hidden variables using the reparameterization trick . It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-",". The paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves",0.21951219512195125,0.06172839506172839,0.17073170731707318,0.870796799659729,0.8251808881759644,0.8473753929138184
https://openreview.net/forum?id=ryMxXPFex,"This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). --------A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. --------The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.--------Overall, the paper is very rich with ideas so I think it would be a great contribution to the conference. ","The authors present a novel reparameterization framework for VAEs with discrete random variables. The idea is to carry out symmetric projections of the approximate posterior and the prior into a continuous space and evaluating the autoencoder term in that space by marginalizing out the discrete variables. They consider the KL divergence between the approximating posterior and the true prior in the original discrete space and show that due to the symmetry of the projection into the continuous space, it does not  contribute to the KL term.     One question that warrants further investigation is whether this framework can be extended to GANs and what empirical success they would have.    The reviewers have presented a strong case for the acceptance of the paper and I go with their recommendation.",This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a, The paper is very rich with ideas so I think it would be a great contribution to the conference . The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and,". --------A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. ------------Overall, the paper is very",0.2603550295857988,0.03592814371257485,0.14201183431952663,0.8693985342979431,0.8204393982887268,0.8442096710205078
https://openreview.net/forum?id=BJAA4wKxg,"The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.----------------Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).----------------The experimental results are well reported in detail.----------------One or two figures would definitely be required to help clarify the architecture.----------------This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in order to get the good results that they do on the reported NMT tasks. In this respect, while I am fairly confident that the paper represents good work in machine learning, I am not quite as confident about its fit for this particular conference.","This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be ""incremental"".     Pros:  - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work  - Quality: The experimental results were thorough, ""very extensive and leaves no doubt that the proposed approach works well"".    Mixed:  - Novelty: There is appreciation that the work is novel. However as the work is somewhat ""application-specific"" the reviewers felt the technical contribution was not an overwhelming contribution.  - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or ""main speed-up factor(s)"" were.     This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.","The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. The use of CNN's for translation has been attempted previously (as described by the authors)",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. The use of CNN's for translation has been attempted previously (as described by the authors)"," The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable . The use of CNN's for translation has been attempted previously .","for NMT, and CNN's are naturally parallelizable.----------------Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation,",0.13793103448275865,0.023255813953488375,0.10344827586206896,0.8522167205810547,0.7990124821662903,0.8247575163841248
https://openreview.net/forum?id=BJAA4wKxg,"This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.----------------One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as well (it does take some effort to understand the last paragraph in Section 2, especially the part on having a linear layer to compute z).","This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be ""incremental"".     Pros:  - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work  - Quality: The experimental results were thorough, ""very extensive and leaves no doubt that the proposed approach works well"".    Mixed:  - Novelty: There is appreciation that the work is novel. However as the work is somewhat ""application-specific"" the reviewers felt the technical contribution was not an overwhelming contribution.  - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or ""main speed-up factor(s)"" were.     This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.",This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare, This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs . The authors present a convincing set of results over many translation tasks and compare,is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and,0.17977528089887637,0.011363636363636366,0.10112359550561797,0.8576037287712097,0.8096082210540771,0.8329150676727295
https://openreview.net/forum?id=BJAA4wKxg,"The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. ----------------Apart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.--------The empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. ----------------The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It’s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. ----------------My main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ","This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be ""incremental"".     Pros:  - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work  - Quality: The experimental results were thorough, ""very extensive and leaves no doubt that the proposed approach works well"".    Mixed:  - Novelty: There is appreciation that the work is novel. However as the work is somewhat ""application-specific"" the reviewers felt the technical contribution was not an overwhelming contribution.  - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or ""main speed-up factor(s)"" were.     This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area.",The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well, The paper reports a result that a convolutional network can be used instead of the recurrent encoder for neural machine translation . The paper features one unexpected architectural twist: two stacks of convolutions .,"was faster at evaluation, but it is not very clear what is the main speed-up factor. ------------The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. ------------",0.26229508196721313,0.13259668508287292,0.21857923497267756,0.8829922676086426,0.8178205490112305,0.8491578102111816
https://openreview.net/forum?id=r1xUYDYgg,"Validity:--------The presented work seems technically valid. Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.----------------https://mil-tokyo.github.io/sukiyaki2/examples/mnist/ was fun, but seemed very slow (5 mnist images per second). The demo page would be more interesting if it showed what model was being trained, which implementation was being used (pure js or webcl?), which hardware was being used for the computation, and how that compared with other people who logged into the page. As it is, the demo is kind of unclear as to what is happening.----------------Relevance:----------------The grand vision of a DLTraining@Home is exciting. While much work remains, having a solid WebCL foundation seems valuable. The big advantage of javascript is that it runs everywhere, especially on idle desktops and laptops around the world. However, these sorts of computers do not (with probability 1) have K80 or S9120 video cards. Instead, they have a wide variety of every consumer-grade card ever sold, which call for different blocking, tiling, and looping strategies in the computational kernels that underpin deep learning inference and training algorithms (hence, autotuning), which isn't discussed.----------------Sushi2 and Sukiyaki2 seem relatively young as projects. They are not widely followed on github, there is no tutorial-style documentation for Sukiyaki2, and the implementations of e.g. convolution do not seem to have seen much engineering work. Speed of evaluation seems to be one of the main focal points of the paper, but it’s not a major selling point to the ICLR audience because it seems about ¼ as fast as e.g. cuDNN on standard (e.g. AWS nodes) NVidia hardware. The performance of sukiyaki2 vs AMD's Caffe port is impressive.----------------Benchmarking on high-end compute server hardware is an interesting point of reference, but the questions that come to mind for me when reading this paper are----------------(1) How would this fit into a live-video processing application on a mobile device--------(2) What kind of a “cluster” would this present to someone trying to do distributed deep learning in the wild by drawing on idle graphics cards: how much memory do they have, how might we handle data for training on such computers, what is the compute speed vs. communication latency and bandwidth.----------------Answers to these questions are out of scope for this paper, but it would have been interesting to see at least some preliminary discussion.----------------Novelty:--------I’m not aware of a more mature WebCL-based HPC library.----------------Presentation:--------Table 1 is hard to read because it is actually two tables with different formatting, and the numbers (speeds?) aren’t labeled with units.","In the § Related work: ""deeplearning4j 2 provides distributed computing of deep learning framework that runs on the distributed computing Hadoop. However, Hadoop must be installed in all computing nodes, thereby imposing high deployment and maintenance costs.""  This is inexact, Deeplearning4j's most basic mode of operation is on a single machine, with Java installed. A GPU is used if available but is not a requirement (Deeplearning4j documentation: https://deeplearning4j.org/quickstart )",The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable. The performance of sukiyaki2 vs AMD's Caffe port is impressive.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable. The performance of sukiyaki2 vs AMD's Caffe port is impressive.," Code for matrix library sushi2 and DL library sukiyaki2 are on GitHub, including live demos that run in your browser . Demo page would be more interesting if it showed what model was being trained, which implementation was being used",.-------------------------------------------------------The grand vision of a DLTraining@Home is exciting.-------------------------------------,0.1836734693877551,0.0,0.1020408163265306,0.8254618048667908,0.8112456798553467,0.8182920217514038
https://openreview.net/forum?id=r1xUYDYgg,"While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. --------There is no new algorithm here, nor is there any UI/meta-design improvement to make it easier for non-experts to design and train neural network systems. ----------------I think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. --------I have no significant objection to the presentation or methodology of the paper. ","In the § Related work: ""deeplearning4j 2 provides distributed computing of deep learning framework that runs on the distributed computing Hadoop. However, Hadoop must be installed in all computing nodes, thereby imposing high deployment and maintenance costs.""  This is inexact, Deeplearning4j's most basic mode of operation is on a single machine, with Java installed. A GPU is used if available but is not a requirement (Deeplearning4j documentation: https://deeplearning4j.org/quickstart )","While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit."," While it is interesting that this can be done, it does seem like the audience is not really the mainstream ICLR audience . There is no new algorithm here, nor is there any UI/meta-design improvement to make it easier",and train neural network systems. -----------I think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. --------I have no significant objection to,0.19819819819819817,0.01834862385321101,0.12612612612612611,0.8333754539489746,0.8132319450378418,0.8231804370880127
https://openreview.net/forum?id=r1xUYDYgg,"This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs. While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.----------------My main points of criticism are:--------1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).----------------2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many ""low performance"" clients, the setting of having a few dedicated high performance servers is quite likely. Even if not, these are good values to compare with. For the sake of consistency, please include in both subfigures Firefox, Chrome, node.js.----------------Apart from these points, well-written, understandable and conclusive.","In the § Related work: ""deeplearning4j 2 provides distributed computing of deep learning framework that runs on the distributed computing Hadoop. However, Hadoop must be installed in all computing nodes, thereby imposing high deployment and maintenance costs.""  This is inexact, Deeplearning4j's most basic mode of operation is on a single machine, with Java installed. A GPU is used if available but is not a requirement (Deeplearning4j documentation: https://deeplearning4j.org/quickstart )",This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology.," This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks . It is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs .","for deploying deep neural networks.----------------2. In Fig. 6, why not include more information in the graphs?---------------My main points of criticism are:--------1. In Tab",0.19801980198019803,0.0,0.07920792079207921,0.8645895719528198,0.8240986466407776,0.8438585996627808
https://openreview.net/forum?id=rJzaDdYxx,"This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.----------------While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-review question articulated this same concern and has not been answered.","This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript.","This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The paper lacks both qualitative and quantitative comparison to prior work. I do not believe it is fit for publication without such comparisons.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The paper lacks both qualitative and quantitative comparison to prior work. I do not believe it is fit for publication without such comparisons."," This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks . The interior gradient is the gradient measured on a scaled version of the input .",". The paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The integrated gradient is the integral of interior gradients over all scaling factors. The integrated gradient is the integral of interior gradient",0.24,0.020408163265306124,0.13999999999999999,0.8699337244033813,0.8595497012138367,0.8647105693817139
https://openreview.net/forum?id=rJzaDdYxx,"The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images “counterfactuals” which seems like quite an unnecessarily grandiose name for literally, a scaled image. ----------------The authors show a number of visualizations that indicate that the proposed feature importance score is more reasonable than just looking at gradients only with respect to the original image. They also show some quantitative evidence that the pixels highlighted by the proposed measure are more likely to fall on the objects rather than spurious parts of the image (in particular, see figure 5). The method is also applied to other types of networks. The quantitative evidence is quite limited and most of the paper is spent on qualitative results.----------------While the goal of understanding deep networks is of key importance, it is not clear whether this paper really help elucidate much. The main interesting observation in this paper is that scaling an image by a small alpha (i.e. creating a faint image) places more “importance” on pixels on the object related to the correct class prediction. Beyond that, the paper builds a bit on this, but no deeper insight is gained. The authors propose some hand-wavy explanation of why using small alpha (faint image) may force the network to focus on the object, but the argument is not convincing. It would have been interesting to try to probe a bit deeper here, but that may not be easy.----------------Ultimately, it is not clear how the proposed scheme for feature importance ranking is useful. First, it is still quite noisy and does not truly help understand what a deep net is doing on a particular image. Performing a single gradient descent step on an image (or on the collection of scaled versions of the image) hardly begins to probe the internal workings of a network. Moreover, as the authors admit, the scheme makes the assumption that each pixel is independent, which is clearly false.----------------Considering the paper presents a very simple idea, it is far too long. The main paper is 14 pages, up to 19 with references and appendix. In general the writing is long-winded and overly verbose. It detracted substantially from the paper. The authors also define unnecessary terminology. “Gradients of Coutnerfactuals” sounds quite fancy, but is not very related to the ideas explored in the writing. I would encourage the authors to tighten up the writing and figures down to a more readable page length, and to more clearly spell out the ideas explored early on.","This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript.","The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The quantitative evidence is quite limited and most of the paper is spent on qualitative results.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The quantitative evidence is quite limited and most of the paper is spent on qualitative results."," The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image . The quantitative evidence is quite limited and most of the paper is spent on qualitative results .",". The authors propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I. This assigns a score to each pixel in I (that ranks",0.28571428571428575,0.04166666666666667,0.16326530612244897,0.8605396151542664,0.8585025072097778,0.859519898891449
https://openreview.net/forum?id=rJzaDdYxx,"This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. ----------------One particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?----------------With regards to the interior gradients, it is unclear how the scaling parameter \alpha affects the feature importance and how it is related to attention.----------------Finally, does this model use batch normalization?","This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript.","This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however the approach is very ad hoc.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however the approach is very ad hoc."," This work proposes to use visualization of gradients to further understand the importance of features for visual classification . Overall, this presented visualizations are interesting, however, the approach is very ad hoc .","for visual classification. ------------------------One particular question with regular gradients at features that form the spatial support of the visual class.----------------Finally, does this model use batch normal",0.27368421052631575,0.021505376344086023,0.1473684210526316,0.8558951020240784,0.8498138189315796,0.8528435826301575
https://openreview.net/forum?id=H1kjdOYlx,"The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that specifying tasks in simplified language is easy, designing a curriculum may in fact be pretty complicated, depending on the task at hand.  The examples provided are fairly small, and there is no hint of how curriculum can be designed for larger problems. Because the approach is sensitive to the curriculum, this limits the potential utility of the work. It is also unclear if there is a way to provide supervision automatically, instead of doing it based on prior domain knowledge.--------More minor comments:--------- The experiments are not described in enough detail in the paper. It's great to provide github code, but one needs to explain in the paper why certain choices were made in the task setup (were these optimized? What's this the first thing that worked?) Even with the code, the experiments as described are not reproducible--------- The description of the approach is pretty tangled with the specific algorithmic choices. Can the authors step back and think more generally of how this approach can be formalized?  I think this would help relate it to the prior work more clearly as well.","As per all the reviews, the work is clearly promising, but is seen to need additional discussion / formalization / experimental comparison with related work, and stronger demonstrations of the application of this technique.  Further back-and-forth with the reviewers would have been useful, but there should be enough to go on in terms of directions. This work would benefit from being part of the workshop track.","The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting."," The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods . The idea of providing symbolic descriptions of tasks and learning corresponding ""implementations"" is potentially interesting and the",. The approach presented relies crucially on curriculum learning (this is quite clear from the experiments) The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL. The approach is illustrated,0.19607843137254904,0.0,0.11764705882352942,0.8395028710365295,0.8302250504493713,0.8348381519317627
https://openreview.net/forum?id=H1kjdOYlx,"This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. ----------------Sketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. ----------------Experiments are provided through a standard game like domain (maze, minecraft etc.). ----------------The paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the context of the specific approach proposed here. Second, the experiments are not thorough enough in terms of comparing with all the related work. For example, Ghavamzadeh et al. explored the use of MAXQ like abstractions in the context of mulitagent RL. It would be great to get a more detailed comparison to MAXQ based multiagent RL approaches, where the value function is explicitly decomposed. ","As per all the reviews, the work is clearly promising, but is seen to need additional discussion / formalization / experimental comparison with related work, and stronger demonstrations of the application of this technique.  Further back-and-forth with the reviewers would have been useful, but there should be enough to go on in terms of directions. This work would benefit from being part of the workshop track.","This paper studies the problem of abstract hierarchical multiagent RL with policy sketches. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper studies the problem of abstract hierarchical multiagent RL with policy sketches. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work."," This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions . It would have been useful to see this spelled out in the context of SMDP models .",", but not sufficiently fleshed out to have any real impact. ------------The paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have",0.29411764705882354,0.04,0.19607843137254904,0.8513621091842651,0.8325307965278625,0.8418411016464233
https://openreview.net/forum?id=H1kjdOYlx,"The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. ----------------The paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant. So since the model proposed in this article is not a major contribution and shares many common ideas with existing hierarchical reinforcement learning methods,  the paper lacks a strong motivation and/or concrete application. So, the paper only has a marginal interest for the RL community----------------@pros: --------* Original problem with well design experiments--------* Simple adaptation of the actor-critic method to the problem of learning sub policies------------------------@cons:--------* Very simple task that can be seen as a simplification of more complex problems like options discovery, hierarchical RL or learning from instructions--------* No strong underlying applications that could help to 'reinforce' the interest of the approach","As per all the reviews, the work is clearly promising, but is seen to need additional discussion / formalization / experimental comparison with related work, and stronger demonstrations of the application of this technique.  Further back-and-forth with the reviewers would have been useful, but there should be enough to go on in terms of directions. This work would benefit from being part of the workshop track.",The paper proposes a new RL architecture that aims at learning policies from sketches. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch. The learning algorithm is based on an,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes a new RL architecture that aims at learning policies from sketches. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch. The learning algorithm is based on an, The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task . The model relies on a hierarchical structure where the sub-policy is chosen depending on,. The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarch,0.2,0.0,0.1272727272727273,0.8336523771286011,0.8153817653656006,0.8244158625602722
https://openreview.net/forum?id=Hy3_KuYxg,"I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it's still missing. After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning.----------------As an example of the difficulty, take section 4.3. It talks about training ""split block"" which is a function that can assign each element to either partition 0 or partition 1. At this point I'm looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as ""we must create artificial targets at every node of the generated tree from the available final target partition"". What are these artificial targets, and how do they relate to the problem of training the splitter? An example that explicitly goes through this construction would help with understanding.","The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13).","I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it's still missing. After going over this paper couple",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","I was holding off on this review hoping to get the missing details from the code at https://github.com/alexnowakvilla/DP, but at this time it's still missing. After going over this paper couple", After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments . I think this would be a common problem for readers of this paper . An example that explicitly goes through this construction would help with understanding .,", but at this time it's still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example",0.25,0.02127659574468085,0.14583333333333334,0.8256998658180237,0.845258355140686,0.8353646397590637
https://openreview.net/forum?id=Hy3_KuYxg,"The basic idea of this contribution is very nice and worth pursuing: how to use the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.----------------My main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of supervision at lower recurrency levels) one can get a partially trainable sorter or convex hull solver.----------------I found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear either how the models operate, nor how much of them was actually learned, and how much was designed. The answer to the questions did hep, nut didn't make it into the paper. Mixing the descriptions of the tricks required to solve the two tasks makes things even more confusing. I believe that the paper would be much more accessible if instead of promising a general solution it clearly stated the challenges faced by the authors and the possible solutions.----------------Highlights:--------+ Proof-of-concept of a partially-trainable implementation of the important “divide and conquer” paradigm--------++ Explicit reasoning about complexity of induced programs--------- The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks specific to each problem--------- The writing style pictures the method as very general, but falls back on very low level details specific to each task","The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13).","The basic idea of this contribution is very nice and worth pursuing. The execution of this idea is not convincing and needs polishing before acceptance. The writing style pictures the method as very general, but falls back on very low level details specific",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The basic idea of this contribution is very nice and worth pursuing. The execution of this idea is not convincing and needs polishing before acceptance. The writing style pictures the method as very general, but falls back on very low level details specific", The basic idea of this contribution is very nice and worth pursuing: how to use the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull . The solution,".-------------I found the contribution relatively hard to understand. High level ideas are mixed with low-level tricks required to get the model to work and it is not clear how the models operate, nor how much was designed ",0.23300970873786406,0.039603960396039604,0.17475728155339806,0.8599840402603149,0.848007321357727,0.8539537191390991
https://openreview.net/forum?id=Hy3_KuYxg,"I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.----------------The paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.----------------Instead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.----------------Both split and merge phases seem to make a lot of discrete choices in a hierarchical manner during training. The paper does not explain how those discrete choices are backpropagated through the network in an unbiased manner, if that is the case at all.----------------In general, the direction this paper is exciting, but the paper itself is a frustrating read in its present form. I have spent several hours on it without having to manage to achieve a clear mental image on how all the presented pieces fit together. I would revise my score if the paper would be improved greatly from a readability perspective, but I think it would require a major rewrite.","The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13).",I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures. I would revise my score if the paper would be improved greatly from a readability perspective.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures. I would revise my score if the paper would be improved greatly from a readability perspective.," The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge . However, the general research direction is fruitful and important .",".----------------The paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random, hard to parse order.----------------",0.2040816326530612,0.04166666666666667,0.14285714285714285,0.8711021542549133,0.8497653007507324,0.8603014945983887
https://openreview.net/forum?id=ryhqQFKgl,"After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper.----------------Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity.----------------I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion..). I get the sense that a long-form journal publication would actually give the authors the space necessary to fully explain these ideas, provide clearer running examples where needed, provide the necessary background for the appropriate readership, provide the necessary background on the previous system, perhaps demonstrating results on a second dataset to show generality of the approach, etc. A short conference paper just seems to me to be too dense a format for giving this project the description it merits. If it were possible to focus on just one aspect of this system, then that might work, but I do not have good suggestions for exactly how to do that. ----------------If the paper were revised substantially (though I cannot suggest details for how to do this within the appropriate page count), I would consider raising my score. I do think that the effort would be better invested in turning this into a long (and clearer) journal submission.----------------[Addendum: based on discussions here & revisions, I have revised my score]","Given that all reviewers were positive aobut this paper and given the unusual application domain, we recommend to accept this paper for poster presentation at the main conference.","After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other"," I did find the paper quite hard to follow at various parts . The extensive use of notation did not help the clarity . I think the ideas and approaches are good, and certainly worth publishing and worth pursuing .",".----------------Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I",0.2727272727272727,0.03125,0.15151515151515152,0.8427640795707703,0.8457887172698975,0.8442736268043518
https://openreview.net/forum?id=ryhqQFKgl,"Summary: --------The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and ""satisfactory gaps"" have been generated in terms of sets of learned rules. ----------------1. Quality:-------- a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and interpretable. The authors also demonstrate an effective memory selection to speed up the learning.---------------- b) Flaws: The paper only discussed N<=5, which might limit the learning and interpretation capacities of the proposed model, failing to capture long-distance dependence of music. (In the replies to questions, the authors mentioned they had experimented with max N=10, but I'm not sure why related results were not included in the paper). Besides the elaborated interpretation of results, a survey seeking the opinions of students in a music department might make the evaluation of system performance more persuasive.----------------2. Clarity:-------- a) Pros: The paper clearly delivers an improved automatic theorist system which learns and represents music concepts as well as thoroughly interprets and compares the learned rules with music theory. Proper analogies and examples help the reader perceive the ideas more easily.---------------- b) Cons: Although detailed definitions can be found in the authors' previous MUS-ROVER I papers, it would be great if they had described the optimization more clearly (in Figure 1. and related parts).  The ""(Conceptual-Hierarchy Filter)"" row in equations (3): the prime symbol should appear in the subscript.----------------3. Originality:--------The representation of music concepts and rules is still an open area, the paper investigate the topic in a novel way. It illustrates an alternative besides other interpretable feature learning methods such as autoencoders, GAN, etc. ----------------4. Significance:--------It is good to see some corresponding interpretations for the learned rules from music theory. The authors mentioned students in music could and should be involved in the self-learning loop to interact, which is very interesting. I hope their advantages can be combined in the practice of music theory teaching and learning.","Given that all reviewers were positive aobut this paper and given the unusual application domain, we recommend to accept this paper for poster presentation at the main conference.",The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. Sonority (column of 4 MIDI numbers) acts as word in language model. Unigram statistics have been used to learn the,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. Sonority (column of 4 MIDI numbers) acts as word in language model. Unigram statistics have been used to learn the, The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales . It extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory,: The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales ,0.1875,0.0,0.125,0.816231906414032,0.8284325003623962,0.8222869634628296
https://openreview.net/forum?id=ryhqQFKgl,"This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach’s chorales) and propose new rules for the student to learn so that it could improve.----------------The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple baselines, e.g., an LSTM generative model. This might pose a concern here. ----------------I found this paper a bit hard to read, partly due to (1) lots of music terms (e.g, Tbl. 1 does not make sense to me) that hinders understanding of how the system performs, and (2) over-complicated math symbols and concept. For example, In Page 4, the concept of raw/high-level feature, Feature-Induced Partition and Conceptual Hierarchy, all means a non-overlapping hierarchical clustering on the 4-dimensional feature space. Also, there seems to be no hierarchy in Informational Hierarchy, but a list of rules. It would be much clearer if the authors write the paper in a plain way. ----------------Overall, the paper proposes a working system that seems to be interesting. But I am not confident enough to give strong conclusions.","Given that all reviewers were positive aobut this paper and given the unusual application domain, we recommend to accept this paper for poster presentation at the main conference.","This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a"," This paper proposes an interesting framework to learn compositional rules used to compose better music . The system consists of two components, a generative component (student) and a discriminative component . The teacher compares the generated music with the empirical","generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the",0.2121212121212121,0.03125,0.15151515151515152,0.8316815495491028,0.8377200961112976,0.8346899151802063
https://openreview.net/forum?id=Sy1rwtKxg,"This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.----------------I feel that there might be some fundamental misunderstanding on SGD.----------------''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O(f^2) . In practice, this would mean that we would need O(f) processors to see constant speedups, an infeasible proposition particularly for datasets that can have--------thousands if not millions of features.""----------------I do not think one needs O(f^2) space and complexity for updating M_i * v, where v is an f-dimensional vector. Note that M_i is a low rank matrix in the form of (I - a_i a_i'). The complexity and space can be reduced to O(f) if compute it by O(v - a_i (a_i' v)) equivalently. If M_i is defined in the form of the product of n number of rank 1 matrices. The complexity and space complexity is O(fn). In the context of this paper, n should be much smaller than f. I seriously doubt that all author's assumptions, experiments, and strategies in this paper are based on this incorrect assumption on space and complexity of SGD. ----------------Why one can have speedup is unclear for me. It is unclear what computations are in parallel and why this sequential algorithms can bring speedup if M_i*v is computed in the most efficient way.--------I suggest authors to make the following changes to make this paper more clear and theoretically solid--------- provide computational complexity per step of the proposed algorithm--------- convergence rate analysis (convergence analysis is not enough): we would like to see how the dimension reduction can affect the complexity.","The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.   The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.",This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and All,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and All, This paper proposes a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations . The space and time complexity of parallel SGD is O(f^2) .,"is O(f2) space and complexity for updating M_i * v, where f is the number of features. The complexity and space complexity of parallel SGD is O(f2). In practice, this",0.1827956989247312,0.05434782608695653,0.12903225806451613,0.8459082841873169,0.8149632811546326,0.8301475048065186
https://openreview.net/forum?id=Sy1rwtKxg,"Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:----------------- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. ----------------- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). ----------------- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the paper can be solved in a few second using a single core CPU. Hogwild! is suitable for sparse dataset because of its asynchronized nature. On data that are very sparse, the proposed approach is only slightly better or is worse than Hogwild. For dense dataset, it is unclear why we need to use SYMSGD instead of simply parallelizing the gradient computation using GPUs. Put them together, the experiment results are not convincing. ","The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.   The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.","The idea in this paper is interesting and the paper is well-written and well-motivated. However, I think it is not ready to publish in ICLR for the following reasons. This paper is not related to representation learning",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The idea in this paper is interesting and the paper is well-written and well-motivated. However, I think it is not ready to publish in ICLR for the following reasons. This paper is not related to representation learning"," The idea in this paper is interesting and the paper is well-written and well-motivated . However, I think it is not ready to publish in ICLR for the following reasons . This paper is not related to representation learning",. ----------------- The idea in this paper is interesting and the paper is well-written and well-motivated. I think it is not ready to publish in ICLR for following reasons:-------------,0.2553191489361702,0.07526881720430108,0.15957446808510636,0.8867155313491821,0.8119727373123169,0.847699761390686
https://openreview.net/forum?id=Sy1rwtKxg,"This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.--------Comments--------1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.--------2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.--------3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.----------------Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the paper could be of interest to people scaling up learning. I would encourage the author to extend the method to the cases of non-linear objective function which could make it more interesting to the ICLR community","The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.   The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community.",This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique. I would encourage the author to extend the method to the cases of non-linear objective function.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique. I would encourage the author to extend the method to the cases of non-linear objective function., This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique . The resulting technique requires book-keeping of a dimensional reduced combiner matrix .,.----------Comments--------1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.-------------1) The proposed method is novel and interesting to allow update,0.2043010752688172,0.05434782608695653,0.15053763440860218,0.8703280091285706,0.8161266446113586,0.8423563241958618
https://openreview.net/forum?id=Sywh5KYex,"This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.----------------It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC still outperforms with 15.3M parameters which is much less that 36.5M). The Res Net variants papers with state of art results report result for Image Net. Therefore the empirical results need also the Image Net to demonstrate that improvement claimed is achieved.----------------The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.","Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.",The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach., This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate . The authors state that they obtained the best results on CIFAR-10 and C,.-------------------The proposed trick adopts Highway Neural Networks and Residual Networks with an intuitive motivation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach,0.21333333333333332,0.0,0.13333333333333336,0.8579563498497009,0.8489083051681519,0.8534083366394043
https://openreview.net/forum?id=Sywh5KYex,"This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.----------------The basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily ""shutting off"" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.----------------Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual layer? Based on the arguments made in the paper, this should work fine. Why wasn't it tested? If it doesn't work, are the arguments incorrect or incomplete?----------------For the MNIST experiments, since the hyperparameters are fixed, the plots are misleading if any dependence on hyperparameters exists for the different models. This experiment appears to be based on Srivastava et al (2015). If it is indeed designed to test optimization at aggressive depths, then apart from doing a hyperparameter search, the authors should not use regularization such as dropout or batch norm, which do not appear in the theoretical arguments for the architecture.----------------For CIFAR experiments, the obtained improvements compared to the baseline (wide resnets) are very small and therefore it is important to report the standard deviations (or all results) in both cases. It's not clear that the differences are significant.----------------Some questions regarding g(): Was g() always ReLU? Doesn't this have potential problems with g(k) becoming 0 and never recovering? Does this also mean that for the wide resnet in Fig 7, most residual blocks are zeroed out since k < 0?","Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.",This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation., This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks . The claim is that such gating is easier to learn and allows a network to flexibly utilize computation .,gating is easier to learn and allows a network to flexibly utilize computation.-------------Did the authors try their original design of u = g(k)f(x) + (1,0.225,0.0,0.17500000000000002,0.8315122723579407,0.8434914946556091,0.8374590277671814
https://openreview.net/forum?id=Sywh5KYex,"The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). ----------------Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. ----------------Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments.","Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance.",The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. Such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. Such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers, The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it . It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity,is shown that such an architecture can ease optimization of a deep network as it is easy learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). ---------------Although good,0.25287356321839083,0.047058823529411764,0.1379310344827586,0.8365753293037415,0.844420313835144,0.8404794931411743
https://openreview.net/forum?id=HyET6tYex,"The authors explore whether the halting time distributions for various algorithms in various settings exhibit ""universality"", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.----------------The idea of the described universality is very interesting. However I see several shortcomings in the paper:----------------In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.----------------Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a sensible initialization to cut of part of the right tail of the distribution.----------------Additionally, I found the paper quite hard to read. Here are some clarity issues:----------------- abstract: ""even when the input is changed drastically"": From the abstract I'm not sure what ""input"" refers to, here--------- I. Introduction: ""where the stopping condition is, essentially, the time to find the minimum"": this doesn't seem to make sense, a condition is not a time. I guess the authors wanted to say that the stopping condition is that the minimum has been reached?--------- I.1 the notions of dimension N, epsilon and ensemble E are introduced without any clarification what they are. From the later parts of the paper I got some ideas and examples, but here it is very hard to understand what these parameters should be (just some examples would be already helpful)--------- I.3 ""We use x^\ell for \ell \in Z=\{1, \dots, S\} where Z is a random sample from of training samples"" This formulation doesn't make sense. Either Z is a random sample, or Z={1, ..., S}.--------- II.1 it took me a long time to find the meaning of M. As this parameter seems to be crucial for universality in this case, it would be very helpful to point out more explicitly what it refers to.",My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.,"The authors explore whether the halting time distributions for various algorithms in various settings exhibit ""universality"" After rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The authors explore whether the halting time distributions for various algorithms in various settings exhibit ""universality"" After rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble."," The authors explore whether halting time distributions for various algorithms in various settings exhibit ""universality"" after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameters, dimensionality and ensemble .",": ""where the stopping condition is, essentially, the time to find the minimum"": This doesn't seem to make sense, a condition is not a time.---------- I.1 the notions of",0.14285714285714285,0.0,0.11428571428571428,0.8013151288032532,0.8436679840087891,0.821946382522583
https://openreview.net/forum?id=HyET6tYex,"Summary------------------------For several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).----------------An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)----------------The authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.----------------A moment-based indicator is introduced to assess whether universality is observed.------------------------Review------------------------This paper presents several problems.--------------------------------page 2: “[…] for sufficiently large N and eps = eps(N)”----------------The dependence of epsilon on N is troubling.--------------------------------page 3: “Universality is a measure of stability in an algorithm […] For example […] halting time for the power method […] has infinite expectation and hence this type of universality is *not* present. One could use this to conclude that the power method is naive. Therefore the presence of universality is a desirable feature of a numerical method”----------------No. An algorithm is naive if there are better ways to answer the problem. One could not conclude from a halting time with infinite expectation (e.g. solving a problem extremely quickly 99% of the time, and looping forever in 1% of cases) or infinite variance, that the algorithm is naive. --------Moreover the universal property is more restrictive than having a finite halting time expectation. Even if in many specific cases, having a finite halting time expectation is a desirable property, showing that the presence of universality is desirable would require a demonstration that the other more restrictive aspects are also desirable. --------Also, the paragraph only concerns one algorithm. why would the conclusions generalise to all numerical methods ? --------Even if the universality property is arguably desirable (i.e. event if the conclusion of this paragraph is assumed correct), the paragraph does not support the given conclusion.--------------------------------Comparing Eq 1 and figures 2,3,4,5 --------From Eq 1, universality means that the centered/scaled halting time fluctuations (which depend on A, epsilon, N, E) can be approximated by a distribution that only depends on A (not on epsilon, N, E) but in the experiments only E varies (figures 2,3,4,5). The validity of the approximation with varying epsilon or N is never tested--------------------------------The ensembles/distributions parameter E (on which halting fluctuations should not depend) and the algorithm A (on which halting fluctuations are allowed to depend) are not well defined, especially w.r.t. the common use of the words. In the optimisation setting we are told that the functional form of the landscape function is part of A (in answer to the question of a reviewer) but what is part of the functional form ? what about computations where the landscape has no known functional form (black box) ?--------------------------------The conclusion claims that the paper “attempts to exhibit cases” where one can answer 5 questions in a robust and quantitative way.----------------Question 1: “What are the conditions on the ensembles and the model that lead to such universality ?” The only quantitative way would be to use the moments based indicator however there is only one example of universality not being observed which concerns only one algorithm (conjugate gradient) and one type of failure (when M = N). This does not demonstrate robustness of the method.----------------Question 2: “What constitutes a good set of hyper parameters for a given algorithm ?”--------The proposed way to choose would be to test whether universality is observed. If it is then the hyper parameters are good, if not the hyper parameters are bad. The correspondance between bad hyper-parameters and observing no universality concerns only one algorithm and one type of failure. Other algorithms may fail in the universal regime or perform well in the non universal regime. The paper does not show how to answer this question in a robust way.----------------Question 3: ""How can we go beyond inspection when tuning a system ? ""--------The question is too vague and general and there is probably no robust and quantitative way to answer it at all.----------------Question 4: ""How can we infer if an algorithm is a good match to the system at hand ? ""--------The paper fails to demonstrate convincingly that universality is either a good or robust way to approach the very few studied algorithms. The suggested generalisation to all systems and algorithms is extremely far fetched.----------------Question 5: ""What is the connection between the universal regime and the structure of the landscape ?""-------- Same as before, the question is extremely vague and cannot be answered in a robust or quantitative way at all. The fact that what corresponds to A and what corresponds to E is not clear does not help.------------------------In the conclusion it is written that the paper validates the claim that universality is present in all or nearly all sensible computation. It does not. The paper does not properly test whether universality is present (only 1 parameter in 3 that should not vary is tested). The paper does not properly test whether universality is lost when the computation is no longer sensible (only one failure case tested). Finally the experiments do not apply to all or nearly all computations but only to very few  specific algorithms.",My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.,"An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm. The authors argue that empirically, the universal property is observed when both",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm. The authors argue that empirically, the universal property is observed when both"," An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on epsilon, an intrinsic measure of dimension N",of the landscape function is part of A (in answer to the question of a reviewer) but what is part of the functional form?-------------------The conclusion claims that the paragraph only concerns one algorithm.,0.2777777777777778,0.02857142857142857,0.19444444444444445,0.8151554465293884,0.8498339056968689,0.8321335315704346
https://openreview.net/forum?id=HyET6tYex,"The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:----------------1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. ----------------2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).----------------3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for algorithm tuning. ----------------At the least, many more empirical comparisons should be provided to convince me that the universality holds across a broad range of algorithms.",My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.,"The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short."," The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one . However, as an empirical study, this paper comes up somewhat short .","of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:----------------1. It would have been more",0.23880597014925375,0.030769230769230767,0.11940298507462688,0.8479688167572021,0.8598921895027161,0.8538888692855835
https://openreview.net/forum?id=B1TTpYKgx,"This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.----------------Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.----------------There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.--------For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.----------------The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.----------------Some findings seem trivial.----------------detailed comments----------------p2 ----------------""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide""----------------I don’t think so. In ""Deep Belief Networks are Compact Universal Approximators"" by Leroux et al., proof is given that deep but narrow feed-forward neural networks with sigmoidal units can represent any Boolean expression i.e. A neural network with 2n−1 + 1 layers of n units (with n the number of input neutron).----------------“Comparing architectures in such a fashion limits the generality of the conclusions”----------------To my knowledge much of the previous work has focused on mathematical proof, and has led to very general conclusions on the representative power of deep networks (one example being Leroux et al again).----------------It is much harder to generalise the approach you propose, based on random networks which are not used in practice.----------------“[we study] a family of networks arising in practice: the behaviour of networks after random initialisation”----------------These networks arise in practice as an intermediate step that is not used to perform computations; this means that the representative power of such intermediate networks is a priori irrelevant. You would need to justify why it is not.----------------“results on random networks provide natural baselines to compare trained networks with”----------------random networks are not “natural” for the study of expressivity of deep networks. It is not clear how the representative power of random networks (what kind of random networks seems an important question here) is linked to the representative power of (i) of the whole class of networks or (ii) the class of networks after training. Those two classes of networks are the ones we would a priori care about and you would need to justify why the study of random networks helps in understanding either (i) or (ii).----------------p5----------------“As FW is a random neural network […] it would suggest that points far enough away from each other would have independent signs, i.e. a direct proportionality between the length of z(n)(t) and the number of times it crosses the decision boundary.”----------------As you say, it seems that proportionality of the two measures depends on the network being random. This seems to invalidate generalisation to other networks, i.e. if the networks are not random, one would assume that path lengths are not proportional.----------------p6----------------the expressivity w.r.t. remaining depth seems a trivial concerns, completely equivalent to the expressivity w.r.t. depth. This makes the remark in figure 5 that the number of achievable dichotomies only depends *only* on the number of layers above the layer swept seem trivial----------------p7----------------in figure 6 a network width of 100 for MNIST seems much too small. Accordingly performance is very poor and it is difficult to generalise the results to relevant situations.","While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.",This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context," This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks . Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions","of deep networks are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.-------------The paper is too obscure and too long.",0.2388059701492537,0.06153846153846154,0.1492537313432836,0.8591838479042053,0.8699257373809814,0.8645214438438416
https://openreview.net/forum?id=B1TTpYKgx,"SUMMARY --------This paper studies the expressive power of deep neural networks under various related measures of expressivity. --------It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). --------The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. ----------------PROS --------The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. ----------------CONS --------The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. ----------------COMMENTS--------- The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush. --------Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature. --------The connection to previous works could also be clearer. ----------------- On page 2 one finds the statement ``Furthermore, architectures are often compared via ‘hardcoded’ weight values -- a specific function that can be represented efficiently by one architecture is shown to only be inefficiently approximated by another.'' ----------------This is partially true, but it neglects important parts of the discussion conducted in the cited papers. --------In particular, the paper [Montufar, Pascanu, Cho, Bengio 2014] discusses not one hard coded function, but classes of functions with a given number of linear regions. --------That paper shows that deep networks generically* produce functions with at least a given number of linear regions, while shallow networks never do. --------* Generically meaning that, after fixing the number of parameters, any function represented by the network, for parameter values form an open, positive -measure, neighbourhood, belongs to the class of functions which have at least a certain number of linear regions. --------In particular, such statements can be directly interpreted in terms of networks with random weights. ----------------- One of the measures for expressivity discussed in the present paper is the number of Dichotomies. In statistical learning theory, this notion is used to define the VC-dimension. In that context, a high value is associated with a high statistical complexity, meaning that picking a good hypothesis requires more data. ----------------- On page 2 one finds the statement ``We discover and prove the underlying reason for this – all three measures are directly proportional to a fourth quantity, trajectory length.'' --------The expected trajectory length increasing exponentially with depth can be interpreted as the increase (or decrease) in the scale by a composition of the form a*...*a x, which scales the inputs by a^d. Such a scaling by itself certainly is not an underlying cause for an increase in the number of dichotomies or activation patterns or transitions. Here it seems that at least the assumptions on the considered types of trajectories also play an important role. --------This is probably related to another observation from page 4: ``if the variance of the bias is comparatively too large... then we no longer see exponential growth.''----------------OTHER SPECIFIC COMMENTS --------In Theorem 1 --------- Here it would be good to be more specific about ``random neural network'', i.e., fixed connectivity structure with random weights, and also about the kind of one-dimensional trajectory, i.e., finite in length, closed, differentiable almost everywhere, etc. ----------------- The notation ``g \geq O(f)'' used in the theorem reads literally as |g| \geq \leq k |f| for some k>0, for large enough arguments. It could also be read as g being not smaller than some function that is bounded above by f, which holds for instance whenever g\geq 0. --------For expressing asymptotic lower bounds one can use the notation \Omega (see https://en.wikipedia.org/wiki/Big_O_notation). ----------------- It would be helpful to mention that the expectation is being taken with respect to the network weights and that these are normally distributed with variance \sigma. ----------------- Theorem 2. Here it would be good to be more specific about the kind of sign transitions. Is this about transitions at any units of the network, or about sign transitions at the scalar output of the entire network. ----------------- Theorem 3 is quite trivial. --------The bijection between transitions and activation patterns is not clear. --------Take a regular n-gon in the plane and a circle that crosses each edge twice. --------This makes 2n transitions but only n+1 activation patterns. ----------------- Theorem 4. --------Where is the proof of this statement? --------How does this relate to the simple fact that each activation pattern corresponds to the vector indicating the units that are `active'? ------------------------MINOR COMMENTS--------- The names of the theorems (e.g. ``Bound on ...'' in Theorem 1) could be separated more clearly from the statements, for instance using bold font, a dot, or parentheses. --------- On page 4, in Latex one can use \gg for the `much larger' symbol. --------- On page 4, explain the notation \delta z_\orth.  --------- On page 4, explain that ``latent image'' refers to the image in the last layer. --------- Why are there no error bars in Figure 2?  --------- On page 5 explain that the hyperplane is in the last hidden layer. --------- On page 5, ``is transitioning for any input''. This is not clearly stated, since a transition takes place at a point in a trajectory of inputs, not for a single input. --------- The y-axis labels in Figure 1 (c) and (d) are too small. --------- Why are there no error bars in Figure 1 (a) and (b)? The caption could at least mention that shown are the averages over experiments. --------- In Figure 4 (b) the curves are occluded by the labels. --------- The numbering of results is confusing. In the Appendix some numbers are repeated with the main part and some are missing. --------- On page 19. Theorem 6. As far as I remember Stanley also provides an elementary proof of case with hyperplanes in general position. Many other works also provide elementary proofs using the same induction arguments in what is known as the sweep hyperplane method. ","While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.",The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions. The paper is a bit long and seems to have been written a bit in a rush.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions. The paper is a bit long and seems to have been written a bit in a rush.," The paper is a bit long (especially the appendix) and seems to have been written a bit in a rush . Overall the main points are presented clearly, but the results and conclusions could be clearer about the assumptions / experimental vs theoretical nature","geq O(f)'' used in the theorem reads literally as |g| geq leq k |f| for some k>0, for large enough arguments",0.16666666666666666,0.034482758620689655,0.1,0.8493649363517761,0.8619726896286011,0.855622410774231
https://openreview.net/forum?id=B1TTpYKgx,"Summary of the paper:----------------Authors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the ‘trajectory length’ of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : ‘transitions’,’activation patterns ’ and ‘Dichotomies’. --------As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.----------------Clarity:----------------The  paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are not clear. ----------------Novelty:----------------Studying the trajectory length as function of transforming the data by a multilayer network is   new and interesting idea. The relation to transition numbers is in term of the growth factor, and not as a quantity to quantity relationship. Hence it is hard to understand what are the implications.----------------Significance:----------------The geometry of the input set (of dimension m)  shows up only weakly in the activation patterns analysis.  The trajectory study should tell us how the network organizes the input set. As observed in the experiments the network becomes contractive/selective as we train the network. It would be interesting to study those phenomenas using this trajectory length , as a measure for disentangling nuisance factors ( such as invariances etc.). In the supervised setting the network need not to be contractive every where , so it needs to be selective to the class label, a  theoretical study of the selectivity and contraction using the trajectory length would be more appealing.----------------Detailed comments:----------------Theorem 1:----------------- As raised by reviewer one the definition of a one dimensional input trajectory is missing. --------- What does theorem 1 tells us about the design and the architecture to use in neural networks as promised in the introduction is not clear. The connection to transitions in Theorem 2 is rather weak. ----------------Theorem 2:----------------- in the proof of theorem 2 it not clear what is meant by T and t. Notations are confusing, the expectation is taken with respect to which weight: is it W_{d+1} or (W_{d+1} and W_{d})? I understand you don't want to overload notation but maybe E_{d+1} can help keeping track. I don't see how the recursion is applied if T and t in it, have different definitions. seems T_{d+1} for you is a random variable and t_{d} is fixed. Are you fixing W_d and then looking at W_{d+1} as  random?----------------- In the same proof:  the recursion  is for d>1  ? your analysis is for W \in R^{k\times k}, you don't not study the W \in \mathbb{R}^{k\times m}. In this case you can not assume assume that |z^(0)|=1.----------------- should d=1, be analyzed alone to know how it scales with m?----------------Theorem 4 in main text:----------------- Is the proof missing? or Theorem 4 in the main text is Theorem 6 in the appendix?----------------Figures 8 and 9:----------------- the trajectory length reduction in the training isn't that just the network becoming contractive to enable mapping the training points to the labels? See for instance  on contraction in deep networks https://arxiv.org/pdf/1601.04920.pdf----------------- How much the plot depends on the shape of the trajectory? have you tried other then circular trajectory?----------------- In these plots the 2 mnist points had same label ? or different label?  both cases should be studied, to see the tradeoff between contraction and selectivity to the class label.","While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold.","The paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are notclear. The relation to transition numbers is in term of the growth factor, and not as a quantity",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper is a little hard to follow, since  the motivations are not clear in the introduction and the definitions across the paper are notclear. The relation to transition numbers is in term of the growth factor, and not as a quantity"," The paper is a little hard to follow, since the motivations are not clear in the introduction and the definitions across the paper aren't clear . It would be interesting to study those phenomenas using this trajectory length as a measure for disent",is missing. ------------Theorem 4 in main text:-----------------Theorem 2:----------------- Is the proof missing?-------------,0.1846153846153846,0.031746031746031744,0.15384615384615383,0.840503990650177,0.8600140810012817,0.8501470685005188
https://openreview.net/forum?id=rJLS7qKel,"Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition.----------------The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.----------------There have been a number of prior works both in predicting future states as part of RL and goal driven function approximators which the authors review in section 2. The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional ‘measurements’ for prediction, the parametrized goals and, perhaps most importantly, the empirical comparison to relevant prior work.----------------In addition to the comparison with Visual Doom AI, the authors show that their algorithm is able to learn generalizable policies which can respond, without further training, to limited changes in the goal.----------------The paper is well-communicated and the empirical results compelling and will be of significant interest.----------------Some minor potential improvements:--------There is an approximation in the supervised training as it is making an on-policy assumption but it learns from a replay buffer (with the Monte Carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy, but is being sampled from episodes generated by prior versions of the policy). This should be discussed.--------The algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not. I think this, and the limitations of this approach (e.g. it may not work well in a sensory environment if such measurements are not provided) should be mentioned more clearly.","This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-written paper.",Algorithm was the winner of the Visual Doom AI competition. Key idea of their algorithm is to use additional low-dimensional observations as a supervised target for prediction.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Algorithm was the winner of the Visual Doom AI competition. Key idea of their algorithm is to use additional low-dimensional observations as a supervised target for prediction.," Algorithm was the winner of the Visual Doom AI competition . Key idea of their algorithm is to use additional low-dimensional observations as a supervised target for prediction . This prediction is conditioned on a goal vector (which is given, not learned",".-------------The authors review this empirically driven work in section 2. The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional ‘measurements’",0.21686746987951808,0.0,0.14457831325301204,0.874009370803833,0.8550599217414856,0.864430844783783
https://openreview.net/forum?id=rJLS7qKel,"This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. ----------------- The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract/intro----------------- I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past. ----------------- Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)----------------Overall, I think this paper does show clear empirical advantage of using the proposed underlying formulations and experimental insights from this paper might be valuable for future agents","This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-written paper.",This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. The biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. The biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems., The biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems . I would have liked to see failure modes of this approach .,of this paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. ----------------- The method is special case of an universal value function based approach and the authors do cite the correct references,0.33333333333333326,0.06818181818181819,0.2222222222222222,0.8827720880508423,0.871204674243927,0.876950204372406
https://openreview.net/forum?id=rJLS7qKel,"The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) ""goal"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.----------------The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:-------- - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).-------- - There is an ablation study that supports the thesis that all the ""added complexity"" of the paper's model is useful.----------------Predicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. ----------------A few comments (nitpicks) on the form:-------- - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.-------- - The use of ""P"" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).-------- - The double use of ""j"" (admittedly, with different fonts) in (6) may be misleading.-------- - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).----------------I think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that ""correct"" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and ""milestone"" (vizDoom winner) papers should get published.","This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-written paper.","The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom. The results are impressive, as this model won the 2016 vizDoom competition.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom. The results are impressive, as this model won the 2016 vizDoom competition.", The paper presents an on-policy method to predict future intrinsic measurements . All the experiments are performed in the game of Doom (vizDoom to be exact) The authors trained their model to predict (a sequence of) triplets,".------------- - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al.,",0.38202247191011235,0.09195402298850573,0.20224719101123595,0.906681478023529,0.8810683488845825,0.8936914205551147
https://openreview.net/forum?id=rJo9n9Feg,"Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. ----------------Authors claim that this:--------1. is a very interesting finding. --------2. CNN has figured out game rules. --------3. Cross modal supervision is applicable to higher-level semantics. ----------------I don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only ""one"" stage of a game - i.e. last move that is considered. Further, the results are on the training set itself - the bare minimum requirement of any implicit or explicit representation of game rules is the ability to act in previously unseen states (i.e. generalization). Even if the CNN did generalize, I would avoid making any claims about knowledge of game rules. ----------------For (3), author's definition of cross-modal seems to be training from images to games moves. In image-classification we go from images --> labels (i.e. between two different domains). We already know CNNs can perform such mappings. CNNs have been used to map images to actions such as in DQN my Mnih et al., or DDPG by Lillicrap et al. and a lot of other classical work such as ALVIN. It's unclear what points authors are trying to make. ----------------For (1): how interesting is an implicit attention mechanism is a subjective matter. The authors claim a difference between the concepts of ""what do do"" and ""what will happen"". They claim by supervising for ""what will happen"", the CNN can automatically learn about ""what to do"". This is extensively studied in the model predictive control literature. Where model is ""what will happen next"", and the model is used to infer a control law - ""what to do"". However, in the experimental setup presented in the paper what will happen and what to do seem to be the exact same things. ----------------For further analysis of what the CNN has learnt I would recommend:--------(a) Visualizing CAM with respect to incorrect classes. For eg, visualize the CAM with respect to player would lose (instead of winning).----------------(b) Split the data into train/val and use the predictions on the val-set for visualization. These would be much more informative about what kind of ""generalizable"" features the CNN pays attention to. ----------------In summary, understanding why CNN's make what decisions they make is a very interesting area of research. While the emergence of an implicit attention mechanism may be considered to be an interesting finding by some, many claims made by the authors are not supported by experiments (see comments above). ------------------------ ","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work.",Game of tic-tac-toe is considered. 1029 board combinations are chosen so that a single move will result into victory of either the black or the white player. A CNN is trained from a visual rendering of the game,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Game of tic-tac-toe is considered. 1029 board combinations are chosen so that a single move will result into victory of either the black or the white player. A CNN is trained from a visual rendering of the game, A CNN is trained from a visual rendering of the game board to these 18 possible outputs . CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes . Authors find that predictions correspond to the winning board,"is a very interesting finding. ------------For (3), author's definition of cross-modal seems to be training from images to games moves. The authors claim a difference between concepts of ""what do do""",0.13333333333333333,0.0,0.08,0.7999768257141113,0.8244982957839966,0.8120524883270264
https://openreview.net/forum?id=rJo9n9Feg,"Summary--------===--------This paper presents tic-tac-toe as toy problem for investigating CNNs.--------A dataset is created containing tic-tac-toe boards where one player is one--------move away from winning and a CNN is trained to label boards according--------to (1) the player who can win (2 choices) and (2) the position they may move--------to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper--------performs perfectly at the task and the paper's goal is to inspect how the--------CNN works.----------------The fundamental mechanism for this inspection is Class Activation--------Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention--------in the CNN. These implicit attention maps (localization heat maps) are used to--------derive actions (which square each player should move). The attention maps  ----------------(1) attend to squares in the tic-tac-toe board rather than arbitrary--------blobs, despite the fact that one square in a board has uniform color, and----------------(2) they can be used to pick correct (winning) actions.----------------This experiment are used to support assertions that the network understands--------(1) chess (tic-tac-toe) boards--------(2) a rule for winning tic-tac-toe--------(3) that there are two players.----------------Some follow up experiments indicate similar results under various renderings--------of the tic-tac-toe boards and an incomplete training regime.------------------------More Clarifying Questions--------===----------------* I am not quite sure precisely how CAM is implemented here. In the original CAM--------one must identify a class of interest to visualize (e.g., cat or dog). I don't--------think this paper identifies such a choice. How is one of the 18 possible classes--------chosen for creating the CAM visualization and through that visualization--------choosing an action?----------------* How was the test set for this dataset for the table 1 results created?--------How many of the final 1029 states were used for test and was the--------distribution of labels the same in train and test?----------------* How is RCO computed? Is rank correlation or Pearson correlation used?--------If Pearson correlation is used then it may be good to consider rank correlation,--------as argued in ""Human Attention in Visual Question Answering: Do Humans and--------Deep Networks Look at the Same Regions?"" by Das et. al. in EMNLP 2016.--------In table 1, what does the 10^3 next to RCO mean?------------------------Pros--------===----------------* The proposed method, deriving an action to take from the result of a--------visualization technique, is very novel.----------------* This paper provides an experiment that clearly shows a CNN relying on context--------to make accurate predictions.----------------* The use of a toy tic-tac-toe domain to study attention in CNNs--------(implicit or otherwise) is a potentially fruitful setting that may--------lead to better understanding of implicit and maybe explicit attention mechanisms.------------------------Cons--------===----------------* This work distinguishes between predictions about ""what will happen""--------(will the white player win?) and ""what to do"" (where should the white--------player move to win?). The central idea is generalization from ""what will happen""--------to ""what to do"" indicates concept learning (sec. 2.1). Why should an ability to--------act be any more indicative of a learned concept than an ability to predict--------future states. I see a further issue with the presentation of this approach and--------a potential correctness problem:----------------1. (correctness)--------In the specific setting proposed I see no difference between ""what to do""--------and ""what will happen.""----------------Suppose one created labels dictating ""what to do"" for each example in the--------proposed dataset. How would these differ from the labels of ""what will happen""--------in the proposed dataset? In this case ""what will happen"" labels include--------both player identity (who wins) and board position (which position they move--------to win). Wouldn't the ""what to do"" labels need to indicate board position?--------They could also chosen to indicate player identity, which would make them--------identical to the ""what will happen"" labels (both 18-way softmaxes).----------------2. (presentation)--------I think this distinction would usually be handled by the Reinforcement Learning--------framework, but the proposed method is not presented in that framework or--------related to an RL based approach. In RL ""what will happen"" is the reward an--------agent will receive for making a particular action and ""what to do"" is the--------action an agent should take. From this point of view, generalization from--------""what will happen"" to ""what to do"" is not a novel thing to study.----------------Alternate models include:--------    * A deep Q network (Mnih. et. al. 2015) could predict the value of--------      every possible action where an action is a (player, board position) tuple.--------    * The argmax of the current model's softmax could be used as an action--------      prediction.--------The deep Q network approach need not be implemented, but differences between--------methods should be explained because of the uniqueness of the proposed approach.------------------------* Comparison to work that uses visualization to investigate deep RL networks--------is missing. In particular, other work in RL has used Simonyan et. al.--------(arXiv 2013) style saliency maps to investigate network behavior. For example, --------""Dueling Network Architectures for Deep Reinforcement Learning"" by Wang et. al.--------in (ICML 2016) uses saliency maps to identify differences between their--------state-value and advantage networks. In ""Graying the black box:--------Understanding DQNs"" by Zahavy et. al. (ICML 2016) these saliency maps are--------also used to analyze network behavior.------------------------* In section 2.3, saliency maps of Simonyan et. al. are said to not be able to--------activate on grid squares because they have constant intensity, yet no empirical--------or theoretical evidence is provided for this claim.----------------On a related note, what precisely is the notion of information referenced in--------section 2.3 and why is it relevant? Is it entropy of the distribution of pixel--------intensities in a patch? To me it seems that any measure which depends only--------on one patch is irrelevant because the methods discussed (e.g., saliency maps)--------depend on context as well as the intensities within a patch.------------------------* The presentation in the paper would be improved if the results in section 7--------were presented along with relevant discussion in preceding sections.------------------------Overall Evaluation--------===--------The experiments presented here are novel, but I am not sure they are very--------significant or offer clear conclusions. The methods and goals are not presented--------clearly and lack the broader relevant context mentioned above. Furthermore, I--------find the lines of thought mentioned in the Cons section possibly incorrect--------or incomplete. As detailed with further clarifying questions, upon closer--------inspection I do not see how some aspects of the proposed approach were--------implemented, so my opinion may change with further details.","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work.",This paper presents tic-tac-toe as toy problem for investigating CNNs. A CNN is trained to label boards according to the player who can win (2 choices) and the position they may move (9 choices) The,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents tic-tac-toe as toy problem for investigating CNNs. A CNN is trained to label boards according to the player who can win (2 choices) and the position they may move (9 choices) The, A paper presents tic-tac-toe as toy problem for investigating CNNs . The CNN evaluated in this paper performs perfectly at the task and the paper's goal is to inspect how the CNN works . The use of a toy,--------------------------------------------------------------------------------------------------------------------------------------------,0.19444444444444445,0.0,0.08333333333333334,0.8070776462554932,0.8381803631782532,0.8223350644111633
https://openreview.net/forum?id=rJo9n9Feg,"1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying ""If you place a black square in the middle right, black will win"" or ""if you place a white square in the upper left, white will win"". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.----------------The focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more clear about what each class is). So we ask CAM to determine the area of focus of the CNN for deciding whether class 1 is exhibited. The focus ends up being on the empty bottom right square (because certainly you can't exhibit class 1 if the bottom right square is occupied). The CNN also needs to condition its decision on other parts of the board -- it needs to know whether there will be 3 in a row from some direction. But maybe that conditioning is weaker?----------------That's kind of interesting but I'm not sure about the deeper statements about discovering game rules that the paper hints at. I'm also not sure about the connection of this work to weakly supervised learning or multi-modal learning.----------------The paper is pretty well written, overall, with some grammatical mistakes, but I simply don't see the surprising discovery of this work. ----------------I also have some concerns about how contrived this scenario is -- using a big, expressive CNN for such a simple game domain and using a particular CNN visualization method.----------------I am not an expert in reinforcement learning (which isn't happening in this paper, but is in related works on CNN game playing), so maybe I'm not appreciating the paper appropriately.","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work.",1029 tic-tac-toe boards are rendered (in various ways) These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",1029 tic-tac-toe boards are rendered (in various ways) These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations, 1029 tic-tac-toe boards are legal boards where the next legal play can end the game . CNN is trained to predict these 18 categories and can do so with 100% accuracy . Zhou et al's Class Activation,1029 tic-tac-toe boards are rendered (in various ways) These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the,0.19444444444444445,0.0,0.1388888888888889,0.8063421845436096,0.8321076035499573,0.8190222978591919
https://openreview.net/forum?id=SygvTcYee,"The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.----------------My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely used. I encourage the authors to apply this framework to more generic architectures and problems.----------------Questions:--------1- Does this framework apply to some form of generic multi-layer neural network? If so, some experimental results are useful.--------2- What is the implication of applying this framework to more than two components (an encoder and a decoder) and non-linear components?--------3- It is desired to see a plot of performance as a function of time for different setups to demonstrate the speedup after convergence. It seems the paper only focuses on the speedup factors per iteration. For example, increasing the mini-batch size may improve the speed per iteration but may hurt the convergence speed.--------4- Did you consider a scenario where the dataset is too big that storing the data and auxiliary variables on multiple machines simultaneously is not possible?----------------The paper cites an ArXiv manuscript with the same title by the authors multiple times. Please make the paper self-contained and include any supplementary material in the appendix.----------------I believe without applying this framework to a more generic architecture beyond binary autoencoders, this paper does not appeal to a wide audience at ICLR, hence weak reject.","The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.",The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencod,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencod, The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between,"based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates.",0.12359550561797752,0.005649717514124294,0.08426966292134831,0.8425440192222595,0.7971323728561401,0.8192093968391418
https://openreview.net/forum?id=SygvTcYee,"UPDATE:--------I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.--------However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, ""...ICLR submission focus on the ParMAC algorithm..."", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.----------------ORIGINAL REVIEW:--------The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.----------------Related Work: In the part on convex ERM and methods, I would recommend citing general communication efficient frameworks, COCOA (Ma et al.) and AIDE (Reddi et al.). I believe these works are most related to the practical objectives authors of this paper set, while number of the papers cited are less relevant.----------------Section 2, explaining MAC, is quite clearly written, but I do not find part on MAC and EM particularly useful.----------------Section 3 is much less clearly written. I have trouble following notation, particularly in the speedups part, as different symbols were introduced at different places. Perhaps a quick summary or paragraph on notation in the introduction would be helpful. In paragraph 2, you write as if reader knew how data/anything is distributed, but this was not mentioned yet; it is specified later. It is not clear what is meant by ""submodel"". Perhaps a more precise example pointing back to eqs (1) & (2) would be useful. As far as I understand from what is written, there are P independent sets of submodels, that traverse the machines in circular fashion. I don't understand how are they initialized (identically?), and more importantly I don't understand what would be a single output of the algorithm (averaging? does not seem to make sense). Since this is not addressed, I suppose I get it wrong, leaving me to guess what was actually meant. --------The fact that I am not able to understand what is actually happening, I see as major issue.----------------I don't like the later paragraphs on extensions, model for speedup, convergence and topologies. I don't understand whether these are novel contributions or not, as the authors refer to other work for details. If these are novel, the explanation is not sufficient, particularly speedup part, which contains undefined quantities, e.g. T(P) (or I can't find it). If this is not novel, It does not provide enough explanation to understand anything more, compared with a its version compressed to 1/4 of its size and referring to the other work. The statement that we can recover the original convergence guarantees seems strong and I don't see why it should be trivial to show (but author point to other work which I did not look at). In topologies part, claiming that something does ""true SGD"", without explaining what is ""true SGD"" seems very strange. Other statements in this section seem also very vague and unjustified/unexplained.----------------Experimental section seems to suggest that the method is interesting for binary autoencoders, but I don't see how would I conclude anything about any other models. ParMAC is also not compared to alternative methods, only with itself, focusing on scaling properties.----------------Conclusion contains statements that are too strong or misleading based on what I saw. In particular, ""we analysed its parallel speedup and convergence"" seems ungrounded. Further, the claim ""The convergence properties of MAC remain essentially unaltered in ParMAC"" is unsupported, regardless of the meaning of ""essentially unchanged"".----------------In summary, the method seems relevant for particular model class, binary autoencoders, but clarity of presentation is insufficient - I wouldn't be able to recreate the algorithm used in experiments - and the paper contains a number of questionable claims.","The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.","The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea. If you want to compress it into a conference submission of this type, I would recommend choosing message you want",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea. If you want to compress it into a conference submission of this type, I would recommend choosing message you want"," The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates) formulating a distributed variant of the idea . The paper contains a number of questionable claims .",", but I don't understand whether these are novel contributions or not, as the authors refer to other work for details. I don't understand whether these are novel contributions or not, as the authors refer to other work for details.",0.11299435028248589,0.022727272727272728,0.0903954802259887,0.839074432849884,0.7939034104347229,0.8158642053604126
https://openreview.net/forum?id=SygvTcYee,"This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.----------------This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. ----------------There also seem to be a few ideas worth comparing, at least:--------- Circular vs. parameter server configurations--------- Decoupled sub-problems vs. parallel SGD----------------Parallel SGD also has the benefit that it's extremely easy to implement on top of NN toolboxes, so this has to work a lot better to be practically useful. ----------------Also, it's a bit hard to understand what exactly is being passed around from round to round, and what the trade-offs would be in a deep feed-forward network. Assuming you have one sub-problem for every hidden unit, then it seems like:----------------1. In the W step, different bits of the NN walk their way around the cluster, taking SGD steps w.r.t. the coordinates stored on each machine. This means passing around the parameter vector for each hidden unit.--------2. Then there's a synchronization step to gather the parameters from each submodel, requiring a traversal of the circular structure.--------3. Then each machine updates it's coordinates based on the complete model for a slice of the data. This would mean, for a feed-forward network, producing the intermediate activations of each layer for each data point.----------------So for something comparable to parallel SGD, you could do the following: put a mini-batch of size B on each machine with ParMAC, compared to running such mini-batches in parallel. Completing steps 1-2-3 above would then be roughly equivalent to one synchronized PS type implementation step (distribute model to workers, get P gradients back, update model.)---------------- It would be really helpful to see how this compares in practice. It's hard for me to understand intuitively why the proposed method is theoretically any better than parallel SGD (except for the issue of non-smooth function optimization); the decoupling also can fundamentally change the problem since you're not doing back-propagation directly anymore, so that seems like it would conflate things as well and it's not necessarily going to just work for other types of architectures.","The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.",This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of, This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration . The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of,.----------------This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to de,0.1452513966480447,0.028089887640449437,0.111731843575419,0.8465921878814697,0.7984322905540466,0.8218072056770325
https://openreview.net/forum?id=SygvTcYee,"This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are that with large datasets in distributed systems, it is imperative to minimise data movement over the network because of the communication time generally far exceeds the computation time in modern architectures. Thus, the authors propose the ParMAC to translate the parallelism inherent in MAC into a distributed system by data parallelism and model parallelism. They also analyse its parallel speedup and convergence, and demonstrated it with MPI-based implementation to optimise binary autoencoders. The proposed ParMAC is tested on 3 colour image retrieval datasets. ----------------The organization of the paper is well written, and the presentation is clear. My questions are included in the following:--------- The MAC framework solves the original problem approximately. If people use the sigmoid function to smooth the stepwise function, the naive optimization methods can be easier applied. What is the difference between these two? Or why do we want to use a new approach to solve it?--------- The authors do not compare their ParMAC model with other distributed approaches for the same nested function optimization problem.","The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD.","This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models. The basic idea of MAC to optimise the nested objective",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models. The basic idea of MAC to optimise the nested objective"," ParMAC is a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets)","of MAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers",0.13636363636363638,0.017142857142857144,0.0909090909090909,0.8458465337753296,0.7925257682800293,0.8183185458183289
https://openreview.net/forum?id=S1j4RqYxg,"The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.----------------Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets. ","The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper.","The algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR"," The authors present a new algorithm for the effective calculation of polynomial features on Sparse Matrices . The fit with the main scope of ICLR is far from obvious with this work, that should probably be re-submitted",. The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The authors present here a new algorithm for the effective calculation of polynomial features on Spar,0.35514018691588783,0.07619047619047618,0.26168224299065423,0.8816664218902588,0.864586353302002,0.8730428814888
https://openreview.net/forum?id=S1j4RqYxg,"This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.----------------The background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.----------------The experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as ""likely a language implementation"", which is not convincing. To fairly compare the two methods, of course you need to implement both in the same programming language and run experiments in the same environment. For higher degree feature expansion, there is no empirical experiments to show the advantage of the proposed method.----------------Some minor problems are listed below.--------1) In Section 2, the notation ""p_i:p_i+1"" is not clearly defined.--------2) In Section 3.1, typo: ""efter"" - ""after""--------3) All the algorithms in this paper are not titled. The input and output is not clearly listed.--------4) In Figure 1, the meaning of the colored area is not described. Is it standard deviation or some quantile of the running time? How many runs of each algorithm are used to generate the ribbons? Many details of the experimental settings are missing.","The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper.",This paper proposes an algorithm for polynomial feature expansion on CSR matrices. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing. The background of the problem is not sufficiently introduced.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper proposes an algorithm for polynomial feature expansion on CSR matrices. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing. The background of the problem is not sufficiently introduced., This paper proposes an algorithm for polynomial feature expansion on CSR matrices . The main contribution of this work is not significant enough . The experiments are incomplete and not convincing .,", which reduces the time complexity of the standard method by a factor dk where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and",0.3018867924528302,0.057692307692307696,0.18867924528301885,0.8669396042823792,0.8487548232078552,0.8577508330345154
https://openreview.net/forum?id=S1j4RqYxg,"The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.----------------However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. ","The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper.",The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess. Also the quality of writing requires improvement.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess. Also the quality of writing requires improvement.," It seems to me that the topic is not very relevant to the focus of ICLR . Also the quality of writing requires improvement, especially literature review .","is beyond my expertise. I cannot give any solid review comments regarding the techniques that better than an educated guess.-------------Totuşi, it seems to me that the topic is not very relevant to the focus of IC",0.18367346938775508,0.04166666666666666,0.14285714285714282,0.8645167350769043,0.8502532839775085,0.8573256731033325
https://openreview.net/forum?id=r1aPbsFle,"This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.----------------It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection layer, as is usually done.----------------The experimental results are good and provide support for the approximate derivation done in section 4, particularly the distance plots in figure 1.----------------Minor comments:--------Third line in abstract: where model -> where the model--------Second line in section 7: into space -> into the space--------Shouldn't the RHS in Eq 3.5 be \sum \tilde{y_{t,i}}(\frac{\hat{y}_t}{\tilde{y_{t,i}}} - e_i) ?","pros:  - nice results on the tasks that justify acceptance of the paper    cons:  - In my opinion its a big stretch to describe this paper as a novel framework. The reasons for using the specific contrived augmented loss is based on the good results it produces. I view it more as regularization.  - The ""theoretical justification"" for coupling of the input and output layers is based on the premise that the above regularization is the correct thing to do. Since that's really not justified by some kind of theory, I think its questionable to call this simple observation a theoretical justification.  - Tying weights on the inputs and output layers is far from novel.",This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It proposes an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It proposes an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution., This work offers a theoretical justification for reusing the input word embedding in the output projection layer . It does by proposing an additional loss designed to minimize the distance between the predictive distribution and an estimate of the true data distribution .,"in Eq 3.6 (let's rename it to L') is to use an independent matrix for the output projection layer, as is usually done.------------The experimental results are good and provide support for the",0.27450980392156865,0.06622516556291391,0.18300653594771243,0.8591578006744385,0.8222136497497559,0.8402798175811768
https://openreview.net/forum?id=r1aPbsFle,"This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.--------Experiments on PTB shows significant improvement.--------The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.--------I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.--------Have you considered using character or sub-word units in that context?","pros:  - nice results on the tasks that justify acceptance of the paper    cons:  - In my opinion its a big stretch to describe this paper as a novel framework. The reasons for using the specific contrived augmented loss is based on the good results it produces. I view it more as regularization.  - The ""theoretical justification"" for coupling of the input and output layers is based on the premise that the above regularization is the correct thing to do. Since that's really not justified by some kind of theory, I think its questionable to call this simple observation a theoretical justification.  - Tying weights on the inputs and output layers is far from novel.","This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax. The proposed justification appears new to me though, and certainly interesting.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax. The proposed justification appears new to me though, and certainly interesting."," This paper provides a theoretical framework for tying parameters between input and output word representations in the softmax . The proposed justification appears new to me though, and certainly interesting .",".--------The idea of sharing or tying weights between input and output word embeddings is not new to me though, and certainly interesting.----------I was concerned that results are only given on",0.2237762237762238,0.0425531914893617,0.1678321678321678,0.8652713298797607,0.81989985704422,0.8419747948646545
https://openreview.net/forum?id=r1aPbsFle,"This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. ----------------I see two main drawbacks from this framework:--------The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. --------The augmented loss is heavily “engineered” to produce the desired result of parameter tying. It’s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. ----------------Nevertheless the argument is very interesting, and clearly written.--------The simulated results indeed validate the argument, and the PTB results seem promising.----------------Minor comments:--------Section 3:--------Can you clarify if y~ is conditioned on the t example or on the entire history.--------Eq. 3.5: i is enumerated over V (not |V|) ","pros:  - nice results on the tasks that justify acceptance of the paper    cons:  - In my opinion its a big stretch to describe this paper as a novel framework. The reasons for using the specific contrived augmented loss is based on the good results it produces. I view it more as regularization.  - The ""theoretical justification"" for coupling of the input and output layers is based on the premise that the above regularization is the correct thing to do. Since that's really not justified by some kind of theory, I think its questionable to call this simple observation a theoretical justification.  - Tying weights on the inputs and output layers is far from novel.",This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding., This paper gives a theoretical motivation for tying the word embedding and output projection matrices in RNN LMs . The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding .,----------The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. -----------The augmented loss function has no trainable parameters and is used for,0.21476510067114096,0.054421768707483,0.1476510067114094,0.8474401831626892,0.814178466796875,0.8304764032363892
https://openreview.net/forum?id=HkljfjFee,"The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation).----------------Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance.----------------Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast encoder. Here too, the idea is interesting and seems to be quite efficient from experiments on USPS data, even if the framework seems to be strongly inspired from LISTA. That scheme should however be better motivated, by the limitations of SRSC that should be presented more clearly. ----------------Overall, the paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective. ","Adding a manifold regularizer to a learning objective function is certainly not a new direction. The paper argues that using a support based regularizer is superior to using a standard graph Laplacian regularizer (which has been explored before), although this argument is not developed particularly rigorously and dominantly has to fall back on empirical evidence. The main contribution of the paper appears to be theoretical justification of an alternating optimization scheme for minimizing the resulting objective function (yet the optimization aspects of dealing with a sparse support regularizer are somewhat orthogonal to the current context). The empirical results are not very convincing since the dictionary size is relatively large compared to the dataset size; the gains with respect to l2 manifold regularizer are not consistent; and the gains using deep architectures to directly predict sparse codes are also modest and somewhat inconsistent. These points aside, the reviewers are overall enthusiastic about the paper and find it to be well written and complete.","The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. The general idea is not unique, but it is an interesting one.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. The general idea is not unique, but it is an interesting one."," The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding . Namely, two data samples that are similar or neighbours, should have a sparse code that",".----------------Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding",0.20100502512562812,0.04060913705583756,0.1407035175879397,0.8653355836868286,0.8203318119049072,0.8422329425811768
https://openreview.net/forum?id=HkljfjFee,"In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail. ----------------In the abstract the authors mention “extensive experimental results …”, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in “CUDA C++ with extreme efficiency” (page 10). But more importantly, I find it hard to interpret and compare the results: The paper reports accuracy and and normalized mutual information for a image retrieval / clustering task where the proposed SRSC is used as a feature extractor. The improvements relative to standard Sparse Coding seem very small  (often < 1% in terms of NMI; it looks more promising in terms of accuracy) and if I understand the description on page 11 correctly, than the test set was used to select some hyperparameters (the best similarity measure for clustering step)? There is no comparisons to other baselines / state of the art image clustering methods.--------Besides of providing features for a small scale image clustering system, are there maybe ways to more directly evaluate the properties and qualities of  a sparse coding approach? E.g. reconstruction error / sparsity; maybe even denoising performance?-------- --------In summary, I think in it current form the paper lacks the evaluation and experimental results for an ICLR publication. Intuitively, I agree with the authors that the proposed regularization is an interesting direction, but I don’t see experiments that directly show that the regularization has the desired effect; and the improvements in the clustering task where SRSC is used as a feature extractor are very modest.","Adding a manifold regularizer to a learning objective function is certainly not a new direction. The paper argues that using a support based regularizer is superior to using a standard graph Laplacian regularizer (which has been explored before), although this argument is not developed particularly rigorously and dominantly has to fall back on empirical evidence. The main contribution of the paper appears to be theoretical justification of an alternating optimization scheme for minimizing the resulting objective function (yet the optimization aspects of dealing with a sparse support regularizer are somewhat orthogonal to the current context). The empirical results are not very convincing since the dictionary size is relatively large compared to the dataset size; the gains with respect to l2 manifold regularizer are not consistent; and the gains using deep architectures to directly predict sparse codes are also modest and somewhat inconsistent. These points aside, the reviewers are overall enthusiastic about the paper and find it to be well written and complete.","In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed"," In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary . The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed",SRSC in “CUDA C++ with extreme efficiency” (page 10). But I find the experiments not very convincing: The paper reports accuracy and normalized mutual information for a image retrieval / clustering task where the,0.18811881188118812,0.039999999999999994,0.12871287128712872,0.8544891476631165,0.8234463930130005,0.8386805653572083
https://openreview.net/forum?id=HkljfjFee,"I'd like to thank the authors for their detailed response to my questions.----------------The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.----------------The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. ----------------Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.----------------The authors also propose a fast encoding scheme for their proposed method. --------The authors included a new experiment in semi-supervised consists of a very interesting use (of the method and the fast approximation). While this is an interesting addition, I think that using fast encoders is not particularly novel or the main part of the work. ""Converting"" iterative optimization algorithms into feed-forward nets for accelerating the inference process has been done in the past (several times with quite similar problems). Is natural that this can be done, and not very surprising. Maybe would be interesting to evaluate how important is to have an architecture matching the optimization algorithm, compared to a generic network (though some of this analysis has also been performed in the past).","Adding a manifold regularizer to a learning objective function is certainly not a new direction. The paper argues that using a support based regularizer is superior to using a standard graph Laplacian regularizer (which has been explored before), although this argument is not developed particularly rigorously and dominantly has to fall back on empirical evidence. The main contribution of the paper appears to be theoretical justification of an alternating optimization scheme for minimizing the resulting objective function (yet the optimization aspects of dealing with a sparse support regularizer are somewhat orthogonal to the current context). The empirical results are not very convincing since the dictionary size is relatively large compared to the dataset size; the gains with respect to l2 manifold regularizer are not consistent; and the gains using deep architectures to directly predict sparse codes are also modest and somewhat inconsistent. These points aside, the reviewers are overall enthusiastic about the paper and find it to be well written and complete.","I'd like to thank the authors for their detailed response to my questions. The most relevant contribution of this work is to including (and optimizing) the regularization function, not an approximation or surrogate.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","I'd like to thank the authors for their detailed response to my questions. The most relevant contribution of this work is to including (and optimizing) the regularization function, not an approximation or surrogate.", The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data . The authors also propose a fast encoding scheme for their proposed method .,the authors propose a support regularized version of sparse coding that takes into account the underlying manifold structure of the data.----------------The paper proposes a support regularized version of,0.173469387755102,0.010309278350515464,0.11224489795918366,0.8626623749732971,0.8156497478485107,0.8384976387023926
https://openreview.net/forum?id=HJWzXsKxx,The findings of applying sparsity in the backward gradients for training LSTMs is interesting. ----------------But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. ----------------Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger.,"The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.    Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc).     Paper would be strengthened by a better exploration of the problem.",The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient., The paper seems incomplete without the proper experimental justification . Only the validation loss is reported which is definitely insufficient . Proper testing results and commonly reported evaluation criterion needs to be included .,for training LSTMs is interesting. ---------------But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation ,0.2077922077922078,0.013157894736842105,0.15584415584415584,0.878515899181366,0.8307960629463196,0.8539898991584778
https://openreview.net/forum?id=HJWzXsKxx,"This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.----------------Minor note:--------The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.----------------The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.----------------While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?----------------At present this is an interesting technical report and I would like to see more detailed results in the future.","The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.    Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc).     Paper would be strengthened by a better exploration of the problem.","This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am", This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy . It will be much more convincing if a well known dataset and experiment set up are used .,.-------------The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific,0.24844720496894412,0.050314465408805034,0.1614906832298137,0.868649423122406,0.8295149207115173,0.8486312031745911
https://openreview.net/forum?id=HJWzXsKxx,"CONTRIBUTIONS--------When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.----------------NOVELTY--------Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.----------------MISSING CITATIONS--------Prior work has explored low-precision arithmetic for recurrent neural network language models:----------------Hubara et al, “Quantized Neural Networks: Training Neural Networks with--------Low Precision Weights and Activations”, https://arxiv.org/abs/1609.07061----------------Ott et al, “Recurrent Neural Networks With Limited Numerical Precision”, https://arxiv.org/abs/1608.06902 ----------------Low-precision arithmetic for recurrent networks promises to improve both training and inference efficiency. How does the proposed sparsification method compare with low-precision arithmetic? Are the ideas complementary?----------------EXPERIMENTS--------The main experimental result of the paper (Section 4.1) is that training LSTM language models with sparse gradients does not affect convergence or final performance (Figure 5). This result is promising, but I do not think that this single experiment is enough to prove the utility of the proposed method.----------------I also have some problems with this experiment. Plotting validation loss for character-level language modeling is not a standard way to report results; it is much more typical to report bits-per-character or perplexity on a held-out test set. These experiments also lack sufficient details for replication. What optimization algorithm, learning rate, and regularization were used? How were these hyperparameters chosen? Is the “truncated Wikipedia dataset” used for training the standard text8 dataset? In addition, the experiments do not compare with existing published results on this dataset.----------------In the OpenReview discussion, the authors remarked that the “The final validation loss for the sparsified model is [...] almost the same as the baseline.” Comparing validation loss at the end of training is not the proper way to compare models. From Figure 5, it is clear that all models achieve minimal validation loss after around 10k iterations, after which the validation losses increase, suggesting that the models have slightly overfit the training data by the end of training.----------------In Section 4.2 the authors claim to obtain similar experimental results with other network architectures, on other datasets (tiny-shakespeare and War and Peace), and for other tasks (image captioning and machine translation). However, the details and results of these experiments are not included in the paper, making it difficult to assess the utility of the proposed method and the significance of the results.----------------ENERGY EFFICIENCY AND TRAINING SPEED--------One of the main claims of the paper is that sparse gradients can be exploited in hardware to reduce the training speed and improve the energy efficiency of recurrent network training, but these benefits are neither quantified nor demonstrated experimentally. Even without actually implementing custom hardware, would it be possible to estimate the expected improvements in efficiency through simulation or other means? Such results would significantly strengthen the paper.----------------GRADIENT TRUNCATION AS REGULARIZATION--------In Figure 5 all models appear to reach a minimum validation loss at around 10k iterations and then overfit; at this point the Low model achieves even lower loss than the baseline. This is an interesting experimental result, but it is not discussed in the paper. Perhaps a low truncation threshold acts as a weak regularizer to prevent overfitting? Is this a general phenomenon of training recurrent networks with sparse gradients, or is it just a quirk of this particular experiment? This idea deserves more investigation, and could strengthen the paper.----------------SUMMARY--------The core idea of the paper (thresholding gradients to induce sparsity and improve efficiency of RNN training) is interesting and practically useful, if a bit incremental. Nevertheless with thorough and deliberate experiments quantifying the tradeoffs between task performance, training speed, and energy efficiency across a variety of tasks and datasets, this simple idea could be the core of a strong paper.----------------Unfortunately, as written the paper provides neither theoretical arguments nor convincing experimental results to justify the proposed method, and as such I do not believe the paper is ready for publication in its current form.----------------PROS--------- The proposed method is simple, and seems to be a promising direction for improving the speed of training recurrent networks.----------------CONS--------- No discussion of prior work on low-precision recurrent networks--------- Experimental results are not sufficient to validate the method--------- Many experimental details are missing--------- Results of key experiments (image captioning and machine translation) are missing","The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.    Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc).     Paper would be strengthened by a better exploration of the problem.",This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent," When training LSTMs, many intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin . rounding these small gradients to zero results in matrices with","80% sparsity during training, and that training character-level LSTM language models with this sparsification does not affect convergence or final performance (Figure 5). The authors argue that this sparsity could be exploited",0.22360248447204972,0.012578616352201259,0.12422360248447206,0.8542202115058899,0.8230987191200256,0.8383707404136658
https://openreview.net/forum?id=rJfMusFll,"This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning – training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.----------------While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. ----------------My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able to generate reasonable utterances on such a small dataset. Wen et al. (2016) are able to do this on a similarly small restaurant dataset, but this is mostly because they map directly from dialogue states to surface form, rather than some embedding representation of the context. Thus, it remains to be seen if the approaches in this paper also result in improvements when much more unsupervised data is available.----------------References:----------------Wen, Tsung-Hsien, Milica Gasic, Nikola Mrksic, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, David Vandyke, and Steve Young. ""A Network-based End-to-End Trainable Task-oriented Dialogue System."" arXiv preprint arXiv:1604.04562 (2016).","This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy.   The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.",This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model.," This paper extends neural conversational models into the batch reinforcement learning setting . The idea is that you can collect human scoring data for some responses from a dialogue model . However such scores are expensive. Thus, it is natural to use off-","for dialogue generation. This paper extends neural conversational models into the batch reinforcement learning setting. It is natural to use off-policy learning – training a base policy on unsupervised data, deploying that policy to collect human",0.15126050420168066,0.0,0.11764705882352942,0.8716914653778076,0.8246496915817261,0.8475183248519897
https://openreview.net/forum?id=rJfMusFll,"The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.--------The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on ""clarification regarding batch vs. online setting"").--------The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. ","This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy.   The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.",The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots. The approach is well motivated and the paper is well written.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots. The approach is well motivated and the paper is well written., The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots . The approach is well motivated and the paper is well written .,", and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on ""clarification regarding batch vs. online setting"").--------",0.3333333333333333,0.10169491525423728,0.18333333333333332,0.8869227170944214,0.848443329334259,0.8672563433647156
https://openreview.net/forum?id=rJfMusFll,"The paper discuss a ""batch"" method for RL setup to improve chat-bots.--------The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. ----------------I find the writing clear, and the algorithm a natural extension of the online version.----------------Below are some constructive remarks:--------- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:--------- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.--------- section 2.2:--------   sentence before last: s' is not defined. --------   last sentence: missing ""... in the stochastic case."" at the end.--------- Section 4.1 last paragraph: ""While Bot-1 is not significant ..."" => ""While Bot-1 is not significantly different from ML ...""","This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy.   The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR.","The paper discuss a ""batch"" method for RL setup to improve chat-bots. The authors provide nice overview of the RL setup they are using.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper discuss a ""batch"" method for RL setup to improve chat-bots. The authors provide nice overview of the RL setup they are using."," The paper discuss a ""batch"" method for RL setup to improve chat-bots . The authors provide nice overview of the RL setup they are using .",True,0.19130434782608696,0.035398230088495575,0.10434782608695653,0.8732953071594238,0.8263452649116516,0.8491718769073486
https://openreview.net/forum?id=S1xh5sYgx,"Strengths---------- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. ---------- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.---------- x50 less memory usage than AlexNet, keeping similar accuracy ---------- strong experimental results----------------Weaknesses----------Would be nice to test Sqeezenet on multiple tasks------------------lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the “by-pass” architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?","The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.","Sqeezenet is an interesting proposal for a smaller CNN architecture designed for embedded CNN applications. It has x50 less memory usage than AlexNet, keeping similar accuracy.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","Sqeezenet is an interesting proposal for a smaller CNN architecture designed for embedded CNN applications. It has x50 less memory usage than AlexNet, keeping similar accuracy."," Sqeezenet is an interesting proposal for a smaller CNN architecture designed for embedded CNN applications . It has x50 less memory usage than AlexNet, keeping similar accuracy . Weaknesses include lack of insights into what factors are responsible for",------------- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. ------------- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. --------,0.1956521739130435,0.0,0.10869565217391304,0.864524781703949,0.8478632569313049,0.8561129570007324
https://openreview.net/forum?id=S1xh5sYgx,"Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.----------------Pros: --------Achieves x50 less memory usage than AlexNet while keeping similar accuracy.----------------Cons & Questions:--------Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn’t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?","The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.",Achieves x50 less memory usage than AlexNet while keeping similar accuracy. Complex by-pass is like inception modules in GoogLeNet.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Achieves x50 less memory usage than AlexNet while keeping similar accuracy. Complex by-pass is like inception modules in GoogLeNet., SqueezeNet is composed of fire modules . It achieves x50 less memory usage than AlexNet while keeping similar accuracy . Complex by-pass is like inception modules in GoogLeNet and ResNet .,": The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.-------------Pro",0.20930232558139533,0.0,0.0930232558139535,0.8520781993865967,0.8318045139312744,0.8418192863464355
https://openreview.net/forum?id=S1xh5sYgx,"The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.----------------Since the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.----------------On the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.----------------Oh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.","The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method.","The Squeezenet paper came out in Feb 2016, and I read it with interest. The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. So, very nice results, definitely",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The Squeezenet paper came out in Feb 2016, and I read it with interest. The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. So, very nice results, definitely"," Squeezenet paper came out in Feb 2016, and I read it with interest . It has a series of engineering suggestions for how to save parameter memory for CNNs for object recognition . The suggestions make a lot of sense, and",.-------------------The Squeezenet paper came out in Feb 2016 and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition,0.23529411764705885,0.0,0.13725490196078433,0.853690505027771,0.8394035696983337,0.8464868068695068
https://openreview.net/forum?id=BJuysoFeg,"This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.------------------------Pros:----------------The method is very simple and easy to understand and apply.----------------The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.----------------The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.------------------------Cons:----------------There is little novelty -- the method is arguably too simple to be called a “method.” Rather, it’s the most straightforward/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what’s done in the Inception BN results in Table 1-2?)----------------The analysis in section 4.3.1 is superfluous except as a sanity check -- KL divergence between the distributions should be 0 when each distribution is shifted/scaled to N(0,1) by BN.----------------Section 3.3: it’s not clear to me what point is being made here.------------------------Overall, there’s not much novelty here, but it’s hard to argue that simplicity is a bad thing when the method is clearly competitive with or outperforming prior work on the standard benchmarks (in a domain adaptation tradition that started with “Frustratingly Easy Domain Adaptation”).  If accepted, Sections 4.3.1 and 3.3 should be removed or rewritten for clarity for a final version.","Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? ","This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. The method is arguably too simple to be called a “method.” Rather, it’s the most straightforward/intuitive",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. The method is arguably too simple to be called a “method.” Rather, it’s the most straightforward/intuitive", This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain . The method is arguably too simple to be called a “method.” It’s the most straightforward/intuitive approach when,a simple domain adaptation technique in which batch normalization is performed separately in each domain.---------------Pros:----------------The method is very simple and easy to understand and apply.------------,0.14973262032085563,0.021621621621621623,0.12834224598930483,0.8473427295684814,0.8144262433052063,0.8305585384368896
https://openreview.net/forum?id=BJuysoFeg,"Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.----------------This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.----------------Overall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:----------------1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). ----------------2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).","Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? ","This paper proposes a simple trick turning batch normalization into a domain adaptation technique. My main concerns are the following: Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper proposes a simple trick turning batch normalization into a domain adaptation technique. My main concerns are the following: Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution", The paper proposes a simple trick turning batch normalization into a domain adaptation technique . My main concerns are that the paper is composed in such a way to make the main contribution less obvious . The authors are using much stronger base CNN .,.-------------------This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as part of the ,0.2061855670103093,0.020833333333333332,0.1134020618556701,0.8544802665710449,0.8149732947349548,0.8342592716217041
https://openreview.net/forum?id=BJuysoFeg,"Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.----------------Detailed comments:----------------Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-SNE embedding. This is not the particular feature of batch normalization: visualizing a set of intermediate features with AlexNet and one gets the same results. So the premise in section 3.1 is not accurate.----------------Section 3.3: I have the same concern as the other reviewer. It seems to be quite detatched from the general idea of AdaBN. Equation 2 presents an obvious argument that the combined BN-fully_connected layer forms a linear transform, which is true in the original BN case and in this case as well. I do not think it adds much theoretical depth to the paper. (In general the novelty of this paper seems low)----------------Experiments:----------------- section 4.3.1 is not an accurate measure of the ""effectiveness"" of the proposed method, but a verification of a simple fact: previously, we normalize the source domain features into a Gaussian distribution. the proposed method is explicitly normalizing the target domain features into the same Gaussian distribution as well. So, it is obvious that the KL divergence between these two distributions are closer - in fact, one is *explicitly* making them close. However, this does not directly correlate to the effectiveness of the final classification performance.----------------- section 4.3.2: the sensitivity analysis is a very interesting read, as it suggests that only a very few number of images are needed to account for the domain shift in the AdaBN parameter estimation. This seems to suggest that a single ""whitening"" operation is already good enough to offset the domain bias (in both cases shown, a single batch is sufficient to recover about 80% of the performance gain, although I cannot get data for even smaller number of examples from the figure). It would thus be useful to have a comparison between these approaches, and also a detailed analysis of the effect from each layer of the model - the current analysis seems a bit thin.","Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? ",This is an interesting paper which shows empirical performance improvement over baselines. My main concern with the paper is regarding its technical depth. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This is an interesting paper which shows empirical performance improvement over baselines. My main concern with the paper is regarding its technical depth. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.," This paper shows empirical performance improvement over baselines . It would be useful to have a comparison between these approaches, and a detailed analysis of the effect from each layer of the model .","of the original batch normalization paper. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation",0.1761658031088083,0.031413612565445025,0.10362694300518135,0.86081463098526,0.8120130896568298,0.8357020020484924
https://openreview.net/forum?id=Sk2iistgg,"The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. ----------------The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first and then motivating the need for a regularizer (which brings dimensionality reduction techniques into the picture) may be a more clear flow in my opinion. ----------------The motivation behind relaxation of rank() in Eq 1 to nuclear-norm in Eq 2 is not clear to me in this setting. The relaxation does not yield a convex problem over S,C (Eq 5) and also increases the computations (Algo 2 needs to do full SVD of K(S) every time). The authors should discuss pros/cons over the alternate approach that fixes the rank of C (which can be selected using cross-validation, in the same way as  is selected), leaving just the first two terms in Eq 5. For this simpler objective, an interesting question to ask would be -- are there kernel functions for which it can solved in a scalable manner? ----------------The proposed alternating optimization approach in the current form is computationally intensive and seems hard to scale to even moderate sized data -- in every iteration one needs to compute the kernel matrix over S and perform full SVD over the kernel matrix (Algo 2). Empirical evaluations are also not extensive -- (i) the dataset used for feature imputation is old and non-standard, (ii) for structure estimation from motion on CMU dataset, the paper only compares with linear low-rank regularization, (iii) there is no comment/study on the convergence of the alternating procedure (Algo 1). ","There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews.",The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. The motivation behind,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. The motivation behind," A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace . Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation,",. The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data lie near a low dimensional subspace in,0.23913043478260868,0.022222222222222227,0.1956521739130435,0.8268957734107971,0.8357450366020203,0.8312968015670776
https://openreview.net/forum?id=Sk2iistgg,"This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating “causal factors”, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  ----------------- Not sure what the authors mean by “causal factors”. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition/discussion.----------------- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each----------------- Authors outline two key disadvantages of the existing KPCA approach. The first one, that of low-dimensional manifold assumption not holding exactly, has received lots of attention in the machine learning literature. It is common to assume that the data lies near a low-dimensional manifold rather than on a low-dimensional manifold. Second disadvantage is somewhat unclear as finding “a data point (pre-image) corresponding to each projection in the input space” is not a standard step in KPCA. ----------------- On page 3, you never define , , . Clearly, they cannot be cartesian products. I have to assume that notation somehow implies N-tuples. ----------------- On page 3, Section 2,  and  are sets. What do you mean by ----------------- On page 5,  is never defined. ----------------- Experiments: None of the standard algorithms for matrix completion such as OptSpace or SVT were considered ----------------- Experiments: There is no comparison with alternate existing approaches for Non-rigid structure from motion.  ----------------- Proof of the main result Theorem 3.1: To get from (16) to (17) using the Holder inequality (as stated) one would end up with a term that involves sum of fourth powers of weights w_{ij}. Why would they equal to one using the orthonormal constraints? It would be useful to give more details here, as I don’t see how the argument goes through at this point. ","There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews.","This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating “causal factors’, to nonlinear dimension",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating “causal factors’, to nonlinear dimension", This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective . The writing is not clear. The focus keeps shifting from estimating “causal factors” to nonlinear dimensionality,of Kernel PCA with rank constraints incorporated as regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating “causal factors” to nonlinear dimensionality reduction to Kern,0.26666666666666666,0.04545454545454545,0.17777777777777776,0.8414719700813293,0.8562397360801697,0.8487915992736816
https://openreview.net/forum?id=Sk2iistgg,"This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. --------The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. ----------------The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. --------The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.----------------The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.------------------------The paper contains errors:----------------- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the proposed approach consists of iteratively solving iterativey a set of closed form updates  and Levenberg-Marquard optimizationd. This is not any more closed form!----------------- In the same paragraph (and later in the text) the authors claim that the proposed approach can be trivially generalized to incorporate other cost functions. This is not true, as in general there will be no more inner loop closed form updates and the authors will need to solve a much more complex optimization problem. ----------------- The third paragraph of section 2 claims that this paper presents a novel energy minimization framework to solve problems of the general form of eq. (2). However, this is not what the authors solve at the end. They solve a different problem that has been subject to at least two relaxations. It is not clear how solving for a local optima of this double relaxed problem is related to the original problem they want to solve. ----------------- The paper says that Geiger et al defined non linearities on a latent space of pre-defined dimensionality. This is wrong. This paper discovers the dimensionality of the latent space by means of a regularizer that encourages the singular values to be sparse. Thus, it does not have a fixed dimensionality, the latent space is just bounded to be smaller or equal than the dimensionality of the original space. ------------------------It is not clear to me why the author say for LVMs such as GPLVM that ""the latent space is learned a priority with clean training data"". One can use different noise models within the GP framework. Furthermore, the proposed approach assumes Gaussian noise (see eq. 6), which is also the trivial case for GP-based LVMs.  ------------------------It is not clear what the authors mean in the paper by ""pre-training"" or saying that techniques do not have a training phase. KPCA is trained via a closed-form update, but there is still training.  ","There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews.",This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets., This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space . The paper contains errors and the experimental evaluation is not convincing . The authors do not address the out-of-sample,a set of closed form updates and Levenberg-Marquard optimizationd. This is not any more closed form! --------------The authors propose a closed form solution to robust KPCA. This is simply,0.3368421052631579,0.1935483870967742,0.2736842105263158,0.8636155128479004,0.8652368783950806,0.8644254803657532
https://openreview.net/forum?id=S1jmAotxg,"This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.----------------There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic ""black-box"" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for the SB-VAE is drawing attention to the use of the Kumaraswamy distribution which hasn't been popular in ML.----------------Although the paper is in general well structured, I found it confusing at parts. I think the major source of confusion comes from the fact that the model specification and model inference are discussed in a somehow mixed manner. The pre-review questions clarified most parts.----------------I have two main concerns regarding the methodology and motivation of this paper. Firstly, conditioning the model directly on the stick-breaking weights seems a little odd. I initially thought that there was some mixture probabilistic model involved, but this is not the case. To be fair, the authors discuss about this issue (which became clearer to me after the pre-review questions), and explain that they're investigating the apparently challenging problem of using a base distribution G_0. The question is whether their relaxation is still useful. From the experiments it seems that the method is at least competitive, so the answer is yes. Hopefully an extension will come in the future, as the authors mention.----------------The second concern is about the motivation of this method. It seems that the paper fails to clearly explain in a convincing way why it is beneficial to reformulate the VAE as a SB-VAE. I understand that the non-parametric property induced by the prior might result in better capacity control, however I feel that this advantage (and potentially others which are still unclear to me) is not sufficiently explained and demonstrated. Perhaps some comparison with a dropout approach or a more thorough discussion related to dropout would make this clearer.----------------Overall, I found this to be an interesting paper, it would be a good fit for ICLR.","This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.","This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process.",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process."," This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality . This is achieved by using an inherently infinite prior, the stick-breaking process .","inference for the SB-VAE. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as",0.24657534246575344,0.056338028169014086,0.1917808219178082,0.8484588265419006,0.8606996536254883,0.8545354604721069
https://openreview.net/forum?id=S1jmAotxg,"The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.--------After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are ""used"" (which actually enable backpropagation) but the latent variables are parametrized differently (into ) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.--------With respect to the Figure 5b showing the decoder input weights: component collapsing probably does not have the same effect as Gaussian prior.  is positive therefore having a very small average value might mean that its value is close to zero most of the time, not requiring any update on the weight. For the standard Gaussian prior, component collapsing means having a very noisy input with no signal involved, which forces the decoder to shut down this channel, i.e. have small incoming weights from this collapsed variable.--------Adding a histogram of the latent variables in addition to that might help decide if the associated weights are relatively large because they are actually used or if it's because the inputs are zero anyway.--------The semi-supervised results are better than a weaker version of the model used in (Kingma et al., 2014), but as to have a fairer comparison, the results should be compared with the M1+M2 model in that paper, even if that requires also using two VAEs.","This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.","The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the",facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}","The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the", The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process . The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality . The way all these latent variables are used does,.------------The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stoch,0.34210526315789475,0.05405405405405405,0.2631578947368421,0.8531047105789185,0.8610104918479919,0.8570393919944763
https://openreview.net/forum?id=S1jmAotxg,"Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.----------------In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.----------------Comments:--------- sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.--------- sec 2.2: two comma's--------- text flow eq 6: please refer to appendix with the closed-form KL divergence--------- ""The v's are sampled via"" => ""In the posterior, the v's are sampled via"". It's not clear you're talking about the posterior here, instead of the prior.--------- The last paragraph of section 4 is great.--------- Sec 7.1: ""Density estimation"" => Technically you're also doing mass estimation.--------- Sec 7.1: 100 IS samples is a bit on the low side. --------- Figure 3(f). Interesting that k-NN works so well on raw pixels.","This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference.",Stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting. The paper is really well written.,facebook/bart-large-cnn,facebook/bart-large-cnn,"{'facebook/bart-large-cnn': -1.0986123085021973, 'sshleifer/distilbart-cnn-12-6': -1.0986123085021973, 'Falconsai/text_summarization': -1.0986123085021973}","{'facebook/bart-large-cnn': 0.0, 'sshleifer/distilbart-cnn-12-6': 0.0, 'Falconsai/text_summarization': 0.0}",Stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting. The paper is really well written.," Stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood . The paper is really well written .",": This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and",0.22580645161290322,0.06666666666666667,0.16129032258064516,0.8492997884750366,0.8603401184082031,0.8547843098640442
