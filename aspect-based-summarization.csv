id,best_rsa,best_base,initial_consensuality_scores,consensuality_scores,language_model_proba,gold
https://openreview.net/forum?id=B1TTpYKgx,"['The paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.'
 'The paper studies the expressive power of deep neural networks under various related measures of expressivity.'
 'The paper is a little hard to follow. The motivations are not clear in the introduction and the definitions are not clear.']","['The paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.'
 'The paper studies the expressive power of deep neural networks under various related measures of expressivity.'
 'The paper is a little hard to follow. The motivations are not clear in the introduction and the definitions are not clear.']","The paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.    0.374134
The paper studies the expressive power of deep neural networks under various related measures of expressivity.                0.614856
The paper is a little hard to follow. The motivations are not clear in the introduction and the definitions are not clear.    0.222464
dtype: float32","The paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.    1.098603
The paper studies the expressive power of deep neural networks under various related measures of expressivity.                1.098612
The paper is a little hard to follow. The motivations are not clear in the introduction and the definitions are not clear.    1.098566
dtype: float32","{""The paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks."":{""This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.----------------Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.----------------There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.--------For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.----------------The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.----------------Some findings seem trivial.----------------detailed comments----------------p2 ----------------\""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide"":-3.2276053429,""SUMMARY --------This paper studies the expressive power of deep neural networks under various related measures of expressivity. --------It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). --------The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. ----------------PROS --------The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. ----------------CONS --------The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. ----------------COMMENTS--------- The paper is a bit long (especially the appendix) and seems to"":-5.139275074,""Summary of the paper:----------------Authors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the \u2018trajectory length\u2019 of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : \u2018transitions\u2019,\u2019activation patterns \u2019 and \u2018Dichotomies\u2019. --------As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.----------------Clarity:----------------The  paper is a little hard to follow, since  the motivations are not clear in the"":-4.9677619934},""The paper studies the expressive power of deep neural networks under various related measures of expressivity."":{""This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.----------------Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.----------------There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.--------For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.----------------The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.----------------Some findings seem trivial.----------------detailed comments----------------p2 ----------------\""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide"":-7.5507049561,""SUMMARY --------This paper studies the expressive power of deep neural networks under various related measures of expressivity. --------It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). --------The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. ----------------PROS --------The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. ----------------CONS --------The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. ----------------COMMENTS--------- The paper is a bit long (especially the appendix) and seems to"":-5.2526574135,""Summary of the paper:----------------Authors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the \u2018trajectory length\u2019 of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : \u2018transitions\u2019,\u2019activation patterns \u2019 and \u2018Dichotomies\u2019. --------As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.----------------Clarity:----------------The  paper is a little hard to follow, since  the motivations are not clear in the"":-8.1415557861},""The paper is a little hard to follow. The motivations are not clear in the introduction and the definitions are not clear."":{""This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks.----------------Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.----------------There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.--------For instance the trajectory length seems a very discutable measure of expressivity. The only justification given for why it should be a good measure of expressivity is proportionality with other measures of expressivity in the specific case of random networks.----------------The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.----------------Some findings seem trivial.----------------detailed comments----------------p2 ----------------\""Much of the work examining achievable functions relies on unrealistic architectural assumptions such as layers being exponentially wide"":-3.233934164,""SUMMARY --------This paper studies the expressive power of deep neural networks under various related measures of expressivity. --------It discusses how these measures relate to the `trajectory length', which is shown to depend exponentially on the depth of the network, in expectation (at least experimentally, at an intuitive level, or theoretically under certain assumptions). --------The paper also emphasises the importance of the weights in the earlier layers of the network, as these have a larger influence on the represented classes of functions, and demonstrates this in an experimental setting. ----------------PROS --------The paper further advances on topics related to the expressive power of feedforward neural networks with piecewise linear activation functions, in particular elaborating on the relations between various points of view. ----------------CONS --------The paper further advances and elaborates on interesting topics, but to my appraisal it does not contribute significantly new aspects to the discussion. ----------------COMMENTS--------- The paper is a bit long (especially the appendix) and seems to"":-3.4241185188,""Summary of the paper:----------------Authors study in this paper quantities related to the expressivity of neural networks.The analysis is done for a random network. authors define the \u2018trajectory length\u2019 of a one dimensional trajectory as the length of the trajectory as the points (in a m- dimensional space) are embedded by layers of the network. They provide growth factors as function of hidden units k, and number of layers d.  the growth factor is exponential in the number of layers. Authors relates this trajectory length to authors quantities : \u2018transitions\u2019,\u2019activation patterns \u2019 and \u2018Dichotomies\u2019. --------As a consequence of this study authors suggest that training only  earlier layers in the network  leads higher accuracy then just training later layers. Experiments are presented on MNIST and CIFAR10.----------------Clarity:----------------The  paper is a little hard to follow, since  the motivations are not clear in the"":-1.9712790251}}","While the reviewers saw some value in your contribution, there were also serious issues, so the paper does not reach the acceptance threshold."
https://openreview.net/forum?id=B1jnyXXJx,"['This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics.'
 'A paper proposes a novel method for accelerating optimization near saddle points. The idea is potentially very valuable, but needs more rigorous comparison.'
 ' network.']","['This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics.'
 'A paper proposes a novel method for accelerating optimization near saddle points. The idea is potentially very valuable, but needs more rigorous comparison.'
 ' network.']","This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics.                                 0.857540
A paper proposes a novel method for accelerating optimization near saddle points. The idea is potentially very valuable, but needs more rigorous comparison.    0.282735
 network.                                                                                                                                                       0.150767
dtype: float32","This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics.                                 1.098612
A paper proposes a novel method for accelerating optimization near saddle points. The idea is potentially very valuable, but needs more rigorous comparison.    1.097789
 network.                                                                                                                                                       1.072273
dtype: float32","{""This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics."":{""Summary:--------This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be pushed away from saddle points due to the saddle point being positively changed as well. Authors of the paper show results over several different datasets.----------------Overview of the Review:--------    Pros:--------        - The idea is very interesting.--------        - The diverse set of results on different datasets.--------    Cons:--------        - The justification is not strong enough.--------        - The paper is not well-written.--------        - Experiments are not convincing enough.----------------Criticisms:----------------I liked the idea and the intuitions coming from the paper. However, I think"":-1.8995443583,""This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.----------------On the surface, the proposed method seems extremely close to momentum. It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point. The illustration of better convergence on the toy saddle example is not what I mean here\u2014optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from\u2014and clearly qualitatively better than\u2014momentum.----------------Another way of getting at the relationship to momentum would be to try to find a form for R_t(f) that yields"":-5.0828781128,""The research direction taken by this paper is of great interest. --------However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). --------\""Throughout this paper the selection of hyper-parameters was kept rather simple.\"" but the momentum term of CPN is set to 0.95 --------and not 0.9 as in all\/most optimizers CPN is compared to. I suppose that the positive effect of CPN (if any) is mostly due to its momentum term."":-6.0707683563},""A paper proposes a novel method for accelerating optimization near saddle points. The idea is potentially very valuable, but needs more rigorous comparison."":{""Summary:--------This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be pushed away from saddle points due to the saddle point being positively changed as well. Authors of the paper show results over several different datasets.----------------Overview of the Review:--------    Pros:--------        - The idea is very interesting.--------        - The diverse set of results on different datasets.--------    Cons:--------        - The justification is not strong enough.--------        - The paper is not well-written.--------        - Experiments are not convincing enough.----------------Criticisms:----------------I liked the idea and the intuitions coming from the paper. However, I think"":-3.9174265862,""This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.----------------On the surface, the proposed method seems extremely close to momentum. It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point. The illustration of better convergence on the toy saddle example is not what I mean here\u2014optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from\u2014and clearly qualitatively better than\u2014momentum.----------------Another way of getting at the relationship to momentum would be to try to find a form for R_t(f) that yields"":-2.7827804089,""The research direction taken by this paper is of great interest. --------However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). --------\""Throughout this paper the selection of hyper-parameters was kept rather simple.\"" but the momentum term of CPN is set to 0.95 --------and not 0.9 as in all\/most optimizers CPN is compared to. I suppose that the positive effect of CPN (if any) is mostly due to its momentum term."":-4.7595901489},"" network."":{""Summary:--------This paper proposes a regularizer that is claimed to help escaping from the saddle points. The method is inspired from physics, such that thinking of the optimization process is moving a positively charged particle would over the error surface which would be pushed away from saddle points due to the saddle point being positively changed as well. Authors of the paper show results over several different datasets.----------------Overview of the Review:--------    Pros:--------        - The idea is very interesting.--------        - The diverse set of results on different datasets.--------    Cons:--------        - The justification is not strong enough.--------        - The paper is not well-written.--------        - Experiments are not convincing enough.----------------Criticisms:----------------I liked the idea and the intuitions coming from the paper. However, I think"":-79.7387466431,""This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.----------------On the surface, the proposed method seems extremely close to momentum. It would be very valuable to think of a clear diagram illustrating how it differs from momentum and why it might be better near a saddle point. The illustration of better convergence on the toy saddle example is not what I mean here\u2014optimization speed comparisons are always difficult due to the many details and hyper parameters involved, so seeing it work faster in one specific application is not as useful as a conceptual diagram which shows a critical case where CPN will behave differently from\u2014and clearly qualitatively better than\u2014momentum.----------------Another way of getting at the relationship to momentum would be to try to find a form for R_t(f) that yields"":-81.162940979,""The research direction taken by this paper is of great interest. --------However, the empirical results are not great enough to pay for the weaknesses of the proposed approach (see Section 6). --------\""Throughout this paper the selection of hyper-parameters was kept rather simple.\"" but the momentum term of CPN is set to 0.95 --------and not 0.9 as in all\/most optimizers CPN is compared to. I suppose that the positive effect of CPN (if any) is mostly due to its momentum term."":-80.4601593018}}","The paper proposes a method for accelerating optimization near saddle points when training deep neural networks. The idea is to repel the current parameter vector from a running average of recent parameter values. The proposed method is shown to optimize faster than a variety of other methods in a variety of datasets and architectures.    The author presents a fresh idea in the area of stochastic optimization for deep neural networks. However the paper doesn't quite appear to be above the Accept bar, due to remaining doubts about the thoroughness of the experiments.We therefore invite this paper for presentation at the Workshop track."
https://openreview.net/forum?id=BJ3filKll,"['Paper presents analysis of ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Authors focus on what they call ""monotonic chains of linear segments""'
 'The paper looks at the ability of neural networks to represent low dimensional manifolds efficiently in a neural network. It also gives a bound on the number of parameters required to do so.'
 'This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in neural networks with two hidden layers.']","['Paper presents analysis of ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Authors focus on what they call ""monotonic chains of linear segments""'
 'The paper looks at the ability of neural networks to represent low dimensional manifolds efficiently in a neural network. It also gives a bound on the number of parameters required to do so.'
 'This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in neural networks with two hidden layers.']","Paper presents analysis of ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Authors focus on what they call ""monotonic chains of linear segments""    0.628178
The paper looks at the ability of neural networks to represent low dimensional manifolds efficiently in a neural network. It also gives a bound on the number of parameters required to do so.               0.346022
This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in neural networks with two hidden layers.                                   0.481353
dtype: float32","Paper presents analysis of ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Authors focus on what they call ""monotonic chains of linear segments""    1.098612
The paper looks at the ability of neural networks to represent low dimensional manifolds efficiently in a neural network. It also gives a bound on the number of parameters required to do so.               1.098608
This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in neural networks with two hidden layers.                                   1.098610
dtype: float32","{""Paper presents analysis of ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Authors focus on what they call \""monotonic chains of linear segments\"""":{""The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \""monotonic chains of linear segments\"", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.----------------While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \""compatible\"". In particular, I have three main concerns with respect to the results presented in this paper:----------------(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis"":-0.7950393558,""Summary:--------In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. --------They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.----------------They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. ----------------Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.----------------Comments:----------------The direction of investigation in the paper (looking at what"":-3.3827929497,""SUMMARY --------This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. ----------------PROS --------Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. ----------------CONS --------The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). ----------------COMMENTS --------It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. --------Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. ----------------MINOR COMMENTS --------- Figure 1 could be referenced first in the text.  --------- ``Color coded'' where the color codes what? --------- Thank you"":-3.4373924732},""The paper looks at the ability of neural networks to represent low dimensional manifolds efficiently in a neural network. It also gives a bound on the number of parameters required to do so."":{""The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \""monotonic chains of linear segments\"", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.----------------While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \""compatible\"". In particular, I have three main concerns with respect to the results presented in this paper:----------------(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis"":-3.2981414795,""Summary:--------In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. --------They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.----------------They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. ----------------Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.----------------Comments:----------------The direction of investigation in the paper (looking at what"":-1.7824909687,""SUMMARY --------This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. ----------------PROS --------Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. ----------------CONS --------The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). ----------------COMMENTS --------It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. --------Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. ----------------MINOR COMMENTS --------- Figure 1 could be referenced first in the text.  --------- ``Color coded'' where the color codes what? --------- Thank you"":-3.7611382008},""This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in neural networks with two hidden layers."":{""The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. Specifically, the paper focuses on what the authors call \""monotonic chains of linear segments\"", which are essentially sets of intersecting tangent planes. The paper presents a construction that efficiently models such manifolds in a deep net, and presents a basic error analysis of the resulting construction.----------------While the presented results are novel to the best of my knowledge, they are hardly surprising (1) given what we already know about the representational power of deep networks and (2) given that the study selects a deep network architecture and a data structure that are very \""compatible\"". In particular, I have three main concerns with respect to the results presented in this paper:----------------(1) In the last decade, there has been quite a bit of work on learning data representations from sets of local tangent planes. Examples that spring to mind are local tangent space analysis"":-5.9223780632,""Summary:--------In this paper, the authors look at the ability of neural networks to represent low dimensional manifolds efficiently e.g. embed them into a lower dimensional Euclidian space. --------They define a class of manifolds, monotonic chains (affine spaces that intersect, with hyperplanes separating monotonic intervals of spaces) and give a construction to embed such a chain with a neural network with one hidden layer.----------------They also give a bound on the number of parameters required to do so, and examine what happens when the manifold is noisy. ----------------Experiments involve looking at embedding synthetic data from a monotonic chain using a distance preservation loss. This experiment supports the theoretical bound on number of parameters needed to embed the monotonic chain. Another experiment varies the elevation and azimuth of of faces, which are known to lie on a monotonic chain, on a regression loss.----------------Comments:----------------The direction of investigation in the paper (looking at what"":-5.287006855,""SUMMARY --------This paper discusses how data from a special type of low dimensional structure (monotonic chain) can be efficiently represented in terms of neural networks with two hidden layers. ----------------PROS --------Interesting, easy to follow view on some of the capabilities of neural networks, highlighting the dimensionality reduction aspect, and pointing at possible directions for further investigation. ----------------CONS --------The paper presents a construction illustrating certain structures that can be captured by a network, but it does not address the learning problem (although it presents experiments where such structures do emerge, more or less). ----------------COMMENTS --------It would be interesting to study the ramifications of the presented observations for the case of deep(er) networks. --------Also, to study to what extent the proposed picture describes the totality of functions that are representable by the networks. ----------------MINOR COMMENTS --------- Figure 1 could be referenced first in the text.  --------- ``Color coded'' where the color codes what? --------- Thank you"":-3.4380989075}}","There is consensus among the reviewers that the paper presents an interesting and novel direction of study. Having said that, there also appears to be a sense that the proposed construction can be studied in more detail: in particular, (1) an average-case analysis is essential as the worst-case bounds appear extremely loose and (2) the learning problem needs to be addressed in more detail. Nevertheless, this paper deserves to appear at the conference."
https://openreview.net/forum?id=BJ46w6Ule,"['The paper addresses the problem of learning compact binary data representations. The related work section does not exist. There are no derivations provided.'
 'The goal of this paper is to learn a collection of experts that are individually-----meaningful. While the results seem promising, the paper exposition needs significant improvement.'
 ""The experiments are only illustrative. They don't compare with other methods (such as an RBM or VAE) nor do they give any quantitative results.""]","['The paper addresses the problem of learning compact binary data representations. The related work section does not exist. There are no derivations provided.'
 'The goal of this paper is to learn a collection of experts that are individually-----meaningful. While the results seem promising, the paper exposition needs significant improvement.'
 ""The experiments are only illustrative. They don't compare with other methods (such as an RBM or VAE) nor do they give any quantitative results.""]","The paper addresses the problem of learning compact binary data representations. The related work section does not exist. There are no derivations provided.                              0.898410
The goal of this paper is to learn a collection of experts that are individually-----meaningful. While the results seem promising, the paper exposition needs significant improvement.    0.824958
The experiments are only illustrative. They don't compare with other methods (such as an RBM or VAE) nor do they give any quantitative results.                                           0.002243
dtype: float32","The paper addresses the problem of learning compact binary data representations. The related work section does not exist. There are no derivations provided.                              1.098612
The goal of this paper is to learn a collection of experts that are individually-----meaningful. While the results seem promising, the paper exposition needs significant improvement.    1.098612
The experiments are only illustrative. They don't compare with other methods (such as an RBM or VAE) nor do they give any quantitative results.                                           1.097754
dtype: float32","{""The paper addresses the problem of learning compact binary data representations. The related work section does not exist. There are no derivations provided."":{""The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published."":-2.8887503147,""The goal of this paper is to learn \u201c a collection of experts that are individually--------meaningful and that have disjoint responsibilities.\u201d Unlike a standard mixture model, they \u201cuse a different mixture for each dimension d.\u201d While the results seem promising, the paper exposition needs significant improvement.----------------Comments:----------------The paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.----------------The subsequent exposition is not very clear. There are assertions made with no justification, e.g. \u201cthe experts only have a small variance for some subset of the variables while the variance of the other variables is large.\u201d ----------------Since you\u2019re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.----------------The horse data set is quite small with respect to the feature"":-6.6350421906,""This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.--------I find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?--------We also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: http:\/\/www.cs.toronto.edu\/~hinton\/absps\/tr00-004.pdf--------I tried to derive"":-6.831507206},""The goal of this paper is to learn a collection of experts that are individually-----meaningful. While the results seem promising, the paper exposition needs significant improvement."":{""The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published."":-4.3057789803,""The goal of this paper is to learn \u201c a collection of experts that are individually--------meaningful and that have disjoint responsibilities.\u201d Unlike a standard mixture model, they \u201cuse a different mixture for each dimension d.\u201d While the results seem promising, the paper exposition needs significant improvement.----------------Comments:----------------The paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.----------------The subsequent exposition is not very clear. There are assertions made with no justification, e.g. \u201cthe experts only have a small variance for some subset of the variables while the variance of the other variables is large.\u201d ----------------Since you\u2019re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.----------------The horse data set is quite small with respect to the feature"":-0.8247090578,""This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.--------I find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?--------We also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: http:\/\/www.cs.toronto.edu\/~hinton\/absps\/tr00-004.pdf--------I tried to derive"":-4.1834926605},""The experiments are only illustrative. They don't compare with other methods (such as an RBM or VAE) nor do they give any quantitative results."":{""The paper addresses the problem of learning compact binary data representations. I have a hard time understanding the setting and the writing of the paper is not making it any easier. For example I can't find a simple explanation of the problem and I am not familiar with these line of research. I read all the responses provided by authors to reviewer's questions and re-read the paper again and I still do not fully understand the setting and thus can't really evaluate the contributions of these work. The related work section does not exist and instead the analysis of the literature is somehow scattered across the paper. There are no derivations provided. Statements often miss references, e.g. the ones in the fourth paragraph of Section 3. This makes me conclude that the paper still requires significant work before it can be published."":-3.9683964252,""The goal of this paper is to learn \u201c a collection of experts that are individually--------meaningful and that have disjoint responsibilities.\u201d Unlike a standard mixture model, they \u201cuse a different mixture for each dimension d.\u201d While the results seem promising, the paper exposition needs significant improvement.----------------Comments:----------------The paper jumps in with no motivation at all. What is the application, or even the algorithm, or architecture that this is used for? This should be addressed at the beginning.----------------The subsequent exposition is not very clear. There are assertions made with no justification, e.g. \u201cthe experts only have a small variance for some subset of the variables while the variance of the other variables is large.\u201d ----------------Since you\u2019re learning both the experts and the weights, can this be rephrased in terms of dictionary learning? Please discuss the relevant related literature.----------------The horse data set is quite small with respect to the feature"":-3.9831323624,""This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen instead of the usual logarithmic opinion pool of a PoE.--------I find the paper very unclear. I tried to find a proper definition of the joint model p(x,z) but could not extract this from the text. The proposed \u201cEM-like\u201d algorithm should then also follow directly from this definition. At this point I do not see if such as definition even exists. In other words, is there is an objective function on which the iterates of the proposed algorithm are guaranteed to improve on the train data?--------We also note that the \u201cproduct of unifac models\u201d from Hinton tries to do something very similar where only a subset of the experts will get activated to generate the input: http:\/\/www.cs.toronto.edu\/~hinton\/absps\/tr00-004.pdf--------I tried to derive"":-3.8362309933}}",This paper is about learning distributed representations. All reviewers agreed that the first draft was not clear enough for acceptance.    Reviewer time is limited and a paper that needed a complete overhaul after the reviews were written is not going to get the same consideration as a paper that was well-drafted from the beginning.    It's still the case that it's unclear from the paper how the learning updates or derived. The results are not visually impressive in themselves. It's also still the case that more is needed to demonstrate that this direction is promising compared to other approaches to representation learning.
https://openreview.net/forum?id=BJAA4wKxg,"[""System described works comparably to bi-directional LSTM baseline for NMT. Uses two stacked CNN's (one for each of encoding and decoding) for translation.""
 'Convnets for NMT encoders can be competitive to RNNs, says reviewer. Authors present convincing set of results over many translation tasks.'
 'The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation.']","[""System described works comparably to bi-directional LSTM baseline for NMT. Uses two stacked CNN's (one for each of encoding and decoding) for translation.""
 'Convnets for NMT encoders can be competitive to RNNs, says reviewer. Authors present convincing set of results over many translation tasks.'
 'The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation.']","System described works comparably to bi-directional LSTM baseline for NMT. Uses two stacked CNN's (one for each of encoding and decoding) for translation.                0.821968
Convnets for NMT encoders can be competitive to RNNs, says reviewer. Authors present convincing set of results over many translation tasks.                               0.709902
The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation.    0.741442
dtype: float32","System described works comparably to bi-directional LSTM baseline for NMT. Uses two stacked CNN's (one for each of encoding and decoding) for translation.                1.098612
Convnets for NMT encoders can be competitive to RNNs, says reviewer. Authors present convincing set of results over many translation tasks.                               1.098612
The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation.    1.098612
dtype: float32","{""System described works comparably to bi-directional LSTM baseline for NMT. Uses two stacked CNN's (one for each of encoding and decoding) for translation."":{""The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.----------------Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).----------------The experimental results are well reported in detail.----------------One or two figures would definitely be required to help clarify the architecture.----------------This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in"":-0.8791211843,""This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.----------------One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as"":-4.2747421265,""The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. ----------------Apart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.--------The empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. ----------------The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. ----------------My main concern is whether or not the paper is appropriate for ICLR, because the contribution"":-4.2906522751},""Convnets for NMT encoders can be competitive to RNNs, says reviewer. Authors present convincing set of results over many translation tasks."":{""The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.----------------Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).----------------The experimental results are well reported in detail.----------------One or two figures would definitely be required to help clarify the architecture.----------------This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in"":-5.7608804703,""This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.----------------One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as"":-3.01187253,""The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. ----------------Apart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.--------The empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. ----------------The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. ----------------My main concern is whether or not the paper is appropriate for ICLR, because the contribution"":-6.1059670448},""The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation."":{""The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable.----------------Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The use of CNN's for translation has been attempted previously (as described by the authors), but presumably it is the authors' combination of various architectural choices (attention, position embeddings, etc) that make the present system competitive with RNN's, whereas earlier attempts were not. They describe system's sensitivity to some of these choices (e.g. experiments to choose appropriate number of layers in each of the CNN's).----------------The experimental results are well reported in detail.----------------One or two figures would definitely be required to help clarify the architecture.----------------This paper is less about new ways of learning representations than about the combination of choices made (over the set of existing techniques) in"":-5.5796451569,""This paper is the first (I believe) to establish a simple yet important result that Convnets for NMT encoders can be competitive to RNNs. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines. I also appreciate the detailed report on training and generation speed. I find it's very interesting when position embeddings turn out to be hugely important (beside residual connections); unfortunately, there is little analysis to shed more lights on this aspect and perhaps compare other ways of capturing positions (a wild guess might be to use embeddings that represent some form of relative positions). The only concern I have (similar to the other reviewer) is that this paper perhaps fits better in an NLP conference.----------------One minor comment: it's slight strange that this well-executed paper doesn't have a single figure on the proposed architecture :) It will also be even better to draw a figure for the biLSTM architecture as"":-5.4240369797,""The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. ----------------Apart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.--------The empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. ----------------The experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. ----------------My main concern is whether or not the paper is appropriate for ICLR, because the contribution"":-2.4614064693}}","This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be ""incremental"".     Pros:  - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work  - Quality: The experimental results were thorough, ""very extensive and leaves no doubt that the proposed approach works well"".    Mixed:  - Novelty: There is appreciation that the work is novel. However as the work is somewhat ""application-specific"" the reviewers felt the technical contribution was not an overwhelming contribution.  - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or ""main speed-up factor(s)"" were.     This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area."
https://openreview.net/forum?id=BJO-BuT1g,"['This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al.'
 'The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles.'
 'The paper is the first to my knowledge that uses a single network to apply different styles to input images. The paper is well-written and its experiments convincingly validate the benefits of the method.']","['This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al.'
 'The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles.'
 'The paper is the first to my knowledge that uses a single network to apply different styles to input images. The paper is well-written and its experiments convincingly validate the benefits of the method.']","This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al.           0.855316
The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles.                                                                                     0.568921
The paper is the first to my knowledge that uses a single network to apply different styles to input images. The paper is well-written and its experiments convincingly validate the benefits of the method.    0.239382
dtype: float32","This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al.           1.098612
The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles.                                                                                     1.098612
The paper is the first to my knowledge that uses a single network to apply different styles to input images. The paper is well-written and its experiments convincingly validate the benefits of the method.    1.098605
dtype: float32","{""This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al."":{""This paper addresses the problem of efficient neural stylization.  Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set \u201cconditional instance normalization\u201d parameters dependent on the desired output style.  The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network.  Qualitative results are shown.----------------I have not worked in this area, but I\u2019m generally aware of the main issues in transferring artistic style.  The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits.  As far as I can tell the results look compelling.  As I\u2019m less confident in my expertise in this area, I\u2019m happy to support another reviewer who is willing to"":-0.9593666196,""The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. ----------------This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.  ----------------Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different"":-4.8102493286,""CONTRIBUTIONS--------The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results.----------------NOVELTY--------The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer"":-4.3121166229},""The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles."":{""This paper addresses the problem of efficient neural stylization.  Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set \u201cconditional instance normalization\u201d parameters dependent on the desired output style.  The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network.  Qualitative results are shown.----------------I have not worked in this area, but I\u2019m generally aware of the main issues in transferring artistic style.  The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits.  As far as I can tell the results look compelling.  As I\u2019m less confident in my expertise in this area, I\u2019m happy to support another reviewer who is willing to"":-11.7252416611,""The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. ----------------This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.  ----------------Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different"":-8.9934473038,""CONTRIBUTIONS--------The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results.----------------NOVELTY--------The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer"":-11.1413755417},""The paper is the first to my knowledge that uses a single network to apply different styles to input images. The paper is well-written and its experiments convincingly validate the benefits of the method."":{""This paper addresses the problem of efficient neural stylization.  Instead of training a separate network for N different styles (as is done, e.g., in Johnson et al.), this paper extends the instance normalization work of Ulyanov et al. to train a single network and learn a smaller set \u201cconditional instance normalization\u201d parameters dependent on the desired output style.  The conditional instance normalization applies a learnt affine transformation on normalized feature maps at each layer in the network.  Qualitative results are shown.----------------I have not worked in this area, but I\u2019m generally aware of the main issues in transferring artistic style.  The paper addresses a known challenge of incorporating different styles into the same net, which have a number of practical benefits.  As far as I can tell the results look compelling.  As I\u2019m less confident in my expertise in this area, I\u2019m happy to support another reviewer who is willing to"":-3.2559971809,""The paper introduces an elegant method to train a single feed-forward style transfer network with a large number of styles. This is achieved by a global, style-dependent scale and shift parameter for each feature in the network. Thus image style is encoded in a very condensed subset of the network parameters, with only two parameters per feature map. ----------------This enables to easily incorporate new styles into an existing network by fine-tuning. At the same time, the quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks. While this also means that the stylisation results in the paper are limited by the quality of current feed-forward methods, the proposed method seems general enough to be combined with future improvements in feed-forward style transfer.  ----------------Finally, the paper shows that having multiple styles encoded in one feature space allows to gradually interpolate between different styles to generate new mixtures of styles. This is comparable to interpolating between the Gram Matrices of different"":-3.4709093571,""CONTRIBUTIONS--------The authors propose a simple architectural modification (conditional instance normalization) for the task of feedforward neural style transfer that allows a single network to apply many different styles to input images. Experiments show that the proposed multi-style networks produce qualitatively similar images as single-style networks, train as fast as single-style networks, and achieve comparable losses as single-style networks. In addition, the authors shows that new styles can be incrementally added to multi-style networks with minimal finetuning, and that convex combinations of per-style parameters can be used for feedforward style blending. The authors have released open-source code and pretrained models allowing others to replicate the experimental results.----------------NOVELTY--------The problem setup is very similar to prior work on feedforward neural style transfer, but the paper is the first to my knowledge that uses a single network to apply different styles to input images; the proposed conditional instance normalization layer"":-1.9505566359}}",The reviewers (two of whom stated maximum confidence) are in consensus that this is a high-quality paper. It also attracted some public feedback which was also positive. The authors have already incorporated much of the feedback into their revised paper. This seems to be a clear accept in my opinion.
https://openreview.net/forum?id=BJh6Ztuxl,"['The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors.'
 'This paper analyzes various unsupervised sentence embedding approaches. The authors aim to assess how much and what type of information is captured by the different embedding models.'
 'This paper presents experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising.']","['The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors.'
 'This paper analyzes various unsupervised sentence embedding approaches. The authors aim to assess how much and what type of information is captured by the different embedding models.'
 'This paper presents experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising.']","The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors.        0.769768
This paper analyzes various unsupervised sentence embedding approaches. The authors aim to assess how much and what type of information is captured by the different embedding models.                              0.450189
This paper presents experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising.    0.642022
dtype: float32","The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors.        1.098612
This paper analyzes various unsupervised sentence embedding approaches. The authors aim to assess how much and what type of information is captured by the different embedding models.                              1.098612
This paper presents experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising.    1.098612
dtype: float32","{""The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors."":{""The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.----------------Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in \""Improving Distributional Similarity with Lessons Learned from Word Embeddings\"" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will"":-0.4787682891,""This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW. (There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).----------------There are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them. However, I found the discussion of the word-order experiment rather unsatisfying. It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper"":-3.2046127319,""This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.----------------Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.----------------One minor red flag: --------- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious"":-4.2464904785},""This paper analyzes various unsupervised sentence embedding approaches. The authors aim to assess how much and what type of information is captured by the different embedding models."":{""The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.----------------Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in \""Improving Distributional Similarity with Lessons Learned from Word Embeddings\"" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will"":-4.1947002411,""This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW. (There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).----------------There are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them. However, I found the discussion of the word-order experiment rather unsatisfying. It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper"":-2.0452067852,""This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.----------------Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.----------------One minor red flag: --------- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious"":-4.0018310547},""This paper presents experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising."":{""The authors present a methodology for analyzing sentence embedding techniques by checking how much the embeddings preserve information about sentence length, word content, and word order. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embedding strategies, such as the fact that word ordering is surprisingly low-entropy conditioned on word content.----------------Exploring what sort of information is encoded in representation learning methods for NLP is an important and under-researched area. For example, the tide of word-embeddings research was mostly stemmed after a thread of careful experimental results showing most embeddings to be essentially equivalent, culminating in \""Improving Distributional Similarity with Lessons Learned from Word Embeddings\"" by Levy, Goldberg, and Dagan. As representation learning becomes even more important in NLP this sort of research will"":-4.9897766113,""This paper analyzes various unsupervised sentence embedding approaches by means of a set of auxiliary prediction tasks. By examining how well classifiers can predict word order, word content, and sentence length, the authors aim to assess how much and what type of information is captured by the different embedding models. The main focus is on a comparison between and encoder-decoder model (ED) and a permutation-invariant model, CBOW. (There is also an analysis of skip-thought vectors, but since it was trained on a different corpus it is hard to compare).----------------There are several interesting and perhaps counter-intuitive results that emerge from this analysis and the authors do a nice job of examining those results and, for the most part, explaining them. However, I found the discussion of the word-order experiment rather unsatisfying. It seems to me that the appropriate question should have been something like, 'How well does model X do compared to the theoretical upper"":-4.7549443245,""This paper presents a set of experiments investigating what kinds of information are captured in common unsupervised approaches to sentence representation learning. The results are non-trivial and somewhat surprising. For example, they show that it is possible to reconstruct word order from bag of words representations, and they show that LSTM sentence autoencoders encode interpretable features even for randomly permuted nonsense sentences.----------------Effective unsupervised sentence representation learning is an important and largely unsolved problem in NLP, and this kind of work seems like it should be straightforwardly helpful towards that end. In addition, the experimental paradigm presented here is likely more broadly applicable to a range of representation learning systems. Some of the results seem somewhat strange, but I see no major technical concerns, and think that that they are informative. I recommend acceptance.----------------One minor red flag: --------- The massive drop in CBOW performance in Figures 1b and 4b are not explained, and seem implausible enough to warrant serious"":-2.2056179047}}","The area chair agrees with the reviewers and think this paper would be of interest to the ICLR audience. There is clearly more to be done in this area, but the authors do a good job shedding some light on what sentence embeddings can encode. We need more work like this that helps us understand what neural networks can model."
https://openreview.net/forum?id=BJm4T4Kgx,"['. Applying adversarial training to imagenet, a larger dataset than previously considered. Comparing different adversarial training approaches.'
 'This paper investigates the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding.'
 'Adversary training on ImageNet.']","['. Applying adversarial training to imagenet, a larger dataset than previously considered. Comparing different adversarial training approaches.'
 'This paper investigates the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding.'
 'Adversary training on ImageNet.']",". Applying adversarial training to imagenet, a larger dataset than previously considered. Comparing different adversarial training approaches.                                                                            0.761229
This paper investigates the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding.    0.761957
Adversary training on ImageNet.                                                                                                                                                                                           0.299056
dtype: float32",". Applying adversarial training to imagenet, a larger dataset than previously considered. Comparing different adversarial training approaches.                                                                            1.098612
This paper investigates the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding.    1.098612
Adversary training on ImageNet.                                                                                                                                                                                           1.098589
dtype: float32","{"". Applying adversarial training to imagenet, a larger dataset than previously considered. Comparing different adversarial training approaches."":{""This paper has two main contributions: --------(1) Applying adversarial training to imagenet, a larger dataset than previously considered --------(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.----------------This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work."":-5.8236961365,""This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.--------The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.----------------Pros:--------For the first time in the literature, this paper proposed the concept of \u2018label leaking\u2019. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.--------Using the ratio of 'clean accuracy' over \u2018adversarial accuracy\u2019 as the measure of robust is more reasonable compared to the existing works in the literature. ----------------Cons:--------Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main"":-8.9529352188,""This paper is a well written paper. This paper can be divided into 2 parts:--------1.Adversary training on ImageNet --------2.Empirical study of label leak, single\/multiple step attack, transferability and importance of model capacity----------------For part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.----------------For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM,"":-8.9381742477},""This paper investigates the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding."":{""This paper has two main contributions: --------(1) Applying adversarial training to imagenet, a larger dataset than previously considered --------(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.----------------This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work."":-3.9864985943,""This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.--------The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.----------------Pros:--------For the first time in the literature, this paper proposed the concept of \u2018label leaking\u2019. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.--------Using the ratio of 'clean accuracy' over \u2018adversarial accuracy\u2019 as the measure of robust is more reasonable compared to the existing works in the literature. ----------------Cons:--------Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main"":-0.9367632866,""This paper is a well written paper. This paper can be divided into 2 parts:--------1.Adversary training on ImageNet --------2.Empirical study of label leak, single\/multiple step attack, transferability and importance of model capacity----------------For part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.----------------For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM,"":-4.1408596039},""Adversary training on ImageNet."":{""This paper has two main contributions: --------(1) Applying adversarial training to imagenet, a larger dataset than previously considered --------(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.----------------This paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work."":-40.2680892944,""This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.--------The paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.----------------Pros:--------For the first time in the literature, this paper proposed the concept of \u2018label leaking\u2019. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.--------Using the ratio of 'clean accuracy' over \u2018adversarial accuracy\u2019 as the measure of robust is more reasonable compared to the existing works in the literature. ----------------Cons:--------Although the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main"":-39.4434432983,""This paper is a well written paper. This paper can be divided into 2 parts:--------1.Adversary training on ImageNet --------2.Empirical study of label leak, single\/multiple step attack, transferability and importance of model capacity----------------For part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.----------------For part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM,"":-38.2464370728}}","This paper is an empirical study of adversarial training in a large-scale regime.   Its main contributions are to demonstrate that Imagenet models can be made robust to adversarial examples using relatively efficient constructions (so-called 'one-step' methods). Along the way, the authors also report the 'label leaking' effect, a flaw in previous adversarial constructions that limits the effectiveness of adversarial examples at regularizing training.     The reviewers were consensual that this contribution is useful to the field, and the label leaking phenomena is an important aspect that this paper helps address and mitigate. The authors responded promptly to reviewers questions. Despite limited novelty and lack of quantitative analysis, I recommend accept as a poster."
https://openreview.net/forum?id=BJuysoFeg,"['This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.'
 ': I think the paper is more suitable for a workshop track rather than for the main conference track. I would love to see a fair comparison of the state-of-the-art methods on toy datasets.'
 'The paper is more fit as a short workshop presentation than a full conference paper.']","['This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.'
 ': I think the paper is more suitable for a workshop track rather than for the main conference track. I would love to see a fair comparison of the state-of-the-art methods on toy datasets.'
 'The paper is more fit as a short workshop presentation than a full conference paper.']","This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.                                                                  0.552205
: I think the paper is more suitable for a workshop track rather than for the main conference track. I would love to see a fair comparison of the state-of-the-art methods on toy datasets.    0.145844
The paper is more fit as a short workshop presentation than a full conference paper.                                                                                                           0.940468
dtype: float32","This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.                                                                  1.098601
: I think the paper is more suitable for a workshop track rather than for the main conference track. I would love to see a fair comparison of the state-of-the-art methods on toy datasets.    1.098452
The paper is more fit as a short workshop presentation than a full conference paper.                                                                                                           1.098612
dtype: float32","{""This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain."":{""This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.------------------------Pros:----------------The method is very simple and easy to understand and apply.----------------The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.----------------The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.------------------------Cons:----------------There is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward\/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?)----------------The analysis in section 4.3.1 is superfluous except as a sanity check"":-14.1030893326,""Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.----------------This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source"":-16.0491733551,""Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.----------------Detailed comments:----------------Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-S"":-16.9978351593},"": I think the paper is more suitable for a workshop track rather than for the main conference track. I would love to see a fair comparison of the state-of-the-art methods on toy datasets."":{""This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.------------------------Pros:----------------The method is very simple and easy to understand and apply.----------------The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.----------------The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.------------------------Cons:----------------There is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward\/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?)----------------The analysis in section 4.3.1 is superfluous except as a sanity check"":-3.523570776,""Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.----------------This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source"":-2.1311635971,""Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.----------------Detailed comments:----------------Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-S"":-2.8480837345},""The paper is more fit as a short workshop presentation than a full conference paper."":{""This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain.------------------------Pros:----------------The method is very simple and easy to understand and apply.----------------The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks.----------------The analysis in section 4.3.2 shows that a very small number of target domain samples are needed for adaptation of the network.------------------------Cons:----------------There is little novelty -- the method is arguably too simple to be called a \u201cmethod.\u201d Rather, it\u2019s the most straightforward\/intuitive approach when using a network with batch normalization for domain adaptation.  The alternative -- using the BN statistics from the source domain for target domain examples -- is less natural, to me. (I guess this alternative is what\u2019s done in the Inception BN results in Table 1-2?)----------------The analysis in section 4.3.1 is superfluous except as a sanity check"":-22.9021244049,""Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.----------------This paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source"":-24.178565979,""Overall I think this is an interesting paper which shows empirical performance improvement over baselines. However, my main concern with the paper is regarding its technical depth, as the gist of the paper can be summarized as the following: instead of keeping the batch norm mean and bias estimation over the whole model, estimate them on a per-domain basis. I am not sure if this is novel, as this is a natural extension of the original batch normalization paper. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.----------------Detailed comments:----------------Section 3.1: I respectfully disagree that the core idea of BN is to align the distribution of training data. It does this as a side effect, but the major purpose of BN is to properly control the scale of the gradient so we can train very deep models without the problem of vanishing gradients. It is plausible that intermediate features from different datasets naturally show as different groups in a t-S"":-19.2443237305}}","Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work.   The work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.  1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain? 2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant.  3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? "
https://openreview.net/forum?id=BJwFrvOeg,"['This paper proposes to incorporate knowledge base facts into language modeling. Authors demonstrate effectiveness on a new dataset WikiFacts which aligns Wikipedia articles with Freebase facts.'
 'The paper proposes an evolution upon traditional Recurrent Language Models. The architecture appears sound, but the writing makes it hard to fully understand.'
 'This paper addresses the problem of generating rare or unseen words in language modeling. Rare words are especially important in context of applications such as question answering.']","['This paper proposes to incorporate knowledge base facts into language modeling. Authors demonstrate effectiveness on a new dataset WikiFacts which aligns Wikipedia articles with Freebase facts.'
 'The paper proposes an evolution upon traditional Recurrent Language Models. The architecture appears sound, but the writing makes it hard to fully understand.'
 'This paper addresses the problem of generating rare or unseen words in language modeling. Rare words are especially important in context of applications such as question answering.']","This paper proposes to incorporate knowledge base facts into language modeling. Authors demonstrate effectiveness on a new dataset WikiFacts which aligns Wikipedia articles with Freebase facts.    0.851744
The paper proposes an evolution upon traditional Recurrent Language Models. The architecture appears sound, but the writing makes it hard to fully understand.                                       0.459843
This paper addresses the problem of generating rare or unseen words in language modeling. Rare words are especially important in context of applications such as question answering.                 0.723822
dtype: float32","This paper proposes to incorporate knowledge base facts into language modeling. Authors demonstrate effectiveness on a new dataset WikiFacts which aligns Wikipedia articles with Freebase facts.    1.098612
The paper proposes an evolution upon traditional Recurrent Language Models. The architecture appears sound, but the writing makes it hard to fully understand.                                       1.098612
This paper addresses the problem of generating rare or unseen words in language modeling. Rare words are especially important in context of applications such as question answering.                 1.098612
dtype: float32","{""This paper proposes to incorporate knowledge base facts into language modeling. Authors demonstrate effectiveness on a new dataset WikiFacts which aligns Wikipedia articles with Freebase facts."":{""This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.----------------The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.----------------At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.----------------However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. --------- I cannot find where  f_{symbkey} \/ f_{voca} \/ f_{copy} are defined--------- w^v, w^s are confusing.--------- e_k"":-0.8033454418,""The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.----------------The writing could be improved. The beginning of Section 3 in particular is hard to parse.----------------There have been similar efforts recently (like \""Pointer Sentinel Mixture Models\"" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is"":-5.0663905144,""This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.----------------The model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.----------------Overall the paper could use some rewriting especially"":-3.8961248398},""The paper proposes an evolution upon traditional Recurrent Language Models. The architecture appears sound, but the writing makes it hard to fully understand."":{""This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.----------------The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.----------------At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.----------------However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. --------- I cannot find where  f_{symbkey} \/ f_{voca} \/ f_{copy} are defined--------- w^v, w^s are confusing.--------- e_k"":-6.5639004707,""The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.----------------The writing could be improved. The beginning of Section 3 in particular is hard to parse.----------------There have been similar efforts recently (like \""Pointer Sentinel Mixture Models\"" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is"":-4.6292009354,""This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.----------------The model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.----------------Overall the paper could use some rewriting especially"":-6.8639492989},""This paper addresses the problem of generating rare or unseen words in language modeling. Rare words are especially important in context of applications such as question answering."":{""This paper proposes to incorporate knowledge base facts into language modeling, thus at each time step, a word is either generated from the full vocabulary or relevant KB entities.----------------The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.  The authors also suggest a modified perplexity metric which penalizes the likelihood of unknown words.----------------At a high level, I do like the motivation of this paper -- named entity words are usually important for downstream tasks, but difficult to learn solely based on statistical co-occurrences. The facts encoded in KB could be a great supply for this.----------------However, I find it difficult to follow the details of the paper (mainly Section 3) and think the paper writing needs to be much improved. --------- I cannot find where  f_{symbkey} \/ f_{voca} \/ f_{copy} are defined--------- w^v, w^s are confusing.--------- e_k"":-5.0237102509,""The paper proposes an evolution upon traditional Recurrent Language Models to give the capability to deal with unknown words. It is done by pairing the traditional RNNLM with a module operating on a KB and able to copy from KB facts to generate unseen words. It is shown to be efficient and much better than plain RNNLM on a new dataset.----------------The writing could be improved. The beginning of Section 3 in particular is hard to parse.----------------There have been similar efforts recently (like \""Pointer Sentinel Mixture Models\"" by Merity et al.) that attempt to overcome limitations of RNNLMs with unknown words; but they usually do it by adding a mechanism to copy from a longer past history. The proposal of the current paper is different and more interesting to me in that it try to bring knowledge from another source (KB) to the language model. This is harder because one needs to leverage the large scale of the KB to do so. Being able to train that conveniently is"":-5.1202325821,""This paper addresses the practical problem of generating rare or unseen words in the context of language modeling. Since language follows a Zipf\u2019s law, most approaches limit the vocabulary (because of computation reasons) and hence rare words are often mapped to a UNK token. Rare words are especially important in context of applications such as question answering. MT etc. This paper proposes a language modeling technique which incorporates facts from knowledge bases (KBs) and thus has the ability to generate (potentially unseen) words from KBs. This paper also releases a dataset by aligning words with Freebase facts and corresponding Wikipedia descriptions.----------------The model first selects a KB fact based on the previously generated words and facts. Based on the selected fact, it then predicts whether to generate a word based on the vocabulary or to output a symbolic word from the KB. For the latter, the model is trained to predict the position of the word from the fact description.----------------Overall the paper could use some rewriting especially"":-2.1041505337}}","This work introduces a combination of a LM with knowledge based retrieval system. This builds upon the recent trend of incorporating pointers and external information into generation, but includes some novelty, making the paper ""different and more interesting"". Generally though the reviewers found the clarity of the work to be sufficiently an issue that no one strongly defended its inclusion.    Pros:  - The reviewers seemed to like the work and particularly the problem space. Issues were mainly on presentation and experiments.     Mixed:  - Reviewers were divided on experimental quality. The work does introduce a new dataset, but reviewers would also have liked use on some existing tasks.     Cons:  - Clarity and writing issues primarily. All reviewers found details missing and generally struggled with comprehension.  - Novelty was a question. Impact of work could also be improved by more clearly defining new contributions"
https://openreview.net/forum?id=BJxhLAuxg,"['This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction.'
 'The paper extends a proposed video frame prediction method with reward prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps.'
 'The paper is well written. I feel that the presented results are too incremental. Neither the methodology not the results are novel / surprising.']","['This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction.'
 'The paper extends a proposed video frame prediction method with reward prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps.'
 'The paper is well written. I feel that the presented results are too incremental. Neither the methodology not the results are novel / surprising.']","This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction.                                                                                            0.825558
The paper extends a proposed video frame prediction method with reward prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps.    0.694414
The paper is well written. I feel that the presented results are too incremental. Neither the methodology not the results are novel / surprising.                                                                0.752241
dtype: float32","This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction.                                                                                            1.098612
The paper extends a proposed video frame prediction method with reward prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps.    1.098612
The paper is well written. I feel that the presented results are too incremental. Neither the methodology not the results are novel / surprising.                                                                1.098612
dtype: float32","{""This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction."":{""This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.----------------Pros:--------- Paper is well written and easy to follow.--------- Model is clear to understand.----------------Cons:--------- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.----------------This paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions."":-9.2634973526,""The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would"":-12.4880447388,""The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel \/ surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.----------------I\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. "":-12.9133834839},""The paper extends a proposed video frame prediction method with reward prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps."":{""This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.----------------Pros:--------- Paper is well written and easy to follow.--------- Model is clear to understand.----------------Cons:--------- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.----------------This paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions."":-3.3655281067,""The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would"":-0.7865115404,""The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel \/ surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.----------------I\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. "":-3.9687373638},""The paper is well written. I feel that the presented results are too incremental. Neither the methodology not the results are novel \/ surprising."":{""This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.----------------Pros:--------- Paper is well written and easy to follow.--------- Model is clear to understand.----------------Cons:--------- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.----------------This paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions."":-8.6138601303,""The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would"":-8.6127605438,""The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel \/ surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.----------------I\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. "":-5.5297603607}}","The authors have combined two known areas of research - frame prediction and reward prediction - and combined them in a feedforward network trained on sequences from Atari games. The fact that this should train well is unsurprising for this domain, and the research yields no other interesting results. Pros - the paper is clearly written and the experiments are sound. Cons - there is very little novelty or contribution."
https://openreview.net/forum?id=Bk67W4Yxl,"['The paper tests various feedforward network architectures for supervised training to predict a human’s next move. The paper’s presentation is inefficient and muddled, and the results seem incremental.'
 'This paper reports new CNN architectures for playing Go. Results are better than previously reported, but no mention of computational time and efficiency.'
 'The paper trains slightly different network architectures on Computer Go. It provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.'
 'The paper investigates several different architectures for move prediction in computer Go. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.']","['The paper tests various feedforward network architectures for supervised training to predict a human’s next move. The paper’s presentation is inefficient and muddled, and the results seem incremental.'
 'This paper reports new CNN architectures for playing Go. Results are better than previously reported, but no mention of computational time and efficiency.'
 'The paper trains slightly different network architectures on Computer Go. It provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.'
 'The paper investigates several different architectures for move prediction in computer Go. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.']","The paper tests various feedforward network architectures for supervised training to predict a human’s next move. The paper’s presentation is inefficient and muddled, and the results seem incremental.                                       0.875048
This paper reports new CNN architectures for playing Go. Results are better than previously reported, but no mention of computational time and efficiency.                                                                                     0.960352
The paper trains slightly different network architectures on Computer Go. It provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.                                            1.092814
The paper investigates several different architectures for move prediction in computer Go. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.    1.193612
dtype: float32","The paper tests various feedforward network architectures for supervised training to predict a human’s next move. The paper’s presentation is inefficient and muddled, and the results seem incremental.                                       1.386294
This paper reports new CNN architectures for playing Go. Results are better than previously reported, but no mention of computational time and efficiency.                                                                                     1.386294
The paper trains slightly different network architectures on Computer Go. It provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.                                            1.386294
The paper investigates several different architectures for move prediction in computer Go. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.    1.386294
dtype: float32","{""The paper tests various feedforward network architectures for supervised training to predict a human\u2019s next move. The paper\u2019s presentation is inefficient and muddled, and the results seem incremental."":{""The paper tests various feedforward network architectures for supervised training to predict a human\u2019s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations\/reflections of each board position. ----------------The paper\u2019s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it\u2019s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it\u2019s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would\u2019ve"":-1.1491651535,""This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance\/flop or performance\/flop\/energy.--------Overall a good paper."":-4.5443329811,""The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.----------------The paper looks like a follow up paper of the author\u2019s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. ----------------The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?"":-4.0776381493,""The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.----------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?--------2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?--------3) It is not"":-4.0337443352},""This paper reports new CNN architectures for playing Go. Results are better than previously reported, but no mention of computational time and efficiency."":{""The paper tests various feedforward network architectures for supervised training to predict a human\u2019s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations\/reflections of each board position. ----------------The paper\u2019s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it\u2019s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it\u2019s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would\u2019ve"":-10.287653923,""This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance\/flop or performance\/flop\/energy.--------Overall a good paper."":-7.0470581055,""The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.----------------The paper looks like a follow up paper of the author\u2019s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. ----------------The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?"":-10.4199609756,""The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.----------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?--------2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?--------3) It is not"":-10.4166316986},""The paper trains slightly different network architectures on Computer Go. It provides an analysis of the accuracy as the number of training samples increases and the network architecture differs."":{""The paper tests various feedforward network architectures for supervised training to predict a human\u2019s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations\/reflections of each board position. ----------------The paper\u2019s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it\u2019s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it\u2019s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would\u2019ve"":-7.984395504,""This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance\/flop or performance\/flop\/energy.--------Overall a good paper."":-8.3406553268,""The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.----------------The paper looks like a follow up paper of the author\u2019s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. ----------------The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?"":-4.2267189026,""The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.----------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?--------2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?--------3) It is not"":-7.9321208},""The paper investigates several different architectures for move prediction in computer Go. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS."":{""The paper tests various feedforward network architectures for supervised training to predict a human\u2019s next move, given a board position. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations\/reflections of each board position. ----------------The paper\u2019s presentation is inefficient and muddled, and the results seem incremental.----------------Presentation:----------------The abstract and introduction point out that AlphaGo requires many RL iterations to train, and propose to improve this by swapping out the policy network with one that is more amenable to training. However, the paper only presents supervised learning results, not RL.----------------While it\u2019s not unreasonable to assume that a higher-capacity network that shows improvements in supervised learning will also yield dividends in RL, it\u2019s still unsatisfying to be presented with SL improvements and be asked to assume that the RL improvements will be of a similar magnitude, whatever that may mean. It would\u2019ve"":-4.9854421616,""This paper reports new CNN architectures for playing Go. The results are better than previously reported, but there is no mention of computational time and efficiency, and relative metric of performance\/flop or performance\/flop\/energy.--------Overall a good paper."":-5.484038353,""The paper trains slightly different network architectures on Computer Go, and provides an analysis of the accuracy as the number of training samples increases and the network architecture differs.----------------The paper looks like a follow up paper of the author\u2019s previous paper Cazenave (2016a), however, the contribution over the previous paper is not clear. A section should be added to state what the differences are. ----------------The paper states that the improvements that are obtained in this study are because of the changes in the training set, the input features and the architecture of the network. It is not clear what are the changes that were done to the training set and input features. How are they different than the previous work?"":-5.0194382668,""The paper investigates several different architectures for move prediction in computer Go. The main innovation seems to be the use of residual networks. The best proposed architecture outperforms previous results on KGS move prediction dataset. The network also reached amateur 3 dan level on KGS.----------------I found the paper to be somewhat poorly written and lacking important details. Here are my main concerns:--------1) This paper references a previous paper by the author(Cazenave 2016a) as having introduced residual network architectures to computer go. The overlap with this paper seems quite significant but I could not find it anywhere. What exactly is new?--------2) The author claims the addition of batch norm to a residual architecture as the main architectural innovation, but the original ResNet paper was already using batch norm between the convolution and activation layers. Have you compared your architecture (ResNet with batch norm after ReLU) with the original ResNet architecture (batch norm before ReLU)?--------3) It is not"":-0.7453770638}}","The authors suggest alternate feedforward architectures (residual layers) for training a Go policy more efficiently. The authors refer to AlphaGo, but the proposed approach does not use reinforcement learning, which is not stated clearly in the introduction of the paper. The contribution is very incremental, there are no new ideas, and the presentation is muddled."
https://openreview.net/forum?id=Bk8N0RLxx,"['This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.'
 'This paper compares several strategies for guessing a short list of vocabulary. The primary findings are that word alignment dictionaries work better than a variety of other techniques.'
 '. This paper will fit better in an NLP venue.' ' translation.']","['This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.'
 'This paper compares several strategies for guessing a short list of vocabulary. The primary findings are that word alignment dictionaries work better than a variety of other techniques.'
 '. This paper will fit better in an NLP venue.' ' translation.']","This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.                                               0.650031
This paper compares several strategies for guessing a short list of vocabulary. The primary findings are that word alignment dictionaries work better than a variety of other techniques.    0.931581
. This paper will fit better in an NLP venue.                                                                                                                                                1.033693
 translation.                                                                                                                                                                                0.250683
dtype: float32","This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.                                               1.372942
This paper compares several strategies for guessing a short list of vocabulary. The primary findings are that word alignment dictionaries work better than a variety of other techniques.    1.384243
. This paper will fit better in an NLP venue.                                                                                                                                                1.386286
 translation.                                                                                                                                                                                1.337637
dtype: float32","{""This paper conducts a series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation."":{""This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.----------------A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.----------------The experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.----------------A major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid"":-6.7212572098,""This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.----------------My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.----------------Minor comments:--------In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of"":-8.7863578796,""In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue."":-9.2144498825,""This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.----------------- Do the reported decoding times take into account the vocabulary reduction step?--------- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?--------- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem."":-9.5706253052},""This paper compares several strategies for guessing a short list of vocabulary. The primary findings are that word alignment dictionaries work better than a variety of other techniques."":{""This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.----------------A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.----------------The experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.----------------A major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid"":-3.7883718014,""This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.----------------My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.----------------Minor comments:--------In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of"":-0.6737667918,""In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue."":-3.9519450665,""This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.----------------- Do the reported decoding times take into account the vocabulary reduction step?--------- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?--------- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem."":-3.97874856},"". This paper will fit better in an NLP venue."":{""This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.----------------A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.----------------The experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.----------------A major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid"":-22.690820694,""This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.----------------My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.----------------Minor comments:--------In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of"":-22.0308704376,""In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue."":-18.8752174377,""This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.----------------- Do the reported decoding times take into account the vocabulary reduction step?--------- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?--------- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem."":-22.8208446503},"" translation."":{""This paper conducts a comprehensive series of experiments on vocabulary selection strategies to reduce the computational cost of neural machine translation.----------------A range of techniques are investigated, ranging from very simple methods such as word co-occurences, to the relatively complex use of SVMs.----------------The experiments are solid, comprehensive and very useful in practical terms.  It is good to see that the best vocabulary selection method is very effective at achieving a very high proportion of the coverage of the full-vocabulary model (fig 3).  However, I feel that the experiments in section 4.3 (vocabulary selection during training) was rather limited in their scope - I would have liked to see more experiments here.----------------A major criticism I have with this paper is that there is little novelty here.  The techniques are mostly standard methods and rather simple, and in particular, there it seems that there is not much additional material beyond the work of Mi et al (2016).  So although the work is solid"":-95.4934997559,""This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques.----------------My take on this paper is that to have a significant impact, it needs to make the case for why one might want vocabulary rather than characters or sub word units like BPE. I think there are likely many very good reasons to do this that could be argued for (synthesize morphology, deal with transliteration, etc), but most of these would suggest some particular models and experiments, which are of course not in this paper. As it is, I think this paper is a useful but minor contribution that shows that word alignment is a good way of getting short lists, but it does not strongly make the case that we should abandon work in other directions.----------------Minor comments:--------In addition to the SVM approach for modeling vocabulary, the discriminative word lexicon of"":-94.2928161621,""In this paper, the authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks. However, there is little novelty in this work: the authors further mostly extend the work of (Mi et al., 2016) with more vocabulary selection strategies and thorough experiments. This paper will fit better in an NLP venue."":-95.5560913086,""This paper evaluates several strategies to reduce output vocabulary size in order to speed up NMT decoding and training. It could be quite useful to practitioners, although the main contributions of the paper seem somewhat orthogonal to representation learning and neural networks, and I am not sure ICLR is the ideal venue for this work.----------------- Do the reported decoding times take into account the vocabulary reduction step?--------- Aside from machine translation, might there be applications to other settings such as language modeling, where large vocabulary is also a scalability challenge?--------- The proposed methods are helpful because of the difficulties induced by using a word-level model. But (at least in my opinion) starting from a character or even lower-level abstraction seems to be the obvious solution to the huge vocabulary problem."":-96.1309890747}}","The reviewers agree that the method is exciting as practical contributions go, but the case for originality is not strong enough."
https://openreview.net/forum?id=BkCPyXm1l,"['The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction.'
 'This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training'
 'This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself.']","['The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction.'
 'This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training'
 'This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself.']","The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction.                                                                                                                                 0.937545
This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training    0.931298
This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself.                                                                                                                                                    0.826573
dtype: float32","The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction.                                                                                                                                 1.098612
This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training    1.098612
This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself.                                                                                                                                                    1.098612
dtype: float32","{""The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction."":{""The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). ----------------Pros: --------+ Comprehensive analysis on the co-label similarity.----------------Cons:--------- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. --------- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR."":-10.060710907,""Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.----------------In order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.----------------The baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with"":-14.1284017563,""This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \""roll-in\"" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.----------------This is an incremental improvement on the idea of label softening\/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening\/"":-14.2442731857},""This paper proposes a soft-target regularization that trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training"":{""The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). ----------------Pros: --------+ Comprehensive analysis on the co-label similarity.----------------Cons:--------- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. --------- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR."":-4.7554192543,""Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.----------------In order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.----------------The baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with"":-0.7884251475,""This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \""roll-in\"" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.----------------This is an incremental improvement on the idea of label softening\/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening\/"":-4.9815917015},""This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself."":{""The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network). ----------------Pros: --------+ Comprehensive analysis on the co-label similarity.----------------Cons:--------- Weak baselines. I am not sure the authors have found the best hyper-parameters in their experiments. I just trained a 5 layer fully connected MNIST model with 512 hidden units without any regularizer and achieved 0.986 acc. using Adam and He initialization, where the paper reported 0.981 for such architecture. --------- The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR."":-18.9833984375,""Inspired by the analysis on the effect of the co-label similarity (Hinton et al., 2015), this paper proposes a soft-target regularization that iteratively trains the network using weighted average of the exponential moving average of past labels and hard labels as target argument of loss. They claim that this prevents the disappearing of co-label similarity after early training and  yields a competitive regularization to dropout without sacrificing network capacity.----------------In order to make a fair comparison to dropout,  the dropout should be tuned carefully. Showing that it performs better than dropout regularization for some particular values of dropout (Table 2) does not demonstrate a convincing advantage. It is possible that dropout performs better after a reasonable tuning with cross-validation.----------------The baseline architectures used in the experiments do not belong the recent state of art methods thus yielding significantly lower accuracy. It seems also that experiment setup does not involve any data augmentation, the results can also change with"":-19.4593849182,""This manuscript tries to tackle neural network regularization by blending the target distribution with predictions of the model itself. In this sense it is similar in spirit to scheduled sampling (Bengio et al) and SEARN (Daume et al) DAgger (Ross et al) which consider a \""roll-in\"" mixture of the target and model distributions during training. It was clarified in the pre-review questions that these targets are generated on-line rather than from a lagged distribution, which I think makes the algorithm pseudocode somewhat misleading if I understand it correctly.----------------This is an incremental improvement on the idea of label softening\/smoothing that has recently been revived, and so the novelty is not that high. The author points out that co-label similarity is better preserved by this method but it doesn't follow that this is causal re: regularization; a natural baseline would be a fixed, soft label distribution, as well as one where the softening\/"":-15.7750930786}}",The reviewers unanimously recommend rejection.
https://openreview.net/forum?id=BkLhzHtlg,"['Reviewer says the paper is interesting, but some of the results are hard to understand.'
 'The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.'
 'This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.']","['Reviewer says the paper is interesting, but some of the results are hard to understand.'
 'The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.'
 'This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.']","Reviewer says the paper is interesting, but some of the results are hard to understand.                                                                                                          0.139145
The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.    1.020434
This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.                                                                                   0.900889
dtype: float32","Reviewer says the paper is interesting, but some of the results are hard to understand.                                                                                                          1.098590
The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.    1.098612
This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.                                                                                   1.098612
dtype: float32","{""Reviewer says the paper is interesting, but some of the results are hard to understand."":{""While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). ----------------The overall approach is interesting: all three of the key techniques (aux. tasks, skip\/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.----------------I found some of the results hard to understand\/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. ----------------It may be worthwhile very briefly mentioning the relationship of \""diagonal\"" connections to other emerging terms for similar ideas (e.g. skip connections, etc). \""Skip\"" seems to me to be accurate regardless of how you draw the network, whereas \""diagonal\"" only makes sense for certain visual layouts.----------------In response to comment in the"":-8.9336366653,""The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.----------------The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.--------Motion is discretized and predicted using classification. The model is trained on classification loss combined with a loss on motion prediction. The goal is to leverage latter loss in a semi-supervised setting from parts of the data which do not contain action labels.----------------The idea of leveraging predictions to train feature representations for discrimination is not new. However, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.----------------My biggest concern is with the experimental evaluation. The experimental section contains a large amount of figures, which visualize what the model has learned in a qualitative way. However, quantitative evaluation is rarer.----------------- On the fly application, the authors"":-9.9844875336,""This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.--------The paper is well written, clear in its presentation and backed up by good experiments.--------They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,--------allowing more accurate classification with less training data.--------They also show how the information learned by the network is interpretable and organised in a hierarchy.----------------Weaknesses:--------- a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.--------- moreover, a discussion on how this approach could scale to more challenging scenarios \""involving animals\"" and visual input for instance and more general \""behaviours\"" is also missing;--------The criticism here is pointed at the fact that the title\/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,--------making the original claim a little"":-10.0092611313},""The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition."":{""While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). ----------------The overall approach is interesting: all three of the key techniques (aux. tasks, skip\/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.----------------I found some of the results hard to understand\/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. ----------------It may be worthwhile very briefly mentioning the relationship of \""diagonal\"" connections to other emerging terms for similar ideas (e.g. skip connections, etc). \""Skip\"" seems to me to be accurate regardless of how you draw the network, whereas \""diagonal\"" only makes sense for certain visual layouts.----------------In response to comment in the"":-6.2079663277,""The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.----------------The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.--------Motion is discretized and predicted using classification. The model is trained on classification loss combined with a loss on motion prediction. The goal is to leverage latter loss in a semi-supervised setting from parts of the data which do not contain action labels.----------------The idea of leveraging predictions to train feature representations for discrimination is not new. However, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.----------------My biggest concern is with the experimental evaluation. The experimental section contains a large amount of figures, which visualize what the model has learned in a qualitative way. However, quantitative evaluation is rarer.----------------- On the fly application, the authors"":-0.9163111448,""This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.--------The paper is well written, clear in its presentation and backed up by good experiments.--------They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,--------allowing more accurate classification with less training data.--------They also show how the information learned by the network is interpretable and organised in a hierarchy.----------------Weaknesses:--------- a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.--------- moreover, a discussion on how this approach could scale to more challenging scenarios \""involving animals\"" and visual input for instance and more general \""behaviours\"" is also missing;--------The criticism here is pointed at the fact that the title\/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,--------making the original claim a little"":-5.7233428955},""This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents."":{""While my above review title is too verbose, it would be a more accurate title for the paper than the current one (an overall better title would probably be somewhere in between). ----------------The overall approach is interesting: all three of the key techniques (aux. tasks, skip\/diagonal connections, and the use of internal labels for the kind of data available) make a lot of sense.----------------I found some of the results hard to understand\/interpret. Some of the explanation in the discussion below has been helpful (e.g. see my earlier questions about Fig 4 and 5); the paper would benefit from including more such explanations. ----------------It may be worthwhile very briefly mentioning the relationship of \""diagonal\"" connections to other emerging terms for similar ideas (e.g. skip connections, etc). \""Skip\"" seems to me to be accurate regardless of how you draw the network, whereas \""diagonal\"" only makes sense for certain visual layouts.----------------In response to comment in the"":-14.0045423508,""The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition.----------------The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.--------Motion is discretized and predicted using classification. The model is trained on classification loss combined with a loss on motion prediction. The goal is to leverage latter loss in a semi-supervised setting from parts of the data which do not contain action labels.----------------The idea of leveraging predictions to train feature representations for discrimination is not new. However, the paper presents a couple of interesting ideas, partially inspired from other work in other areas.----------------My biggest concern is with the experimental evaluation. The experimental section contains a large amount of figures, which visualize what the model has learned in a qualitative way. However, quantitative evaluation is rarer.----------------- On the fly application, the authors"":-13.2415990829,""This paper proposes a recurrent architecture for simultaneously predicting motion and action states of agents.--------The paper is well written, clear in its presentation and backed up by good experiments.--------They demonstrate that by forcing the network to predict motion has beneficial consequences on the classification of actions states,--------allowing more accurate classification with less training data.--------They also show how the information learned by the network is interpretable and organised in a hierarchy.----------------Weaknesses:--------- a critical discussion on the interplay between motion an behaviour that is needed to experience the benefits of their proposed model is missing from the paper.--------- moreover, a discussion on how this approach could scale to more challenging scenarios \""involving animals\"" and visual input for instance and more general \""behaviours\"" is also missing;--------The criticism here is pointed at the fact that the title\/abstract claim general behaviour modelling, whilst the experiments are focused on two very specific and relatively simple scenarios,--------making the original claim a little"":-9.7130851746}}","Originality and Significance:   The paper develops a recurrently coupled discriminative / generative hierarchical model, as applied to fruit-fly behavior and online handwriting. Qualitative evaluation is provided by generating motions, in addition to quantitative results. Writer identity and fly gender are learned in an unsupervised fashion. While the individual components of the solution are not particularly novel, their combination together with the detailed experimental validation makes the method potentially interesting to a broad audience. Some reviewers have concerns about the broad claims that are implied by the title and abstract, and thus it is recommended that these be refined to be more specific about the method and the applications.    Quality and Clarity:   The paper is well written.    Pros:  - interesting problem and application domains; will be of broad interest  - useful ideas and architecture: shows that forcing the network to predictions about motion leads to improved classification  - well written paper backed up by good experiments    Cons:  - the individual architectural features have for the most part been proposed before"
https://openreview.net/forum?id=BkVsEMYel,"['This paper investigates the fact why deep networks perform well in practice. It shows how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank.'
 'The paper provides a complex algebraic machinery to analyze the type of functions covered by convolutional network. The ideal networks described in the paper do not match the type of CNNs used in practice.'
 'Theory is focused on convolutional arithmetic circuits. The basic intuition is convincing and fairly straightforward.']","['This paper investigates the fact why deep networks perform well in practice. It shows how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank.'
 'The paper provides a complex algebraic machinery to analyze the type of functions covered by convolutional network. The ideal networks described in the paper do not match the type of CNNs used in practice.'
 'Theory is focused on convolutional arithmetic circuits. The basic intuition is convincing and fairly straightforward.']","This paper investigates the fact why deep networks perform well in practice. It shows how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank.    0.943550
The paper provides a complex algebraic machinery to analyze the type of functions covered by convolutional network. The ideal networks described in the paper do not match the type of CNNs used in practice.                                     0.764414
Theory is focused on convolutional arithmetic circuits. The basic intuition is convincing and fairly straightforward.                                                                                                                             0.580956
dtype: float32","This paper investigates the fact why deep networks perform well in practice. It shows how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank.    1.098612
The paper provides a complex algebraic machinery to analyze the type of functions covered by convolutional network. The ideal networks described in the paper do not match the type of CNNs used in practice.                                     1.098612
Theory is focused on convolutional arithmetic circuits. The basic intuition is convincing and fairly straightforward.                                                                                                                             1.098612
dtype: float32","{""This paper investigates the fact why deep networks perform well in practice. It shows how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank."":{""This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)----------------In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max\/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. ----------------Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. ----------------This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. ----------------This interesting contribution gives an intuition about how deep network can capture the correlation between the input"":-0.7178320289,""The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).----------------While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.--------My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section"":-5.1701889038,""This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.----------------The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is"":-4.6604499817},""The paper provides a complex algebraic machinery to analyze the type of functions covered by convolutional network. The ideal networks described in the paper do not match the type of CNNs used in practice."":{""This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)----------------In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max\/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. ----------------Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. ----------------This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. ----------------This interesting contribution gives an intuition about how deep network can capture the correlation between the input"":-4.9434165955,""The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).----------------While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.--------My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section"":-1.587366581,""This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.----------------The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is"":-4.5303206444},""Theory is focused on convolutional arithmetic circuits. The basic intuition is convincing and fairly straightforward."":{""This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank (for certain partitioning.)----------------In the authors' previous works, they showed the superiority of deep networks over shallows when the activation function is ReLu and the pooling is max\/mean pooling but in the current paper there is no activation function after conv and the pooling is just a multiplication of the node values. Although for the experimental results they've considered both scenarios. ----------------Actually, the general reasoning for this problem is hard, therefore, this drawback is not significant and the current contribution adds a reasonable amount of knowledge to the literature. ----------------This paper studies the convolutional arithmetic circuits and shows how this model can address the inductive biases and how pooling can adjust these biases. ----------------This interesting contribution gives an intuition about how deep network can capture the correlation between the input"":-16.3419723511,""The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network. As in most attempts  in this direction in the literature, the ideal networks described in paper, which have to be interpretable as polynomials over tensors, do not match the type of CNNs used in practice: for instance the Relu non-linearity is replaced with a product of linear functions (or a sum of logs).----------------While the paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible, and give a more intuitive understanding of what the separation rank means rather before piling up different mathematical interpretation.--------My SVM-era algebra is quite rusted, and I am not familiar with the separation rank framework: it would have been much easier for me to first fully understand a simple and gentle case (shallow network in section"":-17.1672115326,""This paper addresses the question of which functions are well suited to deep networks, as opposed to shallow networks.  The basic intuition is convincing and fairly straightforward.  Pooling operations bring together information.  When information is correlated, it can be more efficiently used if the geometry of pooling regions matches the correlations so that it can be brought together more efficiently.  Shallow networks without layers of localized pooling lack this mechanism to combine correlated information efficiently.----------------The theoretical results are focused on convolutional arithmetic circuits, building on prior theoretical results of the authors.  The results make use of the interesting technical notion of separability, which in some sense measures the degree to which a function can be represented as the composition of independent functions.  Because separability is measured relative to a partition of the input, it is an appropriate mechanism for measuring the complexity of functions relative to a particular geometry of pooling operations.  Many of the technical notions are pretty intuitive, although the tensor analysis is"":-14.2531423569}}","The paper uses the notion of separation rank from tensor algebra to analyze the correlations induced through convolution and pooling operations. They show that deep networks have exponentially larger separation ranks compared to shallow ones, and thus, can induce a much richer correlation structure compared to shallow networks. It is argued that this rich inductive bias is crucial for empirical success.    The paper is technically solid. The reviewers note this, and also make a few suggestions on how to make the paper more accessible. The authors have taken this into account. In order to bridge the gap between theory and practice, it is essential for theory papers to be accessible.    The paper covers related work pretty well. One aspect is misses is the recent geometric analysis of deep learning. Can the algebraic analysis be connected to geometric analysis of deep learning, e.g. in the following paper?  https://arxiv.org/abs/1606.05340"
https://openreview.net/forum?id=BkfiXiUlg,"['This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks.'
 'The model build a hierarchical softmax on top of the input sequence. At each time step SEARCH for the most relevant input to predict the next output.'
 'The authors introduce a new memory model which allows memory access in O(log n) time.']","['This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks.'
 'The model build a hierarchical softmax on top of the input sequence. At each time step SEARCH for the most relevant input to predict the next output.'
 'The authors introduce a new memory model which allows memory access in O(log n) time.']","This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks.    0.833185
The model build a hierarchical softmax on top of the input sequence. At each time step SEARCH for the most relevant input to predict the next output.                                                                                                                              0.924298
The authors introduce a new memory model which allows memory access in O(log n) time.                                                                                                                                                                                              0.569760
dtype: float32","This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks.    1.098612
The model build a hierarchical softmax on top of the input sequence. At each time step SEARCH for the most relevant input to predict the next output.                                                                                                                              1.098612
The authors introduce a new memory model which allows memory access in O(log n) time.                                                                                                                                                                                              1.098612
dtype: float32","{""This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks."":{""This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.  This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.----------------The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.  Improving on these would greatly strengthen the paper, as the core idea is interesting."":-0.6521620154,""This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN\u2026).----------------The model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...----------------The main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log("":-4.1985259056,""The authors introduce a new memory model which allows memory access in O(log n) time.----------------Pros:--------* The paper is well written and everything is clear.--------* It's a new model and I'm not aware of a similar model.--------* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.----------------Cons:--------* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.--------* The model was also not tested on any real-world task.----------------I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model."":-4.0323138237},""The model build a hierarchical softmax on top of the input sequence. At each time step SEARCH for the most relevant input to predict the next output."":{""This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.  This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.----------------The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.  Improving on these would greatly strengthen the paper, as the core idea is interesting."":-10.2601766586,""This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN\u2026).----------------The model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...----------------The main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log("":-6.3505702019,""The authors introduce a new memory model which allows memory access in O(log n) time.----------------Pros:--------* The paper is well written and everything is clear.--------* It's a new model and I'm not aware of a similar model.--------* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.----------------Cons:--------* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.--------* The model was also not tested on any real-world task.----------------I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model."":-10.4957332611},""The authors introduce a new memory model which allows memory access in O(log n) time."":{""This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.  This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.----------------The idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.  Improving on these would greatly strengthen the paper, as the core idea is interesting."":-18.7175159454,""This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN\u2026).----------------The model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...----------------The main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log("":-20.4503116608,""The authors introduce a new memory model which allows memory access in O(log n) time.----------------Pros:--------* The paper is well written and everything is clear.--------* It's a new model and I'm not aware of a similar model.--------* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.----------------Cons:--------* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.--------* The model was also not tested on any real-world task.----------------I think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model."":-16.9727592468}}",All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper.
https://openreview.net/forum?id=ByOK0rwlx,"['This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results.'
 'I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks.'
 '. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The advantage of the given method is unclear.']","['This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results.'
 'I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks.'
 '. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The advantage of the given method is unclear.']","This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results.      0.982429
I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks.                   0.895165
. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The advantage of the given method is unclear.    0.804229
dtype: float32","This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results.      1.098612
I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks.                   1.098612
. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The advantage of the given method is unclear.    1.098612
dtype: float32","{""This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results."":{""This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights."":-3.7911207676,""I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations."":-8.6224622726,""This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.----------------My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the"":-8.0905437469},""I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks."":{""This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights."":-8.4993124008,""I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations."":-4.6725597382,""This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.----------------My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the"":-8.485367775},"". In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The advantage of the given method is unclear."":{""This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights."":-4.6789937019,""I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks. In any case, i would like to see the results when the compression is applied to state of the art nets where the float representation is important. For instance a network with 0.5% - 0.8% in MNIST. A Imagenet lower that 5% - 10%. Some of this results are feasible with float representation but probably imposible for restricted representations."":-4.6069421768,""This paper explores a new quantization method for both the weights and the activations that does not need re-training. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.----------------My major criticisms are three-fold: for one, the results are not compared to one of the many other pruning methods that are described in section 1.1, and as such the performance of the method is difficult to judge from the paper alone. Second, there have been several other compression schemes involving pruning, re-training and vector-quantization [e.g. 1, 2, 3] that seem to achieve much higher accuracies, compression ratios and speed-ups. Hence, for the practical application of running such networks on low-power, low-memory devices, other methods seem to be much more suited. The advantage of the"":-1.3259009123}}","The paper presents a method for quantizing neural network weights and activations. The method is not compared to related state-of-the-art quantization techniques, so in the current form the paper is not ready for acceptance."
https://openreview.net/forum?id=ByQPVFull,"['The paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets.'
 'This paper proposes to use supervision to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.'
 '. The idea is simple, smart, and seems to be effective. But the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet.']","['The paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets.'
 'This paper proposes to use supervision to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.'
 '. The idea is simple, smart, and seems to be effective. But the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet.']","The paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets.    0.857384
This paper proposes to use supervision to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.                                                                 0.702925
. The idea is simple, smart, and seems to be effective. But the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet.                                  0.013750
dtype: float32","The paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets.    1.098612
This paper proposes to use supervision to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.                                                                 1.098612
. The idea is simple, smart, and seems to be effective. But the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet.                                  1.097981
dtype: float32","{""The paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets."":{""This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.------------------------Pros:----------------Proposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.----------------The use of foreground segmentation masks to improve image classification is also novel.----------------The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.------------------------Cons:----------------The evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The"":-1.4855600595,""This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.----------------Pros:--------- The paper is clear and easy to follow--------- The experimental results seem to show some benefit from the proposed approach----------------Cons:--------(1) The paper proposes one core idea (group orthogonality w\/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation--------(2) No comparison with an ensemble--------(3) Full experiments on ImageNet under the \""partial privileged information\"" setting would be more impactful----------------This paper is promising and I would be willing to accept an improved version. However, the current version"":-4.7676429749,""The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \""complementary viewpoints\"" of the input to the subsequent layers, which can be thought of as performing ensembling\/expert combination within the model, rather than using an ensemble of networks. ----------------For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \""foreground\"" and a \""background\"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. ----------------They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"""":-5.4741249084},""This paper proposes to use supervision to assign features to groups in a hand-coded fashion. The developed method is applied to image classification."":{""This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.------------------------Pros:----------------Proposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.----------------The use of foreground segmentation masks to improve image classification is also novel.----------------The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.------------------------Cons:----------------The evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The"":-10.0764884949,""This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.----------------Pros:--------- The paper is clear and easy to follow--------- The experimental results seem to show some benefit from the proposed approach----------------Cons:--------(1) The paper proposes one core idea (group orthogonality w\/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation--------(2) No comparison with an ensemble--------(3) Full experiments on ImageNet under the \""partial privileged information\"" setting would be more impactful----------------This paper is promising and I would be willing to accept an improved version. However, the current version"":-7.4944238663,""The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \""complementary viewpoints\"" of the input to the subsequent layers, which can be thought of as performing ensembling\/expert combination within the model, rather than using an ensemble of networks. ----------------For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \""foreground\"" and a \""background\"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. ----------------They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"""":-10.7532749176},"". The idea is simple, smart, and seems to be effective. But the evaluation is quite limited; the authors use only one network (18 rather than 150 layers) and only part of imagenet."":{""This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group.  The technique is applied in the setting of image classification with \u201cprivileged information\u201d in the form of foreground segmentation masks, where the model is trained to learn orthogonal groups of foreground and background features using the correlation penalty and an additional \u201cbackground suppression\u201d term.------------------------Pros:----------------Proposes a \u201cgroup-wise model diversity\u201d loss term which is novel, to my knowledge.----------------The use of foreground segmentation masks to improve image classification is also novel.----------------The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012.------------------------Cons:----------------The evaluation is lacking.  There should be a baseline that leaves out the background suppression term, so readers know how much that term is contributing to the performance vs. the group orthogonal term.  The"":-4.3267588615,""This paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups such that all pairs of features across all pairs of groups are encouraged to have low statistical correlation. Instead of discovering the groups automatically, the work proposes to use supervision, which they call privileged information, to assign features to groups in a hand-coded fashion. The developed method is applied to image classification.----------------Pros:--------- The paper is clear and easy to follow--------- The experimental results seem to show some benefit from the proposed approach----------------Cons:--------(1) The paper proposes one core idea (group orthogonality w\/ privileged information), but then introduces background feature suppression without much motivation and without careful experimentation--------(2) No comparison with an ensemble--------(3) Full experiments on ImageNet under the \""partial privileged information\"" setting would be more impactful----------------This paper is promising and I would be willing to accept an improved version. However, the current version"":-4.2892684937,""The starting point of this work is the understanding that by having decorrelated neurons (e.g. neurons that only fire on background, or only on foreground regions) one provides independent pieces of information to the subsequent decisions. As such one gives \""complementary viewpoints\"" of the input to the subsequent layers, which can be thought of as performing ensembling\/expert combination within the model, rather than using an ensemble of networks. ----------------For this, the authors propose a sensible method to decorrelate the activations of intermediate neurons, with the aim of delivering complementary inputs to the final classification layers: they split intermediate neurons to a \""foreground\"" and a \""background\"" subset, and append side-losses that force them to be zero on background and foreground pixels respectively. ----------------They demonstrate that this can improve classification on a mid-scale classification example (a fraction of imagenet, and a ResNet with 18, rather than 150 layers), when compared to a \"""":-3.9681603909}}","This paper was reviewed by three experts. While they find interesting ideas in the manuscript, all three point to deficiencies (lack of clean experiments, clarity in the manuscript, etc) and recommend rejection. I believe there are promising ideas here, and this manuscript will be stronger for a future deadline."
https://openreview.net/forum?id=Byj72udxe,"['This paper proposes augmenting RNN-based language models with a pointer network. The paper also introduces a new language modelling dataset.'
 '. This work is basically a combined pointer network applied on language modelling. A memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences.'
 'This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs.']","['This paper proposes augmenting RNN-based language models with a pointer network. The paper also introduces a new language modelling dataset.'
 '. This work is basically a combined pointer network applied on language modelling. A memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences.'
 'This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs.']","This paper proposes augmenting RNN-based language models with a pointer network. The paper also introduces a new language modelling dataset.                                                            0.617992
. This work is basically a combined pointer network applied on language modelling. A memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences.    0.862606
This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs.                                                                                    0.826150
dtype: float32","This paper proposes augmenting RNN-based language models with a pointer network. The paper also introduces a new language modelling dataset.                                                            1.098612
. This work is basically a combined pointer network applied on language modelling. A memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences.    1.098612
This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs.                                                                                    1.098612
dtype: float32","{""This paper proposes augmenting RNN-based language models with a pointer network. The paper also introduces a new language modelling dataset."":{""This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.----------------The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words"":-5.7654047012,""This work is basically a combined pointer network applied on language modelling. --------The smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences. --------Hence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words. ----------------Generally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate. --------This paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling. --------I would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results. ----------------In addition, the new WikiText language modelling dataset is very interesting. --------It probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset."":-7.8275918961,""This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. --------The idea is appealing in general for context biasing and the specific approach appears quite simple.----------------The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,--------but not as a mixture model, as in this paper.----------------The paper is clearly written and the results seem promising.--------The new dataset the authors created (WikiText) also seems of high interest. ----------------A comment regarding notation:--------The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i) --------This is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words.--------It would"":-9.0776748657},"". This work is basically a combined pointer network applied on language modelling. A memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences."":{""This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.----------------The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words"":-4.3095273972,""This work is basically a combined pointer network applied on language modelling. --------The smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences. --------Hence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words. ----------------Generally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate. --------This paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling. --------I would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results. ----------------In addition, the new WikiText language modelling dataset is very interesting. --------It probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset."":-0.9398646951,""This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. --------The idea is appealing in general for context biasing and the specific approach appears quite simple.----------------The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,--------but not as a mixture model, as in this paper.----------------The paper is clearly written and the results seem promising.--------The new dataset the authors created (WikiText) also seems of high interest. ----------------A comment regarding notation:--------The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i) --------This is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words.--------It would"":-4.8676767349},""This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs."":{""This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.----------------The reason for the score I gave for this paper is that I find the proposed model a direct application of the previous work Gulcehre et al., which follows a similar approach but for machine translation and summarization. The main differences I find is that Gulcehre et al. use an encoder-decoder architecture, and use the attention weights of the encoder to point to locations of words in the input, while here an RNN is used and a pointer network produces a distribution over the full vocabulary (by summing the softmax probabilities of words"":-12.1544427872,""This work is basically a combined pointer network applied on language modelling. --------The smart point is that this paper aims at language modelling with longer context, where a memory of seen words (especially the rare words) would be very useful for predicting the rest of the sentences. --------Hence, a combination of a pointer network and a standard language model would balance the copying seen words and predicting unseen words. ----------------Generally, such as the combined pointer networks applied in sentence compression, a vector representation of the source sequence would be used to compute the gate. --------This paper, instead, introduces a sentinel vector to carry out the mixture model, which is suitable in the case of language modelling. --------I would be interested in the variations of sentinel mixture implementation, though the current version has achieved very good results. ----------------In addition, the new WikiText language modelling dataset is very interesting. --------It probably can be a more standard dataset for evaluating the continuously-updated language model benchmarks than ptb dataset."":-12.737112999,""This work is an extension of previous works on pointer models, that mixes its outputs with standard softmax outputs. --------The idea is appealing in general for context biasing and the specific approach appears quite simple.----------------The idea is novel to some extent, as previous paper had already tried to combine pointer-based and standard models,--------but not as a mixture model, as in this paper.----------------The paper is clearly written and the results seem promising.--------The new dataset the authors created (WikiText) also seems of high interest. ----------------A comment regarding notation:--------The symbol p_ptr is used in two different ways in eq. 3 and eq. 5. : p_ptr(w) vs. p_ptr(y_i|x_i) --------This is confusing as these are two different domains: for eq 3. the domain is a *set* of words and for eq. 5 the domain is a *list* of context words.--------It would"":-8.9919967651}}","The reviewers liked this paper quite a bit, and so for this reason it is a perfectly fine paper to accept. However, it should be noted that the area chair was less enthusiastic. The area chairs mentions that the model appears to be an extension of Gulcehre et al. and the Penn Treebank perplexity experiments are too small scale to be taken seriously in 2017. Instead of experimenting on other known large-scale language modeling setups, the authors introduce their own new dataset (which is 1 order of magnitude smaller than the 1-Billion LM dataset by Chelba et al). The new dataset might be a good idea, but the area chair doesn't understand why the authors do not run public available systems as baselines. This should have been fairly easy to do and would have significantly strengthen the result of this work. The PCs thus encourage to authors to take into account this feedback and consider updating their paper accordingly."
https://openreview.net/forum?id=BymIbLKgl,"["". There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I wouldn't mind seeing this paper accepted, since it""
 'Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants.'
 '. A new representation with nice properties that are derived and compared with a mathematical baseline and background. A simple algorithm to obtain the representation.']","["". There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I wouldn't mind seeing this paper accepted, since it""
 'Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants.'
 '. A new representation with nice properties that are derived and compared with a mathematical baseline and background. A simple algorithm to obtain the representation.']",". There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I wouldn't mind seeing this paper accepted, since it                                                           0.750966
Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants.    0.945809
. A new representation with nice properties that are derived and compared with a mathematical baseline and background. A simple algorithm to obtain the representation.                                                                                                        0.988029
dtype: float32",". There is no question that the MPEG-7 dataset/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I wouldn't mind seeing this paper accepted, since it                                                           1.098612
Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants.    1.098612
. A new representation with nice properties that are derived and compared with a mathematical baseline and background. A simple algorithm to obtain the representation.                                                                                                        1.098612
dtype: float32","{"". There is no question that the MPEG-7 dataset\/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I wouldn't mind seeing this paper accepted, since it"":{""I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that \""if it's not worth doing, it's not worth doing well.\"" There is no question that the MPEG-7 dataset\/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of \""why use this representation\"" with the authors and they said their \""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.\"" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use"":-2.011412859,""Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.----------------The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.----------------Furthermore"":-5.0367889404,""Pros : --------- New representation with nice properties that are derived and compared with a mathematical baseline and background--------- A simple algorithm to obtain the representation----------------Cons :--------- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first."":-5.1442527771},""Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants."":{""I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that \""if it's not worth doing, it's not worth doing well.\"" There is no question that the MPEG-7 dataset\/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of \""why use this representation\"" with the authors and they said their \""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.\"" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use"":-4.6095423698,""Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.----------------The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.----------------Furthermore"":-0.5129746795,""Pros : --------- New representation with nice properties that are derived and compared with a mathematical baseline and background--------- A simple algorithm to obtain the representation----------------Cons :--------- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first."":-4.8077077866},"". A new representation with nice properties that are derived and compared with a mathematical baseline and background. A simple algorithm to obtain the representation."":{""I'm torn on this one. Seeing the MPEG-7 dataset and references to curvature scale space brought to mind the old saying that \""if it's not worth doing, it's not worth doing well.\"" There is no question that the MPEG-7 dataset\/benchmark got saturated long ago, and it's quite surprising to see it in a submission to a modern ML conference. I brought up the question of \""why use this representation\"" with the authors and they said their \""main purpose was to connect the theory of differential geometry of curves with the computational engine of a convolutional neural network.\"" Fair enough. I agree these are seemingly different fields, and the authors deserve some credit for connecting them. If we give them the benefit of the doubt that this was worth doing, then the approach they pursue using a Siamese configuration makes sense, and their adaptation of deep convnet frameworks to 1D signals is reasonable. To the extent that the old invariant based methods made use"":-14.690372467,""Authors show that a contrastive loss for a Siamese architecture can be used for learning representations for planar curves. With the proposed framework, authors are able to learn a representation which is comparable to traditional differential or integral invariants, as evaluated on few toy examples.----------------The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes. It is well known fact that in order to achieve a good performance with the contrastive loss, one has to be careful about the hard negative sampling, as using too easy negatives may lead to inferior results. Thus, this may be the underlying reason for such choice of the negatives? Unfortunately, this is not discussed in the paper.----------------Furthermore"":-14.2172241211,""Pros : --------- New representation with nice properties that are derived and compared with a mathematical baseline and background--------- A simple algorithm to obtain the representation----------------Cons :--------- The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done, for instance, by understanding the nature of each layer, or at least, the first."":-9.8324108124}}","This work proposes learning of local representations of planar curves using convolutional neural networks.  Invariance to rigid transformations and discriminability are enforced with a metric learning framework using a siamese architecture. Preliminary experiments on toy datasets compare favorably with predefined geometric invariants (both differential and integral).     The reviewers found value on the problem set-up and the proposed model, and were generally satisfied with the author's response. They also expressed concern that the experimental section is currently a bit weak and does not include any real data. Also, the paper does not offer theoretical insights that inform us about the design of the representation or about the provable invariance guarantees. All things considered, the AC recommends acceptance in the form of a poster, but strongly encourages the authors to strengthen the work both in the experimental and the theoretical aspects."
https://openreview.net/forum?id=H1kjdOYlx,"['The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL. The experiments are not described in enough detail in the paper.'
 'This paper studies the problem of abstract hierarchical multiagent RL with policy sketches. Policy sketches are high level descriptions of abstract actions. Learning occurs through a variant of the standard actor critic architecture.'
 'Paper proposes a new RL architecture that aims at learning policies from sketches.']","['The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL. The experiments are not described in enough detail in the paper.'
 'This paper studies the problem of abstract hierarchical multiagent RL with policy sketches. Policy sketches are high level descriptions of abstract actions. Learning occurs through a variant of the standard actor critic architecture.'
 'Paper proposes a new RL architecture that aims at learning policies from sketches.']","The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL. The experiments are not described in enough detail in the paper.                                                       0.383132
This paper studies the problem of abstract hierarchical multiagent RL with policy sketches. Policy sketches are high level descriptions of abstract actions. Learning occurs through a variant of the standard actor critic architecture.    0.927188
Paper proposes a new RL architecture that aims at learning policies from sketches.                                                                                                                                                           0.648751
dtype: float32","The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL. The experiments are not described in enough detail in the paper.                                                       1.098610
This paper studies the problem of abstract hierarchical multiagent RL with policy sketches. Policy sketches are high level descriptions of abstract actions. Learning occurs through a variant of the standard actor critic architecture.    1.098612
Paper proposes a new RL architecture that aims at learning policies from sketches.                                                                                                                                                           1.098612
dtype: float32","{""The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL. The experiments are not described in enough detail in the paper."":{""The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding \""implementations\"" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that"":-4.5289669037,""This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. ----------------Sketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. ----------------Experiments are provided through a standard game like domain (maze, minecraft etc.). ----------------The paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the"":-6.0104169846,""The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. ----------------The paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant"":-6.7953772545},""This paper studies the problem of abstract hierarchical multiagent RL with policy sketches. Policy sketches are high level descriptions of abstract actions. Learning occurs through a variant of the standard actor critic architecture."":{""The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding \""implementations\"" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that"":-5.2154269218,""This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. ----------------Sketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. ----------------Experiments are provided through a standard game like domain (maze, minecraft etc.). ----------------The paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the"":-0.8838357925,""The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. ----------------The paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant"":-4.6909680367},""Paper proposes a new RL architecture that aims at learning policies from sketches."":{""The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The approach is illustrated in two tasks: gridworld with objects and a simplified Minecraft problem).  The idea of providing symbolic descriptions of tasks and learning corresponding \""implementations\"" is potentially interesting and the empirical results are promising.  However, there are two main drawbacks of the current incarnation of this work.  First, the ideas presented in the paper have all been explored in other work (symbolic specifications, actor-critic, shared representations).  While related work is discussed, it is not really clear what is new here, and what is the main contribution of this work besides providing a new implementation of existing ideas in the context of deep learning. The main contribution if the work needs to be clearly spelled out.  Secondly, the approach presented relies crucially on curriculum learning (this is quite clear from the experiments).  While the authors argue that"":-22.5210762024,""This paper studies the problem of abstract hierarchical multiagent RL with policy sketches, high level descriptions of abstract actions. The work is related to much previous work in hierarchical RL, and adds some new elements by using neural implementations of prior work on hierarchical learning and skill representations. ----------------Sketches are sequences of high level symbolic labels drawn from some fixed vocabulary, which initially are devoid of any meaning. Eventually the sketches get mapped into real policies and enable policy transfer and temporal abstraction. Learning occurs through a variant of the standard actor critic architecture. ----------------Experiments are provided through a standard game like domain (maze, minecraft etc.). ----------------The paper as written suffers from two problems. One, the idea of policy sketches is nice, but not sufficiently fleshed out to have any real impact. It would have been useful to see this spelled out in the context of abstract SMDP models to see what they bring to the table. What one gets here is some specialized invocation of this idea in the"":-21.9446926117,""The paper proposes a new RL architecture that aims at learning policies from sketches i.e sequence of high-level operations to execute for solving a particular task. The model relies on a hierarchical structure where the sub-policy is chosen depending on the current operation to execute in the sketch . The learning algorithm is based on an extension of the actor-critic model for that particular case, and also involves curriculum learning techniques when the task to solve is hard. Experimental results are provided on different learning problems and compared to baseline methods. ----------------The paper is well-written and very easy to follow. I am not really convinced by the impact of such a paper since the problem solved here can be seen as an option-learning problem with a richer supervision (i.e the sequence of option is given). It thus corresponds to an easier problem with a limited impact. Moreover, I do not really understand to which concrete application this setting corresponds. For example, learning from natural langage instructions is clearly more relevant"":-19.5225505829}}","As per all the reviews, the work is clearly promising, but is seen to need additional discussion / formalization / experimental comparison with related work, and stronger demonstrations of the application of this technique.  Further back-and-forth with the reviewers would have been useful, but there should be enough to go on in terms of directions. This work would benefit from being part of the workshop track."
https://openreview.net/forum?id=H1oyRlYgg,"['Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well.'
 'The paper shows that SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability.'
 'I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime.']","['Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well.'
 'The paper shows that SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability.'
 'I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime.']","Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well.                                                        0.986018
The paper shows that SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability.                                                        0.914572
I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime.    0.904164
dtype: float32","Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well.                                                        1.098612
The paper shows that SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability.                                                        1.098612
I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime.    1.098612
dtype: float32","{""Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well."":{""Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well"":-7.9783449173,""The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. ----------------Pros and Cons:--------Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. ----------------Significance:--------I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.----------------Comments:--------Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version."":-12.3872251511,""I think that the paper is quite interesting and useful. --------It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime."":-12.7494506836},""The paper shows that SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability."":{""Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well"":-8.0107812881,""The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. ----------------Pros and Cons:--------Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. ----------------Significance:--------I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.----------------Comments:--------Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version."":-4.1758880615,""I think that the paper is quite interesting and useful. --------It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime."":-8.2550773621},""I think that the paper is quite interesting and useful. It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime."":{""Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well"":-4.5271477699,""The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter minima have better generalization ability. ----------------Pros and Cons:--------Although there is little novelty in the paper, I think the work is of great value in shedding light into some interesting questions around generalization of deep networks. ----------------Significance:--------I think such results may have impact on both theory and practice, respectively by suggesting what assumptions are legitimate for real scenarios for building new theories, or be used heuristically to develop new algorithms with generalization by smart manipulation of mini-batch sizes.----------------Comments:--------Earlier I had some concern about the correctness of a claim made by the authors, which is resolved now. They had claimed their proposed sharpness criterion is scale invariance. They took care of it by removing this claim in the revised version."":-4.2605314255,""I think that the paper is quite interesting and useful. --------It might benefit from additional investigations, e.g., by adding some rescaled Gaussian noise to gradients during the LB regime one can get advantages of the SB regime."":-0.5079032183}}","The values of epsilon in Figure 5 are set as: 1e-3 and 5e-3.  Based on the previous experiments, and the fact that sharpness of 5e-3 values are less than sharpness of 1e-3 values, it is my understanding that 5e-3 should be changed to 5e-4. "
https://openreview.net/forum?id=HJ0NvFzxl,"[' to learn a cellular automaton represented as sequence of graphs. Proposed architecture allows one to learn this sequence of graphs.'
 'This paper proposes learning on the fly to represent a dialog as a graph. It is first demonstrated on the bAbI tasks. Graph learning is part of the inference process.'
 'The paper proposes an extension of the Gated Graph Sequence Neural Network. The underlying idea is to build/modify a graph-structure as an internal representation for solving a problem.']","[' to learn a cellular automaton represented as sequence of graphs. Proposed architecture allows one to learn this sequence of graphs.'
 'This paper proposes learning on the fly to represent a dialog as a graph. It is first demonstrated on the bAbI tasks. Graph learning is part of the inference process.'
 'The paper proposes an extension of the Gated Graph Sequence Neural Network. The underlying idea is to build/modify a graph-structure as an internal representation for solving a problem.']"," to learn a cellular automaton represented as sequence of graphs. Proposed architecture allows one to learn this sequence of graphs.                                                         0.776275
This paper proposes learning on the fly to represent a dialog as a graph. It is first demonstrated on the bAbI tasks. Graph learning is part of the inference process.                       0.899613
The paper proposes an extension of the Gated Graph Sequence Neural Network. The underlying idea is to build/modify a graph-structure as an internal representation for solving a problem.    0.778831
dtype: float32"," to learn a cellular automaton represented as sequence of graphs. Proposed architecture allows one to learn this sequence of graphs.                                                         1.098612
This paper proposes learning on the fly to represent a dialog as a graph. It is first demonstrated on the bAbI tasks. Graph learning is part of the inference process.                       1.098612
The paper proposes an extension of the Gated Graph Sequence Neural Network. The underlying idea is to build/modify a graph-structure as an internal representation for solving a problem.    1.098612
dtype: float32","{"" to learn a cellular automaton represented as sequence of graphs. Proposed architecture allows one to learn this sequence of graphs."":{""The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0\/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.----------------This idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. "":-7.5955858231,""This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.----------------The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has"":-10.7993888855,""The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build\/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.----------------The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus"":-10.7673540115},""This paper proposes learning on the fly to represent a dialog as a graph. It is first demonstrated on the bAbI tasks. Graph learning is part of the inference process."":{""The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0\/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.----------------This idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. "":-5.3667383194,""This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.----------------The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has"":-1.4385204315,""The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build\/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.----------------The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus"":-5.2125754356},""The paper proposes an extension of the Gated Graph Sequence Neural Network. The underlying idea is to build\/modify a graph-structure as an internal representation for solving a problem."":{""The main contribution of this paper seems to be an introduction of a set of differential graph transformations which will allow you to learn graph->graph classification tasks using gradient descent. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0\/1 representing the values. Proposed architecture allows one to learn this sequence of graphs, although in the experiment, this task (Rule 30) was far from solved.----------------This idea is combined with ideas from previous papers (GGS-NN) to allow the model to produce textual output rather than graph output, and use graphs as intermediate representation, which allows it to beat state of the art on BaBi tasks. "":-3.8922278881,""This paper proposes learning on the fly to represent a dialog as a graph (which acts as the memory), and is first demonstrated on the bAbI tasks. Graph learning is part of the inference process, though there is long term representation learning to learn graph transformation parameters and the encoding of sentences as input to the graph. This seems to be the first implementation of a differentiable memory as graph: it is much more complex than previous approaches like memory networks without significant gain in performance in bAbI tasks, but it is still very preliminary work, and the representation of memory as a graph seems much more powerful than a stack. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved later version. This original, technically accurate (within what I understood) and thought provoking paper is worth publishing.----------------The preliminary results do not tell us yet if the highly complex graph-based differentiable memory has"":-4.2203907967,""The paper proposes an extension of the Gated Graph Sequence Neural Network by including in this model the ability to produce complex graph transformations. The underlying idea is to propose a method that will be able build\/modify a graph-structure as an internal representation for solving a problem, and particularly for solving question-answering problems in this paper. The author proposes 5 different possible differentiable transformations that will be learned on a training set, typically in a supervised fashion where the state of the graph is given at each timestep. A particular occurence of the model is presented that takes a sequence as an input a iteratively update an internal graph state to a final prediction, and which can be applied for solving QA tasks (e.g BaBi) with interesting results.----------------The approach  in this paper is really interesting since the proposed model is able to maintain a representation of its current state as a complex graph, but still keeping the property of being differentiable and thus"":-0.8482904434}}","The idea of building a graph-based differentiable memory is very good. The proposed approach is quite complex, but it is likely to lead to future developments and extensions. The paper has been much improved since the original submission. The results could be strengthened, with more comparisons to existing results on bAbI and baselines on the experiments here. Exploring how it performs with less supervision, and different types of supervision, from entirely labeled graphs versus just node labels, would be valuable."
https://openreview.net/forum?id=HJ6idTdgg,"['Paper proposed to use EdgeBoxes + Fast-RCNN with batch normalization for pedestrian detection. Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and the'
 '. Paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in original Fast RCNN method.'
 'The authors apply the commonly used Fast RCNN detection system to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work.'
 ""This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. The paper's two contributions are too minor to merit publication.""]","['Paper proposed to use EdgeBoxes + Fast-RCNN with batch normalization for pedestrian detection. Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and the'
 '. Paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in original Fast RCNN method.'
 'The authors apply the commonly used Fast RCNN detection system to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work.'
 ""This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. The paper's two contributions are too minor to merit publication.""]","Paper proposed to use EdgeBoxes + Fast-RCNN with batch normalization for pedestrian detection. Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and the    0.816110
. Paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in original Fast RCNN method.      0.853695
The authors apply the commonly used Fast RCNN detection system to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work.                                                   0.572742
This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. The paper's two contributions are too minor to merit publication.                                   0.956597
dtype: float32","Paper proposed to use EdgeBoxes + Fast-RCNN with batch normalization for pedestrian detection. Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and the    1.386294
. Paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in original Fast RCNN method.      1.386294
The authors apply the commonly used Fast RCNN detection system to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work.                                                   1.386294
This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. The paper's two contributions are too minor to merit publication.                                   1.386294
dtype: float32","{""Paper proposed to use EdgeBoxes + Fast-RCNN with batch normalization for pedestrian detection. Results do not cover enough datasets, the reported results do not improve over state of the art, writing is poor, and the"":{""Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with--------batch normalization for pedestrian detection----------------Review summary: results do not cover enough datasets, the reported--------results do not improve over state of the art, writing is poor, and--------overall the work lacks novelty. This is a clear reject.----------------Pros:--------* Shows that using batch normalization does improve results----------------Cons:--------* Only results on ETH and INRIA. Should include Caltech or KITTI.--------* Reported results are fair, but not improving over state of the art--------* Overall idea of limited interest when considering works like S.--------Zhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang--------ECCV 2017 (Faster R-CNN for pedestrian detection)--------* Issues with the text quality--------* Limited takeaways----------------Quality: low--------Clarity: fair, but poor English--------Originality: low--------Signific"":-0.9362589121,""This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method. The proposed method is evaluated in INRIA and ETH dataset.----------------Pros:--------- The proposed method shows good performance(but not state-of-the-art).----------------Cons:--------- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, https:\/\/arxiv.org\/abs\/1506.01497; Faster RCNN with ResNet, https:\/\/arxiv.org\/abs\/1512.03385) are widely used in object detection tasks in many literatures. Meanwhile, batch normalization is a common practice to train \/ tune deep networks. It is not new to use batch normalization in CNNs for object \/ pedestrian detection. ----------------- The authors claim using EdgeBoxes for pedestrian proposal is another main"":-3.5759270191,""The authors apply the commonly used Fast RCNN detection system to pedestrian detection. They use \u201cEdgeBoxes\u201d object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets. They are reasonable but not state-of-the-art. Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection. Perhaps more importantly, the paper has no novelty.----------------The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. EdgeBoxes has been used with Fast RCNN before. The authors don\u2019t seem to be aware of more recent developments in object detection, including Faster RCNN (https:\/\/arxiv.org\/abs\/1506.01497), which uses a deep net to perform object proposals and performs much better than Edge"":-3.9725716114,""This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.----------------Pros--------- The paper is clearly written and easy to follow----------------Cons--------- The paper's two contributions are too minor to merit publication--------- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset--------- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed----------------My rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing"":-3.9337131977},"". Paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in original Fast RCNN method."":{""Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with--------batch normalization for pedestrian detection----------------Review summary: results do not cover enough datasets, the reported--------results do not improve over state of the art, writing is poor, and--------overall the work lacks novelty. This is a clear reject.----------------Pros:--------* Shows that using batch normalization does improve results----------------Cons:--------* Only results on ETH and INRIA. Should include Caltech or KITTI.--------* Reported results are fair, but not improving over state of the art--------* Overall idea of limited interest when considering works like S.--------Zhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang--------ECCV 2017 (Faster R-CNN for pedestrian detection)--------* Issues with the text quality--------* Limited takeaways----------------Quality: low--------Clarity: fair, but poor English--------Originality: low--------Signific"":-5.7137188911,""This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method. The proposed method is evaluated in INRIA and ETH dataset.----------------Pros:--------- The proposed method shows good performance(but not state-of-the-art).----------------Cons:--------- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, https:\/\/arxiv.org\/abs\/1506.01497; Faster RCNN with ResNet, https:\/\/arxiv.org\/abs\/1512.03385) are widely used in object detection tasks in many literatures. Meanwhile, batch normalization is a common practice to train \/ tune deep networks. It is not new to use batch normalization in CNNs for object \/ pedestrian detection. ----------------- The authors claim using EdgeBoxes for pedestrian proposal is another main"":-2.7850041389,""The authors apply the commonly used Fast RCNN detection system to pedestrian detection. They use \u201cEdgeBoxes\u201d object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets. They are reasonable but not state-of-the-art. Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection. Perhaps more importantly, the paper has no novelty.----------------The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. EdgeBoxes has been used with Fast RCNN before. The authors don\u2019t seem to be aware of more recent developments in object detection, including Faster RCNN (https:\/\/arxiv.org\/abs\/1506.01497), which uses a deep net to perform object proposals and performs much better than Edge"":-5.5386209488,""This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.----------------Pros--------- The paper is clearly written and easy to follow----------------Cons--------- The paper's two contributions are too minor to merit publication--------- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset--------- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed----------------My rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing"":-6.1265449524},""The authors apply the commonly used Fast RCNN detection system to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work."":{""Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with--------batch normalization for pedestrian detection----------------Review summary: results do not cover enough datasets, the reported--------results do not improve over state of the art, writing is poor, and--------overall the work lacks novelty. This is a clear reject.----------------Pros:--------* Shows that using batch normalization does improve results----------------Cons:--------* Only results on ETH and INRIA. Should include Caltech or KITTI.--------* Reported results are fair, but not improving over state of the art--------* Overall idea of limited interest when considering works like S.--------Zhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang--------ECCV 2017 (Faster R-CNN for pedestrian detection)--------* Issues with the text quality--------* Limited takeaways----------------Quality: low--------Clarity: fair, but poor English--------Originality: low--------Signific"":-5.7980065346,""This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method. The proposed method is evaluated in INRIA and ETH dataset.----------------Pros:--------- The proposed method shows good performance(but not state-of-the-art).----------------Cons:--------- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, https:\/\/arxiv.org\/abs\/1506.01497; Faster RCNN with ResNet, https:\/\/arxiv.org\/abs\/1512.03385) are widely used in object detection tasks in many literatures. Meanwhile, batch normalization is a common practice to train \/ tune deep networks. It is not new to use batch normalization in CNNs for object \/ pedestrian detection. ----------------- The authors claim using EdgeBoxes for pedestrian proposal is another main"":-5.4346375465,""The authors apply the commonly used Fast RCNN detection system to pedestrian detection. They use \u201cEdgeBoxes\u201d object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets. They are reasonable but not state-of-the-art. Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection. Perhaps more importantly, the paper has no novelty.----------------The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. EdgeBoxes has been used with Fast RCNN before. The authors don\u2019t seem to be aware of more recent developments in object detection, including Faster RCNN (https:\/\/arxiv.org\/abs\/1506.01497), which uses a deep net to perform object proposals and performs much better than Edge"":-3.5645008087,""This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.----------------Pros--------- The paper is clearly written and easy to follow----------------Cons--------- The paper's two contributions are too minor to merit publication--------- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset--------- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed----------------My rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing"":-6.302687645},""This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. The paper's two contributions are too minor to merit publication."":{""Paper summary: the authors proposed to use EdgeBoxes + Fast-RCNN with--------batch normalization for pedestrian detection----------------Review summary: results do not cover enough datasets, the reported--------results do not improve over state of the art, writing is poor, and--------overall the work lacks novelty. This is a clear reject.----------------Pros:--------* Shows that using batch normalization does improve results----------------Cons:--------* Only results on ETH and INRIA. Should include Caltech or KITTI.--------* Reported results are fair, but not improving over state of the art--------* Overall idea of limited interest when considering works like S.--------Zhang CVPR 2016 (Fast R-CNN for pedestrian detection) and L. Zhang--------ECCV 2017 (Faster R-CNN for pedestrian detection)--------* Issues with the text quality--------* Limited takeaways----------------Quality: low--------Clarity: fair, but poor English--------Originality: low--------Signific"":-6.6311583519,""This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization, where EdgeBoxes is used to collect pedestrian proposals instead of selective search as used in the original Fast RCNN method. The proposed method is evaluated in INRIA and ETH dataset.----------------Pros:--------- The proposed method shows good performance(but not state-of-the-art).----------------Cons:--------- Lack of novelty. Fast RCNN and its variants (e.g. FasterRCNN, https:\/\/arxiv.org\/abs\/1506.01497; Faster RCNN with ResNet, https:\/\/arxiv.org\/abs\/1512.03385) are widely used in object detection tasks in many literatures. Meanwhile, batch normalization is a common practice to train \/ tune deep networks. It is not new to use batch normalization in CNNs for object \/ pedestrian detection. ----------------- The authors claim using EdgeBoxes for pedestrian proposal is another main"":-7.449201107,""The authors apply the commonly used Fast RCNN detection system to pedestrian detection. They use \u201cEdgeBoxes\u201d object proposals and incorporate batch norm into their network. Results are shown on the INRIA and ETH pedestrian datasets. They are reasonable but not state-of-the-art. Results are not shown on Caltech Pedestrians, the standard modern dataset used to evaluate pedestrian detection. Perhaps more importantly, the paper has no novelty.----------------The detection system described in this paper is a standard application of Fast RCNN to pedestrian detection. The implementation is not state-of-the-art, and there is no novelty in this work. EdgeBoxes has been used with Fast RCNN before. The authors don\u2019t seem to be aware of more recent developments in object detection, including Faster RCNN (https:\/\/arxiv.org\/abs\/1506.01497), which uses a deep net to perform object proposals and performs much better than Edge"":-7.3844618797,""This paper presents experimental results from an EdgeBoxes + Fast R-CNN detector on the task of localizing pedestrians. It uses an AlexNet (CaffeNet) backbone architecture modified to include batch normalization. Experimental results are presented on the INRIA and ETH datasets.----------------Pros--------- The paper is clearly written and easy to follow----------------Cons--------- The paper's two contributions are too minor to merit publication--------- Experimental results should include at least the Caltech pedestrian dataset but likely also the KITTI pedestrian dataset--------- Recent work from ECCV 2016 [a], with superior results and much more experimental evaluation, is not cited or discussed----------------My rating is due primarily to the lack luster contributions. The first claimed contribution is the use of EdgeBoxes as proposals for pedestrian detection. Unless the result of this choice produced a truly surprising experimental result, this is simply too minor to be considered a contribution. Moreover, if this choice is important, then the paper should justify it by showing"":-3.7938802242}}",Four knowledgable reviewers recommend rejection due to too weak of a contribution. The authors did not post a rebuttal. The AC agrees with the reviewers' recommendation.
https://openreview.net/forum?id=HJ7O61Yxe,"['The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach.'
 'Modeling relational timeseries is a well-researched problem. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way.'
 'The model is evaluated on four datasets and compared to several baselines. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small.']","['The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach.'
 'Modeling relational timeseries is a well-researched problem. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way.'
 'The model is evaluated on four datasets and compared to several baselines. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small.']","The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach.                                                                        0.857861
Modeling relational timeseries is a well-researched problem. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way.    0.915029
The model is evaluated on four datasets and compared to several baselines. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small.    0.284036
dtype: float32","The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach.                                                                        1.098612
Modeling relational timeseries is a well-researched problem. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way.    1.098612
The model is evaluated on four datasets and compared to several baselines. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small.    1.098609
dtype: float32","{""The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach."":{""Because the authors did not respond to reviewer feedback, I am maintaining my original review score.-------------------------------------This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account."":-9.5650587082,""This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).----------------Modeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.----------------The formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1]"":-13.1956663132,""In absence of authors' response, the rating is maintained.-----------------------------------This paper introduces a nonlinear dynamical model for multiple related multivariate time series. It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable). The predictions\/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix. Inference is forward only.----------------The model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.----------------The model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.----------------"":-13.1239786148},""Modeling relational timeseries is a well-researched problem. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way."":{""Because the authors did not respond to reviewer feedback, I am maintaining my original review score.-------------------------------------This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account."":-4.6196904182,""This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).----------------Modeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.----------------The formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1]"":-0.6744545102,""In absence of authors' response, the rating is maintained.-----------------------------------This paper introduces a nonlinear dynamical model for multiple related multivariate time series. It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable). The predictions\/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix. Inference is forward only.----------------The model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.----------------The model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.----------------"":-4.6388640404},""The model is evaluated on four datasets and compared to several baselines. Although the experiments seem to suggest that the proposed model tends to outperform RNNs, the datasets are very small."":{""Because the authors did not respond to reviewer feedback, I am maintaining my original review score.-------------------------------------This paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach: they design a flexible parametric (but not generative) model with Gaussian latent factors and fit it using a rich training objective including terms for reconstruction (of observed time series) error, smoothness in the latent state space (via a KL divergence term encouraging neighbor states to be similarly distributed), and a final regularizer that encourages related time series to have similar latent state trajectories. Relations between trajectories are hard coded based on pre-existing knowledge, i.e., latent state trajectories for neighboring (wind speed) base stations should be similar. The model appears to be fit using gradient simple descent. The authors propose several elaborations, including a nonlinear transition function (based on an MLP) and a reconstruction error term that takes variance into account."":-5.7467432022,""This manuscript proposes an approach for modeling correlated timeseries through a combination of loss functions which depend on neural networks. The loss functions correspond to: data fit term, autoregressive latent state term, and a term which captures relations between pairs of timeseries (relations have to be given as prior information).----------------Modeling relational timeseries is a well-researched problem, however little attention has been given to it in the neural network community. Perhaps the reason for this is the importance of having uncertainty in the representation. The authors correctly identify this need and consider an approach which considers distributions in the state space.----------------The formulation is quite straightforward by combining loss functions. The model adds to Ziat et al. 2016 in certain aspects which are well motivated, but unfortunately implemented in an unconvincing way. To start with, uncertainty is not treated in a very principled way, since the inference in the model is rather naive; I'd expect employing a VAE framework [1]"":-5.9854168892,""In absence of authors' response, the rating is maintained.-----------------------------------This paper introduces a nonlinear dynamical model for multiple related multivariate time series. It models a linear observation model conditioned on the latent variables, a linear or nonlinear dynamical model between consecutive latent variables and a similarity constraint between any two time series (provided as prior data and non-learnable). The predictions\/constraints given by the three components of the model are Gaussian, because the model predicts both the mean and the variance or covariance matrix. Inference is forward only.----------------The model is evaluated on four datasets, and compared to several baselines: plain auto-regressive models, feed-forward networks, RNN and dynamic factor graphs DFGs, which are RNNs with forward and backward inference of the latent variables.----------------The model, which introduces lateral constraints between different time series, and which predicts both the mean and covariance seems interesting, but presents two limitations.----------------"":-4.3119821548}}","The reviews of this paper seem to be very aligned: many of the ideas presented in the paper are interesting, the problem is important, and the results encouraging but preliminary. R2 thought the paper could be improved in terms of clarity and offered several specific suggestions to this end. R2 and R1 mentioned the limitations of the linear decoder; which is not a critical flaw, in my opinion, but as R1 points out, many recent works have explored nonlinear decoders and these could be at least discussed, if not compared. All of the reviewers have worked in this area and expressed high-confidence reviews.    I was surprised that the authors did not provide feedback or revise the paper at least with reference to the clarity/presentation suggestions. It seems this may have had an impact on the perception of the reviewers. I encourage the authors to revise the paper in light of the reviews and re-submit to another venue."
https://openreview.net/forum?id=HJWzXsKxx,"['The findings of applying sparsity in the backward gradients for training LSTMs is interesting. The paper seems incomplete without the proper experimental justification.'
 'This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. It will be much more convincing if a well known dataset and experiment set up are used.'
 'This paper shows that rounding small gradients to zero results in matrices with up to 80% sparsity during training. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training']","['The findings of applying sparsity in the backward gradients for training LSTMs is interesting. The paper seems incomplete without the proper experimental justification.'
 'This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. It will be much more convincing if a well known dataset and experiment set up are used.'
 'This paper shows that rounding small gradients to zero results in matrices with up to 80% sparsity during training. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training']","The findings of applying sparsity in the backward gradients for training LSTMs is interesting. The paper seems incomplete without the proper experimental justification.                                                                                                          0.943212
This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. It will be much more convincing if a well known dataset and experiment set up are used.                                           0.780032
This paper shows that rounding small gradients to zero results in matrices with up to 80% sparsity during training. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training    0.826890
dtype: float32","The findings of applying sparsity in the backward gradients for training LSTMs is interesting. The paper seems incomplete without the proper experimental justification.                                                                                                          1.098612
This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. It will be much more convincing if a well known dataset and experiment set up are used.                                           1.098612
This paper shows that rounding small gradients to zero results in matrices with up to 80% sparsity during training. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training    1.098612
dtype: float32","{""The findings of applying sparsity in the backward gradients for training LSTMs is interesting. The paper seems incomplete without the proper experimental justification."":{""The findings of applying sparsity in the backward gradients for training LSTMs is interesting. ----------------But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. ----------------Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger."":-8.1641044617,""This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.----------------Minor note:--------The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.----------------The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.----------------While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and"":-12.2619285583,""CONTRIBUTIONS--------When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.----------------NOVELTY--------Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.----------------MISSING CITATIONS--------Prior work has explored low-precision arithmetic for recurrent neural network language models:----------------Hubara et al, \u201cQuantized Neural Networks: Training Neural Networks with--------Low Precision Weights and Activ"":-12.4112176895},""This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. It will be much more convincing if a well known dataset and experiment set up are used."":{""The findings of applying sparsity in the backward gradients for training LSTMs is interesting. ----------------But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. ----------------Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger."":-5.0702376366,""This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.----------------Minor note:--------The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.----------------The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.----------------While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and"":-1.8008495569,""CONTRIBUTIONS--------When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.----------------NOVELTY--------Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.----------------MISSING CITATIONS--------Prior work has explored low-precision arithmetic for recurrent neural network language models:----------------Hubara et al, \u201cQuantized Neural Networks: Training Neural Networks with--------Low Precision Weights and Activ"":-4.9431595802},""This paper shows that rounding small gradients to zero results in matrices with up to 80% sparsity during training. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training"":{""The findings of applying sparsity in the backward gradients for training LSTMs is interesting. ----------------But the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. ----------------Also actual justification of the gains in terms of speed and efficiency would make the paper much stronger."":-4.1089963913,""This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.----------------Minor note:--------The LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.----------------The paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.----------------While the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and"":-4.2743797302,""CONTRIBUTIONS--------When training LSTMs, many of the intermediate gradients are close to zero due to the flat shape of the tanh and sigmoid nonlinearities far from the origin. This paper shows that rounding these small gradients to zero results in matrices with up to 80% sparsity during training, and that training character-level LSTM language models with this sparsification does not significantly change the final performance of the model. The authors argue that this sparsity could be exploited with specialized hardware to improve the energy efficiency and speed of recurrent network training.----------------NOVELTY--------Thresholding gradients to induce sparsity and improve efficiency in RNN training is a novel result to my knowledge.----------------MISSING CITATIONS--------Prior work has explored low-precision arithmetic for recurrent neural network language models:----------------Hubara et al, \u201cQuantized Neural Networks: Training Neural Networks with--------Low Precision Weights and Activ"":-0.7608683705}}","The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.    Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc).     Paper would be strengthened by a better exploration of the problem."
https://openreview.net/forum?id=HJeqWztlg,"['The paper discusses a method to learn interpretable template representations from given data. Authors illustrate their approach on binary images.'
 'This paper presents an approach to learn object representations by composing a set of templates from binary images. Learning is performed by using approximated inference with MAX-product BP.'
 ""This paper presents a generative model for binary images. Images are composed by placing a set of binary features at locations in the image. Images are OR'd together to produce an image.""]","['The paper discusses a method to learn interpretable template representations from given data. Authors illustrate their approach on binary images.'
 'This paper presents an approach to learn object representations by composing a set of templates from binary images. Learning is performed by using approximated inference with MAX-product BP.'
 ""This paper presents a generative model for binary images. Images are composed by placing a set of binary features at locations in the image. Images are OR'd together to produce an image.""]","The paper discusses a method to learn interpretable template representations from given data. Authors illustrate their approach on binary images.                                                 0.765987
This paper presents an approach to learn object representations by composing a set of templates from binary images. Learning is performed by using approximated inference with MAX-product BP.    0.944762
This paper presents a generative model for binary images. Images are composed by placing a set of binary features at locations in the image. Images are OR'd together to produce an image.        0.622185
dtype: float32","The paper discusses a method to learn interpretable template representations from given data. Authors illustrate their approach on binary images.                                                 1.098612
This paper presents an approach to learn object representations by composing a set of templates from binary images. Learning is performed by using approximated inference with MAX-product BP.    1.098612
This paper presents a generative model for binary images. Images are composed by placing a set of binary features at locations in the image. Images are OR'd together to produce an image.        1.098612
dtype: float32","{""The paper discusses a method to learn interpretable template representations from given data. Authors illustrate their approach on binary images."":{""The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images.----------------The paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems.----------------Summary:--------\u2014\u2014\u2014--------The paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me.----------------Quality: I think some of the techniques could be described more carefully to better convey the intuition.--------Clarity: Some of the derivations and intuitions could be explained in more detail.--------Originality: The suggested idea is reasonable"":-7.8171782494,""This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. --------In particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. ----------------Learning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. ----------------I have however, some concerns about the paper:----------------1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. --------I would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:----------------- Compositional hierarchies of Sanja Fidler--------- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects--------- AND-OR templates of Song-Ch"":-10.6236572266,""This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features\/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance\/location variables is defined.----------------Overall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.--------Why would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning."":-11.3911800385},""This paper presents an approach to learn object representations by composing a set of templates from binary images. Learning is performed by using approximated inference with MAX-product BP."":{""The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images.----------------The paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems.----------------Summary:--------\u2014\u2014\u2014--------The paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me.----------------Quality: I think some of the techniques could be described more carefully to better convey the intuition.--------Clarity: Some of the derivations and intuitions could be explained in more detail.--------Originality: The suggested idea is reasonable"":-6.3477845192,""This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. --------In particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. ----------------Learning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. ----------------I have however, some concerns about the paper:----------------1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. --------I would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:----------------- Compositional hierarchies of Sanja Fidler--------- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects--------- AND-OR templates of Song-Ch"":-2.257143259,""This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features\/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance\/location variables is defined.----------------Overall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.--------Why would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning."":-6.5398731232},""This paper presents a generative model for binary images. Images are composed by placing a set of binary features at locations in the image. Images are OR'd together to produce an image."":{""The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images.----------------The paper presents a novel technique for extracting interpretable hierarchical template representations based on a small set of standard operations. It is then shown how a combination of those standard operations translates into a task equivalent to a boolean matrix factorization. This insight is then used to formulate a message passing technique which was shown to produce accurate results for these types of problems.----------------Summary:--------\u2014\u2014\u2014--------The paper presents an novel formulation for extracting hierarchical template representations that has not been discussed in that form. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me.----------------Quality: I think some of the techniques could be described more carefully to better convey the intuition.--------Clarity: Some of the derivations and intuitions could be explained in more detail.--------Originality: The suggested idea is reasonable"":-3.6232793331,""This paper presents an approach to learn object representations by composing a set of templates which are leaned from binary images. --------In particular, a hierarchical model is learned by combining AND, OR and POOL operations. Learning is performed by using approximated inference with MAX-product BP follow by a heuristic to threshold activations to be binary. ----------------Learning hierarchical representations that are interpretable is a very interesting topic, and this paper brings some good intuitions in light of modern convolutional neural nets. ----------------I have however, some concerns about the paper:----------------1) the paper fails to cite and discuss relevant literature and claims to be the first one that is able to learn interpretable parts. --------I would like to see a discussion of the proposed approach compared to a variety of papers e.g.,:----------------- Compositional hierarchies of Sanja Fidler--------- AND-OR graphs used by Leo Zhu and Alan Yuille to model objects--------- AND-OR templates of Song-Ch"":-3.1882450581,""This paper presents a generative model for binary images.  Images are composed by placing a set of binary features at locations in the image.  These features are OR'd together to produce an image.  In a hierarchical variant, features\/classes can have a set of possible templates, one of which can be active.  Variables are defined to control which template is present in each layer.  A joint probability distribution over both the feature appearance and instance\/location variables is defined.----------------Overall, the goal of this work is interesting -- it would be satisfying if semantically meaningful features could be extracted, allowing compositionality in a generative model of images.  However, it isn't clear this would necessarily result from the proposed process.--------Why would the learned features (building blocks) necessarily semantically meaningful?  In the motivating example of text, rather than discovering letters, features could correspond to many other sub-units (parts of letters), or other features lacking direct semantic meaning."":-0.7982617021}}", The reviewer's opinions were clear for this paper. Mainly it seems that the fact that this work focuses on binary image patterns limited the ability of reviewers to assess the significance of this work based on the instantiation of the model explored in this work. It was also noted that the writing could have been clearer when describing the intuitions for the approach and that the derivations could have been explained in more detail.
https://openreview.net/forum?id=HJpfMIFll,"['The paper mostly ignores the long history of Word Sense Induction and Word Sense Disambiguation. The experiments in this paper done on SemEval-2010 are not very persuasive.'
 'The paper describes a new method to capture word polysemy with word embeddings. The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.'
 'This paper presents a study of the spaces around existing word embeddings.']","['The paper mostly ignores the long history of Word Sense Induction and Word Sense Disambiguation. The experiments in this paper done on SemEval-2010 are not very persuasive.'
 'The paper describes a new method to capture word polysemy with word embeddings. The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.'
 'This paper presents a study of the spaces around existing word embeddings.']","The paper mostly ignores the long history of Word Sense Induction and Word Sense Disambiguation. The experiments in this paper done on SemEval-2010 are not very persuasive.                                                               0.930319
The paper describes a new method to capture word polysemy with word embeddings. The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.    0.822010
This paper presents a study of the spaces around existing word embeddings.                                                                                                                                                                 0.691445
dtype: float32","The paper mostly ignores the long history of Word Sense Induction and Word Sense Disambiguation. The experiments in this paper done on SemEval-2010 are not very persuasive.                                                               1.098612
The paper describes a new method to capture word polysemy with word embeddings. The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.    1.098612
This paper presents a study of the spaces around existing word embeddings.                                                                                                                                                                 1.098612
dtype: float32","{""The paper mostly ignores the long history of Word Sense Induction and Word Sense Disambiguation. The experiments in this paper done on SemEval-2010 are not very persuasive."":{""On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold).----------------On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results).----------------For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (\""MFS\"") yields the highest F-measure of 63.5%. The paper's result with \""2 clusters\"" (with an average of about 1.9"":-3.7327711582,""This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.----------------The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose"":-7.6416344643,""This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks.----------------The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR."":-7.9808559418},""The paper describes a new method to capture word polysemy with word embeddings. The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy."":{""On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold).----------------On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results).----------------For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (\""MFS\"") yields the highest F-measure of 63.5%. The paper's result with \""2 clusters\"" (with an average of about 1.9"":-4.261991024,""This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.----------------The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose"":-0.616057694,""This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks.----------------The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR."":-3.8122634888},""This paper presents a study of the spaces around existing word embeddings."":{""On the plus side, the paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold).----------------On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD), citing and comparing only some relatively recent papers. The experiments in this paper done on SemEval-2010 are not very persuasive. (It's difficult to evaluate the experiments done on the 2016 data, since they are not directly comparable to published results).----------------For example, going back to the SemEval-2010 WSI task in [1], the best system seems to be UoY [2]. The F-measure seems to be a poor metric: always assigning one sense to every word (\""MFS\"") yields the highest F-measure of 63.5%. The paper's result with \""2 clusters\"" (with an average of about 1.9"":-27.3344402313,""This paper describe a new method to capture word polysemy with word embeddings. In order to disambiguate a word in a given sentence, the word is represented by the subspace spanned by the word vectors of the context in which it appears. This departs from a traditional approach were the context is represented as a (weighted) sum of the word vectors. A clustering algorithm (very similar to k-means), is then used to cluster the different usages of a given word, and discover the different senses (each sense corresponding to a cluster). The proposed method is evaluated on various word sense induction datasets. It is compared to other word embedding techniques which model word polysemy.----------------The method proposed in the paper to represent words in context is really interesting, simple to apply and seems very effective, based on the strong experimental results reported in the paper. My main concern about this paper is the writing, which is sometimes a bit verbose"":-27.4352874756,""This paper presents a study of the spaces around existing word embeddings. I proposes something unorthodox: instead of representing a word token by a vector, represent it by the subspace spanned by embeddings of the context word types around that token. These subspaces are fairly low-dimensional and are shown to capture some notions of polysemy (subspaces for tokens of the same sense should all roughly intersect in the same direction). While thinking about the subspace spanned by the context is fairly similar to thinking about a linear combination of the context embeddings, the subspace picture allows for a little more information to be preserved which can improve downstream semantic tasks.----------------The paper is a little dense reading at times, and some things are hard to understand, but the perspective is original enough and the results are good enough that I think it belongs in ICLR."":-24.5422763824}}",The paper considers an important problem largely ignored by continuous word representation learning: polysemy. The approach is mathematically grounded and interesting and well explored.
https://openreview.net/forum?id=Hk8TGSKlg,"['This work introduces a novel memory based artificial neural network for reading comprehension.'
 'Thie paper proposed an iterative memory updating model for cloze-style question-answering task.'
 '. This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting.']","['This work introduces a novel memory based artificial neural network for reading comprehension.'
 'Thie paper proposed an iterative memory updating model for cloze-style question-answering task.'
 '. This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting.']","This work introduces a novel memory based artificial neural network for reading comprehension.                            0.788167
Thie paper proposed an iterative memory updating model for cloze-style question-answering task.                           0.796966
. This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting.    0.618096
dtype: float32","This work introduces a novel memory based artificial neural network for reading comprehension.                            1.098612
Thie paper proposed an iterative memory updating model for cloze-style question-answering task.                           1.098612
. This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting.    1.098612
dtype: float32","{""This work introduces a novel memory based artificial neural network for reading comprehension."":{""First I would like to apologize for the delay in reviewing.----------------Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.--------The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.----------------In order to get a better sense of the reason for improvement it would be interesting to have a complexity and\/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments.----------------The dataset description in section 2.2, should be moved to section 4 where the other datasets are described."":-10.1250400543,""Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments:----------------1. Actually the model in the paper is not single model, it proposed two models. One consists of \""reading\"", \""writing\"", \""adaptive computation\"" and \"" Answer module 2\"", the other one is \""reading\"", \""composing\"", \""writing\"", \""gate querying\"" and \""Answer module 1\"". Based on the method section and the experiment, it seems the \""adaptive computation\"" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine.----------------2. What is the MLP setting in the composing module? ----------------3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could"":-13.3432426453,""This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read\/compose\/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models\/multi-hop reasoning in previous papers.----------------There are a couple places that this paper can improve. First, I would like to see the results from CNN\/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query g"":-13.3900794983},""Thie paper proposed an iterative memory updating model for cloze-style question-answering task."":{""First I would like to apologize for the delay in reviewing.----------------Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.--------The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.----------------In order to get a better sense of the reason for improvement it would be interesting to have a complexity and\/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments.----------------The dataset description in section 2.2, should be moved to section 4 where the other datasets are described."":-7.0812263489,""Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments:----------------1. Actually the model in the paper is not single model, it proposed two models. One consists of \""reading\"", \""writing\"", \""adaptive computation\"" and \"" Answer module 2\"", the other one is \""reading\"", \""composing\"", \""writing\"", \""gate querying\"" and \""Answer module 1\"". Based on the method section and the experiment, it seems the \""adaptive computation\"" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine.----------------2. What is the MLP setting in the composing module? ----------------3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could"":-2.4589891434,""This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read\/compose\/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models\/multi-hop reasoning in previous papers.----------------There are a couple places that this paper can improve. First, I would like to see the results from CNN\/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query g"":-5.053797245},"". This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting."":{""First I would like to apologize for the delay in reviewing.----------------Summary : this work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art.--------The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.----------------In order to get a better sense of the reason for improvement it would be interesting to have a complexity and\/or a time analysis of the algorithm. I might be mistaken but I don't see you reporting anything on the actual number of loops necessary in the reported experiments.----------------The dataset description in section 2.2, should be moved to section 4 where the other datasets are described."":-4.8943386078,""Thie paper proposed an iterative memory updating model for cloze-style question-answering task. The approach is interesting, and result is good. For the paper, I have some comments:----------------1. Actually the model in the paper is not single model, it proposed two models. One consists of \""reading\"", \""writing\"", \""adaptive computation\"" and \"" Answer module 2\"", the other one is \""reading\"", \""composing\"", \""writing\"", \""gate querying\"" and \""Answer module 1\"". Based on the method section and the experiment, it seems the \""adaptive computation\"" model is simpler and performs better. And without two time memory update in single iteration and composing module, the model is similar to neural turing machine.----------------2. What is the MLP setting in the composing module? ----------------3. This paper tested different size of hidden state:[256, 368, 436, 512], I do not find any relation between those numbers, how could"":-3.135393858,""This paper proposed an iterative query updating mechanism for cloze-style QA. The approach is novel and interesting and while it is only verified in the paper for two Cloze-style tasks (CBT and WDW), the concept of read\/compose\/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA. Another advantage of the proposed model is to learn when to terminate the iteration by the so-called adaptive computation model, such that it avoids the issue of treating the number of iterations as another hyper-parameter, which is a common practice of iterative models\/multi-hop reasoning in previous papers.----------------There are a couple places that this paper can improve. First, I would like to see the results from CNN\/Daily Mail as well to have a more comprehensive comparison. Secondly, it will be useful to visualize the entire M^q sequence over time t (not just z or the query g"":-1.2243818045}}","This paper proposes a memory-enhanced RNN in the vein of NTM, and a novel training method for this architecture of cloze-style QA. The results seem convincing, and the training method is decently novel according to reviewers, although the evaluation seemed somewhat incomplete according to reviewers and my own reading. For instance, it is questionable whether or not the advertised human performance on CNN/DM is accurate (based on 100 samples from a 300k+ dataset), so I'm not sure this warrants not evaluating or reporting performance on it. Overall this looks like an acceptable paper, although there is room for improvement."
https://openreview.net/forum?id=HkNKFiGex,"['The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush. A hybridization of GANs and VAEs called Introspective Adversarial Network.'
 'The paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real'
 'The paper presents a tool for exploring latent spaces of generative models. The presented tool is interesting and may be useful for analysis of generative models.']","['The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush. A hybridization of GANs and VAEs called Introspective Adversarial Network.'
 'The paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real'
 'The paper presents a tool for exploring latent spaces of generative models. The presented tool is interesting and may be useful for analysis of generative models.']","The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush. A hybridization of GANs and VAEs called Introspective Adversarial Network.                          0.867894
The paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real    0.853787
The paper presents a tool for exploring latent spaces of generative models. The presented tool is interesting and may be useful for analysis of generative models.                                                                                                0.664813
dtype: float32","The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush. A hybridization of GANs and VAEs called Introspective Adversarial Network.                          1.098612
The paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real    1.098612
The paper presents a tool for exploring latent spaces of generative models. The presented tool is interesting and may be useful for analysis of generative models.                                                                                                1.098612
dtype: float32","{""The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush. A hybridization of GANs and VAEs called Introspective Adversarial Network."":{""The paper presents two main contributions:----------------(1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software.----------------(2) A hybridization of GANs and VAEs called Introspective Adversarial Network.----------------The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together.----------------On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models.----------------On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to"":-1.7318893671,""UPDATE: The rewritten paper is more focused and precise than the previous version. The ablation studies and improved evaluation of the IAN model help to the make clear the relative contributions of the proposed MDC and orthogonal regularization. Though the paper is much improved, in my opinion there is still too much emphasis on the photo-editing interface. In addition, the MDC blocks are used in the generator of the model but their efficacy is measured via discriminative experiments. All-in-all I am updating my score to a 5.----------------==========----------------This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data, and the discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.----------------Pros:"":-5.3058023453,""After rebuttal:----------------I think the presentation improved in the revised version (although still quite cluttered and confusing), and new quantitative results look quite convincing. Therefore I raise my rating. Still, the paper could use polishing. If written in a better way, it would be a definite accept in my opinion.---------------------------------Initial review:----------------The paper presents a tool for exploring latent spaces of generative models, and \""introspective adversarial network\"" model - a new hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE). On the plus side, the presented tool is interesting and may be useful for analysis of generative models, and the proposed architecture seems to perform well. On the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by Zhu et al. [1] is not discussed in enough detail. Overall, I am in the borderline mode, and may change my opinion depending"":-5.4662709236},""The paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real"":{""The paper presents two main contributions:----------------(1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software.----------------(2) A hybridization of GANs and VAEs called Introspective Adversarial Network.----------------The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together.----------------On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models.----------------On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to"":-4.4130048752,""UPDATE: The rewritten paper is more focused and precise than the previous version. The ablation studies and improved evaluation of the IAN model help to the make clear the relative contributions of the proposed MDC and orthogonal regularization. Though the paper is much improved, in my opinion there is still too much emphasis on the photo-editing interface. In addition, the MDC blocks are used in the generator of the model but their efficacy is measured via discriminative experiments. All-in-all I am updating my score to a 5.----------------==========----------------This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data, and the discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.----------------Pros:"":-0.8439779878,""After rebuttal:----------------I think the presentation improved in the revised version (although still quite cluttered and confusing), and new quantitative results look quite convincing. Therefore I raise my rating. Still, the paper could use polishing. If written in a better way, it would be a definite accept in my opinion.---------------------------------Initial review:----------------The paper presents a tool for exploring latent spaces of generative models, and \""introspective adversarial network\"" model - a new hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE). On the plus side, the presented tool is interesting and may be useful for analysis of generative models, and the proposed architecture seems to perform well. On the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by Zhu et al. [1] is not discussed in enough detail. Overall, I am in the borderline mode, and may change my opinion depending"":-4.4179935455},""The paper presents a tool for exploring latent spaces of generative models. The presented tool is interesting and may be useful for analysis of generative models."":{""The paper presents two main contributions:----------------(1) A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software.----------------(2) A hybridization of GANs and VAEs called Introspective Adversarial Network.----------------The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together.----------------On one hand, the neural photo editing technique is presented in sufficient detail to be reproducible and it is shown to be effective. I personally find the idea exciting, but in order for it to be of interest to the ICLR community I think more emphasis should be put on what insights such a technique allows to gain on trained models.----------------On the other hand, the IAF model is introduced, along with multiple network architecture modifications for improving its performance. One criticism that I have regarding the presentation is that it makes it hard to"":-10.7620601654,""UPDATE: The rewritten paper is more focused and precise than the previous version. The ablation studies and improved evaluation of the IAN model help to the make clear the relative contributions of the proposed MDC and orthogonal regularization. Though the paper is much improved, in my opinion there is still too much emphasis on the photo-editing interface. In addition, the MDC blocks are used in the generator of the model but their efficacy is measured via discriminative experiments. All-in-all I am updating my score to a 5.----------------==========----------------This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data, and the discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.----------------Pros:"":-10.637389183,""After rebuttal:----------------I think the presentation improved in the revised version (although still quite cluttered and confusing), and new quantitative results look quite convincing. Therefore I raise my rating. Still, the paper could use polishing. If written in a better way, it would be a definite accept in my opinion.---------------------------------Initial review:----------------The paper presents a tool for exploring latent spaces of generative models, and \""introspective adversarial network\"" model - a new hybrid of a generative adversarial network (GAN) and a variational autoencoder (VAE). On the plus side, the presented tool is interesting and may be useful for analysis of generative models, and the proposed architecture seems to perform well. On the downside, experimental evaluation does not allow for confident conclusions, and a recent closely related work by Zhu et al. [1] is not discussed in enough detail. Overall, I am in the borderline mode, and may change my opinion depending"":-7.9548540115}}","Here is a summary of strengths and weaknesses as per the reviews:    Strengths  Work/application is exciting (R3)  Enough detail for reproducibility (R3)  May provide a useful analysis tool for generative models (R1)    Weaknesses  Clarity of the IAN model - presentation is scattered and could benefit from empirical analysis to tease out which parts are most important (R3,R2,R1); AC comments that the paper was revised in this regard and R3 was satisfied, updating their score  Lack of focus: is it the visualization/photo manipulation technique, or is it the generative model? (R3, R2)  Writing could use improvement (R2)  Mathematical formulation of IAN not precise (R2)    The authors provided a considerable overhaul of the paper, re-organizing/re-focusing it in response to the reviews and adding additional experiments.     This, in turn, resulted in R1, R2 and R3 upgrading their scores. The paper is more clearly in accept territory, in the ACÕs opinion. The AC recommends acceptance as a poster."
https://openreview.net/forum?id=HkljfjFee,"['The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding.'
 '. Sparse Coding is a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space.'
 '.']","['The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding.'
 '. Sparse Coding is a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space.'
 '.']","The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding.                                                                         0.696290
. Sparse Coding is a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space.    0.903905
.                                                                                                                                                                                                               0.762581
dtype: float32","The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding.                                                                         1.098612
. Sparse Coding is a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space.    1.098612
.                                                                                                                                                                                                               1.098612
dtype: float32","{""The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding."":{""The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation).----------------Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance.----------------Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast enc"":-5.7879767418,""In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail. ----------------In the abstract the authors mention \u201cextensive experimental results \u2026\u201d, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in \u201cCUDA"":-8.7820138931,""I'd like to thank the authors for their detailed response to my questions.----------------The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.----------------The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. ----------------Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.----------------The authors also propose a fast encoding scheme for their proposed method. --------The authors included a new"":-8.5242986679},"". Sparse Coding is a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space."":{""The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation).----------------Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance.----------------Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast enc"":-5.29271698,""In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail. ----------------In the abstract the authors mention \u201cextensive experimental results \u2026\u201d, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in \u201cCUDA"":-1.302875638,""I'd like to thank the authors for their detailed response to my questions.----------------The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.----------------The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. ----------------Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.----------------The authors also propose a fast encoding scheme for their proposed method. --------The authors included a new"":-5.0769257545},""."":{""The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. Namely, two data samples that are similar or neighbours, should have a sparse code that is similar (in terms of support). The general idea is not unique, but it is an interesting one (if one admits that the adjacency matrix A is known a priori), and the novelty mostly lies on the definition of the regularisation term that is an l1-norm (while other techniques would mostly use l2 regularisation).----------------Based on this idea, the authors develop a new SRSC algorithm, which is analysed in detail and shown to perform better than its competitors based on l2 sparse coding regularisation and other schemes in terms of clustering performance.----------------Inspired by LISTA, the authors then propose an approximate solution to the SRSC problem, called Deep-SRSC, that acts as a sort of fast enc"":-179.9545440674,""In this paper the authors propose a method to explicitly regularize sparse coding to encode neighbouring datapoints with similar sets of atoms from the dictionary by clustering training examples with KNN in input space. The resulting algorithm is relatively complex and computationally relatively expensive, but the authors provide detailed derivations and use arguments from proximal gradient descent methods to prove convergence (I did not follow all the derivations, only some). In general the paper is well written and the authors explain the motivation behind the algorithms design in detail. ----------------In the abstract the authors mention \u201cextensive experimental results \u2026\u201d, but I find the experiments not very convincing: With experiments on the USPS handwritten digits dataset (why not MNIST?), COIL-20 and COIL-100 and UCI, the datasets are all relatively small and the algorithm is run with dictionary sizes between p=100 to p=500. This seems surprising because the authors state that they implemented SRSC in \u201cCUDA"":-180.4247894287,""I'd like to thank the authors for their detailed response to my questions.----------------The paper proposes a support regularized version of sparse coding that takes into account the underlying manifold structure of the data. For this purpose, the authors augment the classic sparse coding loss with a term that encourages near by points to have similar active set. Convergence guarantees for the optimization procedure are presented. Experimental evaluation on clustering and semi-supervised learning shows the benefits of the proposed approach.----------------The paper is well written and a nice read. The most relevant contribution of this work is to including (and optimizing) the regularization function, and not an approximation or surrogate. The authors derive a a PGD-styple iterative method and present convergence analysis for it. ----------------Thanks for the clarifications regarding the assumptions used in Section 3. It would be nice to include some of that in the manuscript.----------------The authors also propose a fast encoding scheme for their proposed method. --------The authors included a new"":-177.0440216064}}","Adding a manifold regularizer to a learning objective function is certainly not a new direction. The paper argues that using a support based regularizer is superior to using a standard graph Laplacian regularizer (which has been explored before), although this argument is not developed particularly rigorously and dominantly has to fall back on empirical evidence. The main contribution of the paper appears to be theoretical justification of an alternating optimization scheme for minimizing the resulting objective function (yet the optimization aspects of dealing with a sparse support regularizer are somewhat orthogonal to the current context). The empirical results are not very convincing since the dictionary size is relatively large compared to the dataset size; the gains with respect to l2 manifold regularizer are not consistent; and the gains using deep architectures to directly predict sparse codes are also modest and somewhat inconsistent. These points aside, the reviewers are overall enthusiastic about the paper and find it to be well written and complete."
https://openreview.net/forum?id=HkwoSDPgg,"['. A set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers.'
 ' data.'
 'The paper advances the state of the art on differentially-private deep learning. It advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.']","['. A set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers.'
 ' data.'
 'The paper advances the state of the art on differentially-private deep learning. It advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.']",". A set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers.                                                            0.766603
 data.                                                                                                                                                                                                        0.043558
The paper advances the state of the art on differentially-private deep learning. It advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.    0.437134
dtype: float32",". A set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers.                                                            1.098605
 data.                                                                                                                                                                                                        1.075191
The paper advances the state of the art on differentially-private deep learning. It advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.    1.098556
dtype: float32","{"". A set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers."":{""This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing."":-5.6739563942,""This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. ----------------The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.----------------The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. "":-8.4582748413,""Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.----------------One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and\/or clarify what can be generalized about the observed learning performance.----------------Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in"":-9.2928266525},"" data."":{""This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing."":-128.2184906006,""This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. ----------------The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.----------------The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. "":-127.9262542725,""Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.----------------One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and\/or clarify what can be generalized about the observed learning performance.----------------Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in"":-127.502746582},""The paper advances the state of the art on differentially-private deep learning. It advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough."":{""This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. I found the approach altogether plausible and very clearly explained by the authors. Adding more discussion of the bound (and its tightness) from Theorem 1 itself would be appreciated. A simple idea of adding perturbation error to the counts, known from differentially-private literature, is nicely re-used by the authors and elegantly applied in a much broader (non-convex setting) and practical context than in a number of differentially-private and other related papers. The generality of the approach, clear improvement over predecessors, and clarity of the writing makes the method worth publishing."":-2.6822524071,""This paper discusses how to guarantee privacy for training data. In the proposed approach multiple models trained with disjoint datasets are used as ``teachers'' model, which will train a ``student'' model to predict an output chosen by noisy voting among all of the teachers. ----------------The theoretical results are nice but also intuitive. Since teachers' result are provided via noisy voting, the student model may not duplicate the teacher's behavior. However, the probabilistic bound has quite a number of  empirical parameters, which makes me difficult to decide whether the security is 100% guaranteed or not.----------------The experiments on MNIST and SVHN are good. However, as the paper claims, the proposed approach may be mostly useful for sensitive data like medical histories, it will be nice to conduct one or two experiments on such applications. "":-3.4652011395,""Altogether a very good paper, a nice read, and interesting. The work advances the state of the art on differentially-private deep learning, is quite well-written, and relatively thorough.----------------One caveat is that although the approach is intended to be general, no theoretical guarantees are provided about the learning performance. Privacy-preserving machine learning papers often analyze both the privacy (in the worst case, DP setting) and the learning performance (often under different assumptions). Since the learning performance might depend on the choice of architecture; future experimentation is encouraged, even using the same data sets, with different architectures. If this will not be added, then please justify the choice of architecture used, and\/or clarify what can be generalized about the observed learning performance.----------------Another caveat is that the reported epsilons are not those that can be privately released; the authors note that their technique for doing so would change the resulting epsilon. However this would need to be resolved in"":-1.033149004}}","The paper presents a general teacher-student approach for differentially-private learning in which the student learns to predict a noise vote among a set of teachers. The noise allows the student to be differentially private, whilst maintaining good classification accuracies on MNIST and SVHN. The paper is well-written."
https://openreview.net/forum?id=HkzuKpLgg,"['The paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU.'
 'This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. It concludes that scaling is invariant of number of GPUs.']","['The paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU.'
 'This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. It concludes that scaling is invariant of number of GPUs.']","The paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU.                                                               0.438842
This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. It concludes that scaling is invariant of number of GPUs.    0.531034
dtype: float32","The paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU.                                                               0.693147
This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. It concludes that scaling is invariant of number of GPUs.    0.693147
dtype: float32","{""The paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU."":{""This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. ----------------Comments:----------------- The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation.  ----------------- The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. ----------------- The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by"":-10.7690410614,""This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.--------Comments--------1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature. The author should use the standard name to make the connection easier. --------2) The cost analysis of ring-based Allreduce is already provided in the existing literature. This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.--------3) The ring-based allreduce approach is already supported by NVidia\u2019s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.--------4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop"":-13.3517789841},""This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net. It concludes that scaling is invariant of number of GPUs."":{""This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. The paper provides both theoretical analysis and experiments. Overall, the results presented in the paper are interesting, but the writing can be improved. ----------------Comments:----------------- The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation.  ----------------- The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. ----------------- The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by"":-4.1949586868,""This paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.--------Comments--------1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature. The author should use the standard name to make the connection easier. --------2) The cost analysis of ring-based Allreduce is already provided in the existing literature. This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.--------3) The ring-based allreduce approach is already supported by NVidia\u2019s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.--------4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop"":-0.9684172869}}","The authors propose improvements for the utilization of modern hardware when training using stochastic gradient. However, the reviewers bring up several issues with the paper, including major clarity issues as well as notational issues and some comments about the theory vs. practice."
https://openreview.net/forum?id=Hy3_KuYxg,"[""I'm still missing the details necessary to reproduce the experiments. The paper needs to be improved, perhaps with a toy example.""
 'The paper is a proof-of-concept of a partially-trainable implementation of the important “divide and conquer” paradigm. The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks'
 'The paper is intended to train models for combinatorial search procedures. The general research direction is fruitful and important.']","[""I'm still missing the details necessary to reproduce the experiments. The paper needs to be improved, perhaps with a toy example.""
 'The paper is a proof-of-concept of a partially-trainable implementation of the important “divide and conquer” paradigm. The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks'
 'The paper is intended to train models for combinatorial search procedures. The general research direction is fruitful and important.']","I'm still missing the details necessary to reproduce the experiments. The paper needs to be improved, perhaps with a toy example.                                                                                               0.754514
The paper is a proof-of-concept of a partially-trainable implementation of the important “divide and conquer” paradigm. The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks    0.048638
The paper is intended to train models for combinatorial search procedures. The general research direction is fruitful and important.                                                                                            0.852416
dtype: float32","I'm still missing the details necessary to reproduce the experiments. The paper needs to be improved, perhaps with a toy example.                                                                                               1.098612
The paper is a proof-of-concept of a partially-trainable implementation of the important “divide and conquer” paradigm. The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks    1.098410
The paper is intended to train models for combinatorial search procedures. The general research direction is fruitful and important.                                                                                            1.098612
dtype: float32","{""I'm still missing the details necessary to reproduce the experiments. The paper needs to be improved, perhaps with a toy example."":{""I was holding off on this review hoping to get the missing details from the code at https:\/\/github.com\/alexnowakvilla\/DP, but at this time it's still missing. After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning.----------------As an example of the difficulty, take section 4.3. It talks about training \""split block\"" which is a function that can assign each element to either partition 0 or partition 1. At this point I'm looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as \""we must create artificial targets at every node of the generated tree from the available final target partition\"". What are these artificial"":-11.6404304504,""The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.----------------My main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of"":-14.6760368347,""I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.----------------The paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.----------------Instead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random"":-14.7933740616},""The paper is a proof-of-concept of a partially-trainable implementation of the important \u201cdivide and conquer\u201d paradigm. The solution isn\u2019t generic enough to be applicable to unknown problems - the networks require tricks"":{""I was holding off on this review hoping to get the missing details from the code at https:\/\/github.com\/alexnowakvilla\/DP, but at this time it's still missing. After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning.----------------As an example of the difficulty, take section 4.3. It talks about training \""split block\"" which is a function that can assign each element to either partition 0 or partition 1. At this point I'm looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as \""we must create artificial targets at every node of the generated tree from the available final target partition\"". What are these artificial"":-4.0406470299,""The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.----------------My main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of"":-3.3879976273,""I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.----------------The paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.----------------Instead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random"":-3.9966299534},""The paper is intended to train models for combinatorial search procedures. The general research direction is fruitful and important."":{""I was holding off on this review hoping to get the missing details from the code at https:\/\/github.com\/alexnowakvilla\/DP, but at this time it's still missing. After going over this paper couple of times I'm still missing the details necessary to reproduce the experiments. I think this would be a common problem for readers of this paper, so the paper needs to be improved, perhaps with a toy example going through all the stages of learning.----------------As an example of the difficulty, take section 4.3. It talks about training \""split block\"" which is a function that can assign each element to either partition 0 or partition 1. At this point I'm looking at it as a binary classification problem and looking for the parameters, loss, and how this loss is minimized. Instead I get a lot of unexpected information, such as \""we must create artificial targets at every node of the generated tree from the available final target partition\"". What are these artificial"":-16.9213619232,""The basic idea of this contribution is very nice and worth pursuing: how to use the powerful \u201cdivide and conquer\u201d algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. However, the execution of this idea is not convincing and needs polishing before acceptance. As it is right now, the paper has a proof-of-concept feel that makes it great for a workshop contribution.----------------My main concern is that the method presented is currently not easily applicable to other tasks. Typically, demonstrations of program induction from input-output examples on well known tasks serves the purpose of proving, that a generic learning machine is able to solve some well known tasks, and will be useful on other tasks due to its generality. This contribution, however, presents a learning machine that is very hand-tailored to the two chosen tasks. The paper essentially demonstrates that with enough engineering (hardcoding the recurrency structure, designing problem-specific rules of"":-16.8387489319,""I find this paper extremely hard to read. The main promise of the paper is to train models for combinatorial search procedures, especially for dynamic programming to learn where to split and merge. The present methodology is supposed to make use of some form of scale invariance property which is scarcely motivated for most problems this approach should be relevant for. However, the general research direction is fruitful and important.----------------The paper would be much more readable if it would start with a clear, formal problem formulation, followed by some schematic view on the overall flow and description on which parts are supervised, which parts are not. Also a tabular form and sample of the various kinds problems solved by this method could be listed in the beginning as a motivation with some clear description on how they fit the central paradigm and motivate the rest of the paper in a more concrete manner.----------------Instead, the paper is quite chaotic, switching between low-level and high level details, problem formulations and their solutions in a somewhat random"":-13.3155345917}}","The area chair agrees with the reviewers that this paper is not ready for ICLR yet. There are significant issues with the writing, making it difficult to follow the technical details. Writing aside, the technique seems somewhat limited in its applicability. The authors also promised an updated version, but this version was never delivered (latest version is from Nov 13)."
https://openreview.net/forum?id=HyAbMKwxe,"['The paper analyses the misclassification error of discriminators. The problem studied is interesting and the proposed approach is sound.'
 'The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function.'
 'The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas']","['The paper analyses the misclassification error of discriminators. The problem studied is interesting and the proposed approach is sound.'
 'The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function.'
 'The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas']","The paper analyses the misclassification error of discriminators. The problem studied is interesting and the proposed approach is sound.                                                                                                                       0.258307
The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function.                                                                                                                          0.822315
The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas    0.852607
dtype: float32","The paper analyses the misclassification error of discriminators. The problem studied is interesting and the proposed approach is sound.                                                                                                                       1.098591
The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function.                                                                                                                          1.098612
The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas    1.098612
dtype: float32","{""The paper analyses the misclassification error of discriminators. The problem studied is interesting and the proposed approach is sound."":{""The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. --------Consequently, the optimized upper bound (log-loss) gets looser. ----------------As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. ----------------I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. ----------------There is a large body of work on directly optimizing task losses[2][3"":-13.9087934494,""The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers\/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.----------------The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an \""uncertain\"" label. ----------------While the paper is easy to read and well-written overall, in a second"":-15.0671453476,""The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.----------------An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend"":-15.6973991394},""The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function."":{""The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. --------Consequently, the optimized upper bound (log-loss) gets looser. ----------------As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. ----------------I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. ----------------There is a large body of work on directly optimizing task losses[2][3"":-14.3218212128,""The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers\/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.----------------The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an \""uncertain\"" label. ----------------While the paper is easy to read and well-written overall, in a second"":-11.1397418976,""The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.----------------An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend"":-14.8088302612},""The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas"":{""The paper analyses the misclassification error of discriminators and highlights the fact that while uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. --------Consequently, the optimized upper bound (log-loss) gets looser. ----------------As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written. While the main observation made in this paper is a well-known fact, it is presented in a clear and refreshing way that may make it useful to a wide audience at this venue. ----------------I would like to draw the author's attention to the close connections of this framework with curriculum learning. More on this can be found in [1] (which is a relevant reference that should be cited). A discussion on this could enrich the quality of the paper. ----------------There is a large body of work on directly optimizing task losses[2][3"":-4.4627408981,""The paper proposes new bounds on the misclassification error. The bounds lead to training classifiers with an adaptive loss function, and the algorithm operates in successive steps: the parameters are trained by minimizing the log-loss weighted by the probability of the observed class as given by the parameters of the previous steps. The bound improves on standard log-likelihood when outliers\/underfitting prevents the learning algorithm to properly optimize the true classification error. Experiments are performed to confirm the therotical intuition and motivation. They show different cases where the new algorithm leads to improved classification error because underfitting occurs when using standard log-loss, and other cases where the new bounds do not lead to any improvement because the log-loss is sufficient to fit the dataset.----------------The paper also discusses the relationship between the proposed idea and reinforcement learning, as well as with classifiers that have an \""uncertain\"" label. ----------------While the paper is easy to read and well-written overall, in a second"":-4.1994328499,""The paper proposes an alternative to conditional max. log likelihood for training discriminative classifiers. The argument is that the conditional log. likelihood is an upper bound of the Bayes error which becomes lousy during training. The paper then proposes better bounds computed and optimized in an iterative algorithm. Extensions of this idea are developed for regularized losses and a weak form of policy learning. Tests are performed on different datasets.----------------An interesting aspect of the contribution is to revisit a well-accepted methodology for training classifiers. The idea looks fine and some of the results seem to validate it. This is however still a preliminary work and one would like to see the ideas pushed further. Globally, the paper lacks coherence and depth: the part on policy learning is not well connected to the rest of the paper and the link with RL is not motivated in the two examples (ROC optimization and uncertainties). The experimental part needs a rewriting, e.g. I did not find a legend"":-0.760017097}}","Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail."
https://openreview.net/forum?id=HyAddcLge,"['Asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. The increasing shape of the curve is somehow implying some weird implementation of communication.'
 'A paper proposed a synchronous parallel SGD by employing several backup machines.'
 '. The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than. Async-SGD.']","['Asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. The increasing shape of the curve is somehow implying some weird implementation of communication.'
 'A paper proposed a synchronous parallel SGD by employing several backup machines.'
 '. The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than. Async-SGD.']","Asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. The increasing shape of the curve is somehow implying some weird implementation of communication.    0.017458
A paper proposed a synchronous parallel SGD by employing several backup machines.                                                                                                                        0.833015
. The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than. Async-SGD.                                                                                     0.751394
dtype: float32","Asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. The increasing shape of the curve is somehow implying some weird implementation of communication.    1.098052
A paper proposed a synchronous parallel SGD by employing several backup machines.                                                                                                                        1.098612
. The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than. Async-SGD.                                                                                     1.098612
dtype: float32","{""Asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. The increasing shape of the curve is somehow implying some weird implementation of communication."":{""This paper was easy to read, the main idea was presented very clearly.----------------The main points of the paper (and my concerns are below) can be summarized as follows:--------1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some"":-4.9487848282,""This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. ----------------My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:----------------- provide more experiments to show the performance with different efficiency distributions of learners.--------- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm."":-5.2508978844,""The paper claim that, when supported by a number of backup workers, synchronized-SGD --------actually works better than async-SGD. The paper first analyze the problem of staled updates--------in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the --------authors shows the effectiveness of the proposed method in applications to Inception Net--------and PixelCNN.----------------The idea is very simple, but in practice it can be quite useful in industry settings where --------adding some backup workders is not a big problem in cost. Nevertheless, I think the --------proposed solution is quite straightforward to come up with when we assume that --------each worker contains the full dataset and we have budge to add more workers. So, --------under this setting, it seems quite natural to have a better performance with the additional --------backup workers that avoid the staggering worker problem. And, with this assumtion I'm not --------sure if the proposed solution is"":-5.3877983093},""A paper proposed a synchronous parallel SGD by employing several backup machines."":{""This paper was easy to read, the main idea was presented very clearly.----------------The main points of the paper (and my concerns are below) can be summarized as follows:--------1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some"":-18.5220184326,""This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. ----------------My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:----------------- provide more experiments to show the performance with different efficiency distributions of learners.--------- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm."":-14.7058820724,""The paper claim that, when supported by a number of backup workers, synchronized-SGD --------actually works better than async-SGD. The paper first analyze the problem of staled updates--------in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the --------authors shows the effectiveness of the proposed method in applications to Inception Net--------and PixelCNN.----------------The idea is very simple, but in practice it can be quite useful in industry settings where --------adding some backup workders is not a big problem in cost. Nevertheless, I think the --------proposed solution is quite straightforward to come up with when we assume that --------each worker contains the full dataset and we have budge to add more workers. So, --------under this setting, it seems quite natural to have a better performance with the additional --------backup workers that avoid the staggering worker problem. And, with this assumtion I'm not --------sure if the proposed solution is"":-17.8805274963},"". The paper claim that, when supported by a number of backup workers, synchronized-SGD works better than. Async-SGD."":{""This paper was easy to read, the main idea was presented very clearly.----------------The main points of the paper (and my concerns are below) can be summarized as follows:--------1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some"":-6.4907689095,""This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. ----------------My main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:----------------- provide more experiments to show the performance with different efficiency distributions of learners.--------- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm."":-5.6249055862,""The paper claim that, when supported by a number of backup workers, synchronized-SGD --------actually works better than async-SGD. The paper first analyze the problem of staled updates--------in async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the --------authors shows the effectiveness of the proposed method in applications to Inception Net--------and PixelCNN.----------------The idea is very simple, but in practice it can be quite useful in industry settings where --------adding some backup workders is not a big problem in cost. Nevertheless, I think the --------proposed solution is quite straightforward to come up with when we assume that --------each worker contains the full dataset and we have budge to add more workers. So, --------under this setting, it seems quite natural to have a better performance with the additional --------backup workers that avoid the staggering worker problem. And, with this assumtion I'm not --------sure if the proposed solution is"":-2.9179136753}}","Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that ""backup workers"" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware."
https://openreview.net/forum?id=HyET6tYex,"['The authors explore whether the halting time distributions for various algorithms in various settings exhibit universality.'
 'Universality is a measure of stability in an algorithm. An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations depend on the algorithm but do not depend on the target accuracy epsilon.'
 'The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short.']","['The authors explore whether the halting time distributions for various algorithms in various settings exhibit universality.'
 'Universality is a measure of stability in an algorithm. An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations depend on the algorithm but do not depend on the target accuracy epsilon.'
 'The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short.']","The authors explore whether the halting time distributions for various algorithms in various settings exhibit universality.                                                                                                                             0.780408
Universality is a measure of stability in an algorithm. An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations depend on the algorithm but do not depend on the target accuracy epsilon.    0.653514
The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short.                                              0.765998
dtype: float32","The authors explore whether the halting time distributions for various algorithms in various settings exhibit universality.                                                                                                                             1.098612
Universality is a measure of stability in an algorithm. An algorithm is considered to satisfy the universality property when the centered/scaled halting time fluctuations depend on the algorithm but do not depend on the target accuracy epsilon.    1.098612
The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short.                                              1.098612
dtype: float32","{""The authors explore whether the halting time distributions for various algorithms in various settings exhibit universality."":{""The authors explore whether the halting time distributions for various algorithms in various settings exhibit \""universality\"", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.----------------The idea of the described universality is very interesting. However I see several shortcomings in the paper:----------------In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.----------------Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a"":-19.1583747864,""Summary------------------------For several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).----------------An algorithm is considered to satisfy the universality property when the centered\/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution\/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)----------------The authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.----------------A moment-"":-21.9863014221,""The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:----------------1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. ----------------2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).----------------3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for"":-22.8712768555},""Universality is a measure of stability in an algorithm. An algorithm is considered to satisfy the universality property when the centered\/scaled halting time fluctuations depend on the algorithm but do not depend on the target accuracy epsilon."":{""The authors explore whether the halting time distributions for various algorithms in various settings exhibit \""universality\"", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.----------------The idea of the described universality is very interesting. However I see several shortcomings in the paper:----------------In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.----------------Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a"":-3.7648200989,""Summary------------------------For several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).----------------An algorithm is considered to satisfy the universality property when the centered\/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution\/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)----------------The authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.----------------A moment-"":-1.3285592794,""The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:----------------1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. ----------------2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).----------------3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for"":-4.3481197357},""The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short."":{""The authors explore whether the halting time distributions for various algorithms in various settings exhibit \""universality\"", i.e. after rescaling to zero mean and unit variance, the distribution does not depend on stopping parameter, dimensionality and ensemble.----------------The idea of the described universality is very interesting. However I see several shortcomings in the paper:----------------In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The discussion of exponential tailed halting time distributions is a good start, but I am not sure how often this might be actually helpful. Still, the findings in the paper might be interesting from a theoretical point of view.----------------Especially for ICLR, I think it would have been more interesting to look into comparisons between stochastic gradient descent, momentum, ADAM etc on different deep learning architectures. Over which of those parameters does universality hold?. How can different initializations influence the halting time distribution? I would expect a"":-6.5525064468,""Summary------------------------For several algorithms, previous research has shown that the halting time follows a two-parameter distribution (the so-called universal property investigated by the authors). In this work, the authors extend the investigation to new algorithms (spin-glass, gradient descent in deep learning).----------------An algorithm is considered to satisfy the universality property when the centered\/scaled halting time fluctuations (empirical distribution of halting times) depend on the algorithm but do not depend on the target accuracy epsilon, an intrinsic measure of dimension N, the probability distribution\/random ensemble. (This is clear from Eq 1 where on the left the empirical halting time distribution depends on epsilon, N, A, E and on the right, the approximation only depends on the algorithm)----------------The authors argue that empirically, the universal property is observed when both algorithms (spin glass and deep learning) perform well and that it is not observed when they do not perform well.----------------A moment-"":-6.9415793419,""The idea of universality that is independent of input distribution and dimension, depending only on the algorithm is an appealing one. However, as an empirical study, this paper comes up somewhat short:----------------1. Exactly one algorithm is shown for the deep learning example. It would have been more convincing to compare distributions with one or more algorithms. ----------------2. The definition (1),  and much of the work of Section 2.1 seems to have already been covered in Deift (2014), Section 1.3. In that paper, a number of different algorithms for the solution of linear systems are considered, and then the concept of universality becomes more plausible. I do not see enough of such algorithmic comparisons in this paper (same problem setup, different algorithms).----------------3.  It seems to me that what practitioners might care about in practice are both the mean and variance in running times; these quantities are buried in (1). So I question how useful the distribution itself might be for"":-3.5922164917}}",My overall conclusion is that the paper needs more work to be sufficiently convincing. I think the reviews are sufficiently careful. My recommendation is based on the fact that none of the reviewers supports acceptance.
https://openreview.net/forum?id=HyEeMu_xx,"['Progressive attention is essentially a gating on every spatial feature. No significant performance gain on any standard datasets. No real novelty.'
 'This paper presents a hierarchical attention model. It uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset.'
 'The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. The approach is novel and interesting to my knowledge and speaks for acceptance.']","['Progressive attention is essentially a gating on every spatial feature. No significant performance gain on any standard datasets. No real novelty.'
 'This paper presents a hierarchical attention model. It uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset.'
 'The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. The approach is novel and interesting to my knowledge and speaks for acceptance.']","Progressive attention is essentially a gating on every spatial feature. No significant performance gain on any standard datasets. No real novelty.                                                  0.551564
This paper presents a hierarchical attention model. It uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset.                             0.769631
The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. The approach is novel and interesting to my knowledge and speaks for acceptance.    0.755072
dtype: float32","Progressive attention is essentially a gating on every spatial feature. No significant performance gain on any standard datasets. No real novelty.                                                  1.098612
This paper presents a hierarchical attention model. It uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset.                             1.098612
The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. The approach is novel and interesting to my knowledge and speaks for acceptance.    1.098612
dtype: float32","{""Progressive attention is essentially a gating on every spatial feature. No significant performance gain on any standard datasets. No real novelty."":{""This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results.----------------Pros:--------- The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections]--------- Good visualisations.----------------Cons:--------- No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps.--------- Not clear how the query is fed into the localisation networks of baselines.--------- The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different"":-6.9286837578,""This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.----------------Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.  ----------------An important missing citation is Graves 2013 which had an early version of the attention model. ----------------Minor typo:--------\""It confins possible attributes..\"" -> It confines..--------\""ImageNet (Deng et al., 2009), is used, and three additional\"" -> \"".., are used,\"""":-9.1991405487,""The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention.----------------1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.--------2. Qualitative results: --------2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the \u201cnumber\u201d dataset and more on the Visual Genome dataset.--------2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.--------3. Qualitative results"":-9.3877353668},""This paper presents a hierarchical attention model. It uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset."":{""This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results.----------------Pros:--------- The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections]--------- Good visualisations.----------------Cons:--------- No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps.--------- Not clear how the query is fed into the localisation networks of baselines.--------- The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different"":-6.2506608963,""This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.----------------Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.  ----------------An important missing citation is Graves 2013 which had an early version of the attention model. ----------------Minor typo:--------\""It confins possible attributes..\"" -> It confines..--------\""ImageNet (Deng et al., 2009), is used, and three additional\"" -> \"".., are used,\"""":-3.0507674217,""The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention.----------------1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.--------2. Qualitative results: --------2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the \u201cnumber\u201d dataset and more on the Visual Genome dataset.--------2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.--------3. Qualitative results"":-6.1686477661},""The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. The approach is novel and interesting to my knowledge and speaks for acceptance."":{""This paper proposes an attention mechanism which is essentially a gating on every spatial feature. Though they claim novelty through the attention being progressive, progressive attention has been done before [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections], and the element-wise multiplicative gates are very similar to convolutional LSTMs and Highway Nets. There is a lack of novelty and no significant results.----------------Pros:--------- The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections]--------- Good visualisations.----------------Cons:--------- No progressive baselines were evaluated, e.g. STN and HAN at every layer acting on featuremaps.--------- Not clear how the query is fed into the localisation networks of baselines.--------- The difference in performance between author-made synthetic data and the Visual Genome datasets between baselines and PAN is very different"":-5.0170321465,""This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset.----------------Overall I think this is a well executed paper, with good experimental results and nice qualitative visualizations. The main thing I believe it is missing would be experiments on a dataset like VQA which would help better place the significance of this work in context of other approaches.  ----------------An important missing citation is Graves 2013 which had an early version of the attention model. ----------------Minor typo:--------\""It confins possible attributes..\"" -> It confines..--------\""ImageNet (Deng et al., 2009), is used, and three additional\"" -> \"".., are used,\"""":-5.2957863808,""The paper presents an architecture to incrementally attend to image regions - at multiple layers of a deep CNN. In contrast to most other models, the model does not apply a weighted average pooling in the earlier layers of the network but only in the last layer. Instead, the features are reweighted in each layer with the predicted attention.----------------1. Contribution of approach: The approach to use attention in this way is to my knowledge novel and interesting.--------2. Qualitative results: --------2.1. I like the large number of qualitative results; however, I would have wished the focus would have been less on the \u201cnumber\u201d dataset and more on the Visual Genome dataset.--------2.2. The qualitative results for the Genome dataset unfortunately does not provide the predicted attributes. It would be interesting to see e.g. the highest predicted attributes for a given query. So far the results only show the intermediate results.--------3. Qualitative results"":-2.054595232}}","The program committee appreciates the authors' response to concerns raised in the reviews. Authors have conducted additional experiments and provided comparisons to other existing models. However, reviewer scores are not leaning sufficiently towards acceptance.    The effectiveness of this approach on realistic data still remains unclear in the context of existing approaches. I agree that the reported improvement on Visual Genome over the baseline is non-trivial. But evaluating an existing state-of-the-art VQA approach (for instance) would help better place the performance of this approach in perspective relative to state-of-the-art.     Incorporating reviewer comments, and more convincing demonstration of the model's capabilities on realistic data will help make the paper stronger."
https://openreview.net/forum?id=HyQWFOVge,"['I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al'
 'I don\'t understand why Distance Metric Learning is being used for classification tasks. Even when these datasets are used in a ""retrieval"" setting, the ground truth is still defined by category membership. Direct classification is better.'
 'A recent paper shows that a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches.']","['I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al'
 'I don\'t understand why Distance Metric Learning is being used for classification tasks. Even when these datasets are used in a ""retrieval"" setting, the ground truth is still defined by category membership. Direct classification is better.'
 'A recent paper shows that a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches.']","I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al    0.932210
I don't understand why Distance Metric Learning is being used for classification tasks. Even when these datasets are used in a ""retrieval"" setting, the ground truth is still defined by category membership. Direct classification is better.                             0.468085
A recent paper shows that a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches.                                                                                                                           0.760116
dtype: float32","I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al    1.098612
I don't understand why Distance Metric Learning is being used for classification tasks. Even when these datasets are used in a ""retrieval"" setting, the ground truth is still defined by category membership. Direct classification is better.                             1.098612
A recent paper shows that a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches.                                                                                                                           1.098612
dtype: float32","{""I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al"":{""I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. ----------------Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all"":-0.6998983622,""I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding). I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc). As far as I can tell, there is really only a single \""retrieval\""-like benchmark being used here - the Stanford Online Products database. All the other datasets are used in a \""classification-by-retrieval\"" approach which seems contrived. While ostensibly evaluating \""retrieval\"", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category. With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task.----------------It seems obvious that if your"":-5.0249757767,""There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance \/ interest to ICLR. This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches? Thorough experiments seem to indicate that this is indeed the case. Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets \/ tasks for which the DML approaches were designed. ----------------This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature. The authors argue that previous approaches did not fully\/properly tune the softmax baselines, or that"":-4.5768914223},""I don't understand why Distance Metric Learning is being used for classification tasks. Even when these datasets are used in a \""retrieval\"" setting, the ground truth is still defined by category membership. Direct classification is better."":{""I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. ----------------Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all"":-4.3681330681,""I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding). I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc). As far as I can tell, there is really only a single \""retrieval\""-like benchmark being used here - the Stanford Online Products database. All the other datasets are used in a \""classification-by-retrieval\"" approach which seems contrived. While ostensibly evaluating \""retrieval\"", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category. With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task.----------------It seems obvious that if your"":-2.3766257763,""There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance \/ interest to ICLR. This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches? Thorough experiments seem to indicate that this is indeed the case. Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets \/ tasks for which the DML approaches were designed. ----------------This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature. The authors argue that previous approaches did not fully\/properly tune the softmax baselines, or that"":-4.6007418633},""A recent paper shows that a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches."":{""I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method. However, does it support the claim that softmax-based features work much better than DML learned features? I have doubts on this claim. ----------------Also the experiments are a little bit misleading. What is vanilla googleNet softmax finetuned results? It seems it is not Rippel et al. (softmax prob) result. I am wondering whether the improvement comes from a) using retrieval (nearest neighbor) for classification or b) adding a new layer on top of pool5 or c) L2 normalization of the features. It is not clear to me at all"":-10.4749660492,""I have a huge, big picture concern about this paper and the papers it most closely addresses (MagnetLoss and Lifted Feature Structure Embedding). I don't understand why Distance Metric Learning (DML) is being used for classification tasks (Stanford Cars 196, UCSD Birds 200, Oxford 102 flowers, Stanford Dogs, ImageNet attributes, etc). As far as I can tell, there is really only a single \""retrieval\""-like benchmark being used here - the Stanford Online Products database. All the other datasets are used in a \""classification-by-retrieval\"" approach which seems contrived. While ostensibly evaluating \""retrieval\"", the retrieval ground truth is totally defined by category membership so these are still classification tasks with many instances in each category. With the Online Products dataset the correspondence between queries and correct results is much more fine grained so it makes sense to think of it as a retrieval task.----------------It seems obvious that if your"":-10.9517526627,""There has been substantial recent interest in representation learning, and specifically, using distance metric learning (DML) to learn representations where semantic distance between inputs can be measured. This is a topic of particular relevance \/ interest to ICLR. This paper poses a simple yet provocative question: can a standard SoftMax based approach learn features that match or even outperform recent state-of-the-art DML approaches? Thorough experiments seem to indicate that this is indeed the case. Comparisons are made to recent DML approaches including Magnet Loss (ICLR2016) and Lifted structure embedding (CVPR2016) and superior results are shown across a number of datasets \/ tasks for which the DML approaches were designed. ----------------This main result is a bit surprising since SoftMax is a natural and trivial baseline, so it should have been properly evaluated in previous DML literature. The authors argue that previous approaches did not fully\/properly tune the softmax baselines, or that"":-7.5778622627}}","The paper aims to compare the representations learnt by metric learning and classification objectives. While this is an interesting topic, the presented evaluation is not sufficiently clear for the paper to be accepted."
https://openreview.net/forum?id=S1AG8zYeg,"['This paper extends the ""order matters"" idea from the sentence level to an interesting application on discourse level. Experiments show the capacity of the proposed model on both order discrimination task and sentence ordering.'
 'This paper presents an empirical study on sentence ordering using RNN variants. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015)'
 'This paper introduces a new neural network model for sentence ordering. The model is presented as an encoder-decoder, and uses recent development for neural network. While I am not entirely convinced by the task of sentence ordering, this approach seems']","['This paper extends the ""order matters"" idea from the sentence level to an interesting application on discourse level. Experiments show the capacity of the proposed model on both order discrimination task and sentence ordering.'
 'This paper presents an empirical study on sentence ordering using RNN variants. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015)'
 'This paper introduces a new neural network model for sentence ordering. The model is presented as an encoder-decoder, and uses recent development for neural network. While I am not entirely convinced by the task of sentence ordering, this approach seems']","This paper extends the ""order matters"" idea from the sentence level to an interesting application on discourse level. Experiments show the capacity of the proposed model on both order discrimination task and sentence ordering.                               0.979589
This paper presents an empirical study on sentence ordering using RNN variants. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015)                                                                              0.671483
This paper introduces a new neural network model for sentence ordering. The model is presented as an encoder-decoder, and uses recent development for neural network. While I am not entirely convinced by the task of sentence ordering, this approach seems    0.342179
dtype: float32","This paper extends the ""order matters"" idea from the sentence level to an interesting application on discourse level. Experiments show the capacity of the proposed model on both order discrimination task and sentence ordering.                               1.098612
This paper presents an empirical study on sentence ordering using RNN variants. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015)                                                                              1.098612
This paper introduces a new neural network model for sentence ordering. The model is presented as an encoder-decoder, and uses recent development for neural network. While I am not entirely convinced by the task of sentence ordering, this approach seems    1.098612
dtype: float32","{""This paper extends the \""order matters\"" idea from the sentence level to an interesting application on discourse level. Experiments show the capacity of the proposed model on both order discrimination task and sentence ordering."":{""This paper extends the \""order matters\"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.----------------I think the problem is interesting and the results are promising. However, there are some problems about technical details:----------------- Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)?--------- Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function?--------- What is the connection between the encoder and decoder in the proposed model? How to combine them together?"":-3.2047629356,""This paper presents an empirical study on sentence ordering using RNN variants.----------------I\u2019m not sure how strong novelty this paper brings in terms of technical contributions. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks. Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence. Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.----------------That said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs."":-7.8899893761,""The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).----------------The first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a \""bag-of-sentences\"", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.----------------The model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-"":-7.5568580627},""This paper presents an empirical study on sentence ordering using RNN variants. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015)"":{""This paper extends the \""order matters\"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.----------------I think the problem is interesting and the results are promising. However, there are some problems about technical details:----------------- Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)?--------- Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function?--------- What is the connection between the encoder and decoder in the proposed model? How to combine them together?"":-7.1301608086,""This paper presents an empirical study on sentence ordering using RNN variants.----------------I\u2019m not sure how strong novelty this paper brings in terms of technical contributions. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks. Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence. Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.----------------That said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs."":-4.22006464,""The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).----------------The first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a \""bag-of-sentences\"", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.----------------The model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-"":-6.8568735123},""This paper introduces a new neural network model for sentence ordering. The model is presented as an encoder-decoder, and uses recent development for neural network. While I am not entirely convinced by the task of sentence ordering, this approach seems"":{""This paper extends the \""order matters\"" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering.----------------I think the problem is interesting and the results are promising. However, there are some problems about technical details:----------------- Why there are two components of LSTM hidden state (h_{enc}^{t-1},c_{ent}^{t-1}), what information is captured by each of these hidden states? Refer to (Vinyals et al. 2015a)?--------- Some notations in this paper are confusing. For example, what is the form of W in the feed-forward scoring function? Does it have the same form as the W in the bilinear score function?--------- What is the connection between the encoder and decoder in the proposed model? How to combine them together?"":-3.8249118328,""This paper presents an empirical study on sentence ordering using RNN variants.----------------I\u2019m not sure how strong novelty this paper brings in terms of technical contributions. The proposed method closely follows the read, process, and write framework of Vinyals et al. (2015) that has been developed for set-to-sequence type tasks. Sentence ordering is a good fit for set-to-sequence formulation, as the input sentences are rather a set than a sequence. Thus, the main contribution of this work can be viewed as a new application of an existing model rather than a new model development.----------------That said, the empirical evaluation of the paper is very solid and the authors have updated the supplementary material to strengthen the evaluation even further in response to the QAs."":-3.8139801025,""The main contribution of this paper is to introduce a new neural network model for sentence ordering. This model is presented as an encoder-decoder, and uses recent development for neural network (pointer networks and order invariant RNNs).----------------The first processing step of the model is to encode each sentence using a word level LSTM recurrent network. An order invariant encoder (based on Vinyals et al. (2015)) is then used to obtain a representation of the set of sentences. The input at each time step of this encoder is a \""bag-of-sentences\"", over which attention probabilities are computed. The last hidden representation of the encoder is then used to initialize the decoder. This decoder is a pointer network, and is used to predict the order of the input sentences.----------------The model introduced in this paper is then compared to standard method for sentence ordering, as well as a model with the decoder only. The encoder-"":-2.0897698402}}","Sentence ordering is central to a number of NLP tasks (e.g., summarization), and this paper introduces a very sensible application of existing techniques, experimentations is also very solid. However, the technical contributions seems quite limited.    Positive:    -- An important NLP problem (I would disagree here with Reviewer1), a good match between the problem and the methods  -- Solid experiments   -- A well-written paper    Negative:    -- A straightforward application of existing methods    Unfortunately, the latter, I believe, is a problem. Though application papers are welcome, I believe they should go beyond direct applications and provide extra insights to the representation learning community."
https://openreview.net/forum?id=S1Bm3T_lg,"['Compositional Kernel Machines (CKMs) are a new learning model. CKMs use sum-product networks to construct compositional kernel functions.'
 'The authors propose a method to efficiently augment an SVM variant with many virtual instances. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.'
 'The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.'
 'This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines two ideas: kernel methods and sum-product network (SPN)']","['Compositional Kernel Machines (CKMs) are a new learning model. CKMs use sum-product networks to construct compositional kernel functions.'
 'The authors propose a method to efficiently augment an SVM variant with many virtual instances. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.'
 'The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.'
 'This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines two ideas: kernel methods and sum-product network (SPN)']","Compositional Kernel Machines (CKMs) are a new learning model. CKMs use sum-product networks to construct compositional kernel functions.                                                                                           0.604597
The authors propose a method to efficiently augment an SVM variant with many virtual instances. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.    1.189986
The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.                                                                             0.927445
This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines two ideas: kernel methods and sum-product network (SPN)                                                                       0.701109
dtype: float32","Compositional Kernel Machines (CKMs) are a new learning model. CKMs use sum-product networks to construct compositional kernel functions.                                                                                           1.386294
The authors propose a method to efficiently augment an SVM variant with many virtual instances. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.    1.386294
The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.                                                                             1.386294
This paper proposes a new learning framework called ""compositional kernel machines"" (CKM) It combines two ideas: kernel methods and sum-product network (SPN)                                                                       1.386293
dtype: float32","{""Compositional Kernel Machines (CKMs) are a new learning model. CKMs use sum-product networks to construct compositional kernel functions."":{""This paper proposes a new learning model \""Compositional Kernel Machines (CKMs)\"" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets' success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper \""Deep Convolutional Networks are Hierarchical Kernel Machines\"" by Anselmi, F. et al. seems to be interesting to the authors.--------Experiments seem to be preliminary in this paper. It"":-3.851647377,""The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.----------------Pros:--------- Thoughtful methodology with sensible design choices--------- Potentially useful for smaller (n < 10000) datasets with a lot of statistical structure--------- Nice connections with sum-product literature----------------Cons:--------- Claims about scalability are very unclear--------- Generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.--------- Experiments are very preliminary ----------------The scalability claims are particularly unclear. The paper repeatedly mentions lack of scalability as a drawback for convnets, but it appears the proposed CKM is less scalable than a standard SVM, yet SVMs often handle much fewer training instances than deep neural networks. It appears the scalability advantages are mostly for training sets with roughly fewer than 10,000 instances -- and even if the"":-6.6557712555,""Thank you for an interesting read. The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.----------------Pros--------- The approach seems to decrease the training time, which is of prime importance in deep learning. Although, that comes at a price of slightly more complex model.--------- There is a grounded theory for sum-product functions which is basis for the compositional architecture described in the paper. Theoretically, any semiring and kernel could be used for the model which decreases need for handcrafting the structure of the model, which is a big problem in existing convolutional neural networks.----------------Cons--------- The experiments are on very simple dataset NORB. Although, it is great to understand a model's dynamics on a simpler dataset, some analysis on complex datasets are important to act as empirical evidence. The compositional kernel approach is compared to convolutional neural networks, hence it is only fair to compare"":-6.2364478111,""This paper proposes a new learning framework called \""compositional kernel machines\"" (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.----------------Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). ----------------Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do"":-5.7497086525},""The authors propose a method to efficiently augment an SVM variant with many virtual instances. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims."":{""This paper proposes a new learning model \""Compositional Kernel Machines (CKMs)\"" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets' success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper \""Deep Convolutional Networks are Hierarchical Kernel Machines\"" by Anselmi, F. et al. seems to be interesting to the authors.--------Experiments seem to be preliminary in this paper. It"":-5.0281310081,""The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.----------------Pros:--------- Thoughtful methodology with sensible design choices--------- Potentially useful for smaller (n < 10000) datasets with a lot of statistical structure--------- Nice connections with sum-product literature----------------Cons:--------- Claims about scalability are very unclear--------- Generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.--------- Experiments are very preliminary ----------------The scalability claims are particularly unclear. The paper repeatedly mentions lack of scalability as a drawback for convnets, but it appears the proposed CKM is less scalable than a standard SVM, yet SVMs often handle much fewer training instances than deep neural networks. It appears the scalability advantages are mostly for training sets with roughly fewer than 10,000 instances -- and even if the"":-0.5413906574,""Thank you for an interesting read. The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.----------------Pros--------- The approach seems to decrease the training time, which is of prime importance in deep learning. Although, that comes at a price of slightly more complex model.--------- There is a grounded theory for sum-product functions which is basis for the compositional architecture described in the paper. Theoretically, any semiring and kernel could be used for the model which decreases need for handcrafting the structure of the model, which is a big problem in existing convolutional neural networks.----------------Cons--------- The experiments are on very simple dataset NORB. Although, it is great to understand a model's dynamics on a simpler dataset, some analysis on complex datasets are important to act as empirical evidence. The compositional kernel approach is compared to convolutional neural networks, hence it is only fair to compare"":-4.8244247437,""This paper proposes a new learning framework called \""compositional kernel machines\"" (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.----------------Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). ----------------Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do"":-4.9039707184},""The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence."":{""This paper proposes a new learning model \""Compositional Kernel Machines (CKMs)\"" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets' success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper \""Deep Convolutional Networks are Hierarchical Kernel Machines\"" by Anselmi, F. et al. seems to be interesting to the authors.--------Experiments seem to be preliminary in this paper. It"":-7.2957806587,""The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.----------------Pros:--------- Thoughtful methodology with sensible design choices--------- Potentially useful for smaller (n < 10000) datasets with a lot of statistical structure--------- Nice connections with sum-product literature----------------Cons:--------- Claims about scalability are very unclear--------- Generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.--------- Experiments are very preliminary ----------------The scalability claims are particularly unclear. The paper repeatedly mentions lack of scalability as a drawback for convnets, but it appears the proposed CKM is less scalable than a standard SVM, yet SVMs often handle much fewer training instances than deep neural networks. It appears the scalability advantages are mostly for training sets with roughly fewer than 10,000 instances -- and even if the"":-7.0831384659,""Thank you for an interesting read. The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.----------------Pros--------- The approach seems to decrease the training time, which is of prime importance in deep learning. Although, that comes at a price of slightly more complex model.--------- There is a grounded theory for sum-product functions which is basis for the compositional architecture described in the paper. Theoretically, any semiring and kernel could be used for the model which decreases need for handcrafting the structure of the model, which is a big problem in existing convolutional neural networks.----------------Cons--------- The experiments are on very simple dataset NORB. Although, it is great to understand a model's dynamics on a simpler dataset, some analysis on complex datasets are important to act as empirical evidence. The compositional kernel approach is compared to convolutional neural networks, hence it is only fair to compare"":-3.9533603191,""This paper proposes a new learning framework called \""compositional kernel machines\"" (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.----------------Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). ----------------Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do"":-7.1394000053},""This paper proposes a new learning framework called \""compositional kernel machines\"" (CKM) It combines two ideas: kernel methods and sum-product network (SPN)"":{""This paper proposes a new learning model \""Compositional Kernel Machines (CKMs)\"" that extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. This paper considers the convnets as nicely learned nonlinear decision functions and resort their success in classification to their compositional nature. This perspective motivates the design of compositional kernel functions and the sum-product implementation is indeed interesting. I agree the composition is important for convnets, but it is not the whole story of convnets' success. One essential difference between convnets and CKMs is that all the kernels in convnets are learned directly from data while CKMs still build on top of feature descriptors. This, I believe, limits the representation power of CKMs. A recent paper \""Deep Convolutional Networks are Hierarchical Kernel Machines\"" by Anselmi, F. et al. seems to be interesting to the authors.--------Experiments seem to be preliminary in this paper. It"":-2.9013507366,""The authors propose a method to efficiently augment an SVM variant with many virtual instances, and show promising preliminary results. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.----------------Pros:--------- Thoughtful methodology with sensible design choices--------- Potentially useful for smaller (n < 10000) datasets with a lot of statistical structure--------- Nice connections with sum-product literature----------------Cons:--------- Claims about scalability are very unclear--------- Generally the paper does not succeed in telling a complete story about the properties and applicability of the proposed method.--------- Experiments are very preliminary ----------------The scalability claims are particularly unclear. The paper repeatedly mentions lack of scalability as a drawback for convnets, but it appears the proposed CKM is less scalable than a standard SVM, yet SVMs often handle much fewer training instances than deep neural networks. It appears the scalability advantages are mostly for training sets with roughly fewer than 10,000 instances -- and even if the"":-4.4876365662,""Thank you for an interesting read. The ideas presented have a good basis of being true, but the experiments are rather too simple. It would be interesting to see more empirical evidence.----------------Pros--------- The approach seems to decrease the training time, which is of prime importance in deep learning. Although, that comes at a price of slightly more complex model.--------- There is a grounded theory for sum-product functions which is basis for the compositional architecture described in the paper. Theoretically, any semiring and kernel could be used for the model which decreases need for handcrafting the structure of the model, which is a big problem in existing convolutional neural networks.----------------Cons--------- The experiments are on very simple dataset NORB. Although, it is great to understand a model's dynamics on a simpler dataset, some analysis on complex datasets are important to act as empirical evidence. The compositional kernel approach is compared to convolutional neural networks, hence it is only fair to compare"":-3.7117872238,""This paper proposes a new learning framework called \""compositional kernel machines\"" (CKM). It combines two ideas: kernel methods and sum-product network (SPN). CKM first defines leaf kernels on elements of the query and training examples, then it defines kernel recursively (similar to sum-product network). This paper has shown that the evaluation CKM can be done efficiently using the same tricks in SPN.----------------Positive: I think the idea in this paper is interesting. Instance-based learning methods (such as SVM with kernels) have been successful in the past, but have been replaced by deep learning methods (e.g. convnet) in the past few years. This paper investigate an unexplored area of how to combine the ideas from kernel methods and deep networks (SPN in this case). ----------------Negative: Although the idea of this paper is interesting, this paper is clearly very preliminary. In its current form, I simply do"":-1.0191167593}}","There is consensus among the reviewers that the proposed method has potential merit, but that the experimental evaluation is too preliminary to warrant publication of the current manuscript. The paper also appears to make broad claims that are not fully supported by the results of the study. I encourage the authors to address the comments of the reviewers in future revisions of this work. Meanwhile, this paper would make a good contribution to the workshop track."
https://openreview.net/forum?id=S1HEBe_Jl,"['The idea considered here is cute. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact.'
 'The paper deals with an interesting application of adversarial training to encryption. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.'
 'This paper proposed to use GAN for encrypted communications.']","['The idea considered here is cute. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact.'
 'The paper deals with an interesting application of adversarial training to encryption. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.'
 'This paper proposed to use GAN for encrypted communications.']","The idea considered here is cute. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact.                                          0.061397
The paper deals with an interesting application of adversarial training to encryption. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.    0.913058
This paper proposed to use GAN for encrypted communications.                                                                                                                                     0.738579
dtype: float32","The idea considered here is cute. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact.                                          1.098461
The paper deals with an interesting application of adversarial training to encryption. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.    1.098612
This paper proposed to use GAN for encrypted communications.                                                                                                                                     1.098612
dtype: float32","{""The idea considered here is cute. While this is a nice thought experiment, there are significant barriers to this submission having a practical impact."":{""The submission proposes to modify the typical GAN architecture slightly to include \""encrypt\"" (Alice) and \""decrypt\"" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:--------\""As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.\""----------------The idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to"":-6.3831167221,""The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.----------------The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is"":-7.0249242783,""This paper proposed to use GAN for encrypted communications.----------------In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.----------------In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text."":-7.1492009163},""The paper deals with an interesting application of adversarial training to encryption. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy."":{""The submission proposes to modify the typical GAN architecture slightly to include \""encrypt\"" (Alice) and \""decrypt\"" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:--------\""As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.\""----------------The idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to"":-4.6320233345,""The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.----------------The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is"":-0.59429425,""This paper proposed to use GAN for encrypted communications.----------------In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.----------------In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text."":-4.4447002411},""This paper proposed to use GAN for encrypted communications."":{""The submission proposes to modify the typical GAN architecture slightly to include \""encrypt\"" (Alice) and \""decrypt\"" (Bob) modules as well as a module trying to decrypt the signal without a key (Eve).  Through repeated transmission of signals, the adversarial game is intended to converge to a system in which Alice and Bob can communicate securely (or at least a designated part of the signal should be secure), while a sophisticated Eve cannot break their code.  Examples are given on toy data:--------\""As a proof-of-concept, we implemented Alice, Bob, and Eve networks that take N-bit random plain-text and key values, and produce N-entry floating-point ciphertexts, for N = 16, 32, and 64.  Both plaintext and key values are uniformly distributed.\""----------------The idea considered here is cute.  If some, but not necessarily all of the signal is meant to be secure, the modules can learn to"":-26.7874183655,""The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy. The concepts, ideas and previous literature are quite nicely and carefully presented.----------------The only major concern I have - and I apologize to the authors for not raising this earlier - are the experiments in section 3. In particular, I don't quite get the scenario. The reasoning here seems to be as follows: given information < A, B, C, D >, I want to give the public the value of D (e.g. movies watched) without releasing information about C (e.g. gender). In this scenario, Eve would need to be able to reconstruct D as good as possible without gaining information about C. What is"":-27.1673870087,""This paper proposed to use GAN for encrypted communications.----------------In section 2, the authors proposed a 3 part neural network trained to encode and decode data. This model does not have any practical value except paving the way for describing the next model in section 3: it is strictly worse than any provable cryptography system.----------------In section 3, the authors designed a task where they want to hide part of the data, which has correlated fields, while publishing the rest. However, I'm having trouble thinking of an application where this system is better than simply decorrelating the data and encrypting the fields one wants to hide with a provable cryptography system while publishing the rest in plain text."":-23.9391860962}}",Interesting paper but not over the accept bar.
https://openreview.net/forum?id=S1J0E-71l,"['This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network as an input to the network.'
 'This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function.'
 'This paper proposes to leverage ""surprisal"" as top-down signal in RNN. The paper in its current form has some important flaws. Overall, the paper writing could be improved.']","['This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network as an input to the network.'
 'This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function.'
 'This paper proposes to leverage ""surprisal"" as top-down signal in RNN. The paper in its current form has some important flaws. Overall, the paper writing could be improved.']","This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network as an input to the network.    0.653575
This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function.                                                                                               0.867192
This paper proposes to leverage ""surprisal"" as top-down signal in RNN. The paper in its current form has some important flaws. Overall, the paper writing could be improved.                                            0.643936
dtype: float32","This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network as an input to the network.    1.098612
This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function.                                                                                               1.098612
This paper proposes to leverage ""surprisal"" as top-down signal in RNN. The paper in its current form has some important flaws. Overall, the paper writing could be improved.                                            1.098612
dtype: float32","{""This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network as an input to the network."":{""Summary:--------This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.----------------Contributions:--------The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.----------------Questions:--------A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.----------------Criticisms:--------The paper is really badly written, authors should rethink the organization of the paper.--------Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. --------The justification is not convincing enough."":-1.3596280813,""This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. -----------------This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.-----------------The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other"":-4.4790320396,""This paper proposes to leverage \""surprisal\"" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.----------------The general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.----------------- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.----------------- The experimental evaluation is limited. Only one dataset"":-3.7286143303},""This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function."":{""Summary:--------This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.----------------Contributions:--------The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.----------------Questions:--------A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.----------------Criticisms:--------The paper is really badly written, authors should rethink the organization of the paper.--------Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. --------The justification is not convincing enough."":-14.7094116211,""This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. -----------------This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.-----------------The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other"":-11.1877746582,""This paper proposes to leverage \""surprisal\"" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.----------------The general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.----------------- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.----------------- The experimental evaluation is limited. Only one dataset"":-14.9741106033},""This paper proposes to leverage \""surprisal\"" as top-down signal in RNN. The paper in its current form has some important flaws. Overall, the paper writing could be improved."":{""Summary:--------This paper proposes to use surprisal-driven feedback for training recurrent neural networks where they feedback the next-step prediction error of the network as an input to the network. Authors have shown a result on language modeling tasks.----------------Contributions:--------The introduction of surprisal-driven feedback, which is just the feedback from the errors of the model from the previous time-steps.----------------Questions:--------A point which is not fully clear from the paper is whether if you have used the ground-truth labels on the test set for the surprisal feedback part of the model? I assume that authors do that since they claim that they use the misprediction error as additional input.----------------Criticisms:--------The paper is really badly written, authors should rethink the organization of the paper.--------Most of the equations presented in the paper, about BPTT are not necessary for the main-text and could be moved to Appendix. --------The justification is not convincing enough."":-3.563441515,""This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. -----------------This paper makes an  erroneous assumption: test label information is not given in most of the real world applications, except few applications. This means that the language modelling task, which is the only experiment of this paper, may not be the right task to test this approach. Also, comparing against the models that do not use test error signal at inference time is unfair. We cannot just say that the test label information is being observed, this only holds in online-prediction problems.-----------------The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them. I understand that this paper came before the other"":-3.8382530212,""This paper proposes to leverage \""surprisal\"" as top-down signal in RNN. More specifically author uses the error corresponding to the previous prediction as an extra input at the current timestep in a LSTM.----------------The general idea of suprising-driven feedback is interesting for online prediction task. It is a simple enough idea that seems to bring some significant improvements. However, the paper in its current form has some important flaws.----------------- Overall, the paper writing could be improved. In particular, section 2.4 and 2.5 is composed mostly by the equations of the forward and backward propagation of feedback RNN and feedback LSTM. However, author provides no analysis along with those equations. It is therefore not clear what insight the author tries to express in those sections. In addition, feedback RNN is not evaluated in the experimental section, so it is not clear why feedback RNN is described.----------------- The experimental evaluation is limited. Only one dataset"":-1.0259371996}}","Based on the feedback, I'm going to be rejecting the paper on the following grounds:  1. Results are not SOTA as reported.  2. No real experiments other than cursory experiments on Hutter prize data.  2. Writing is very poor.    However, just for playing devil's advocate, to the reviewers, I would like to point out that I am in agreement with the author that dynamic evaluation is not equivalent to this method. The weights are not changed in this model, as far as I can see, for the test set. Surprisal is just an extra input to the model. I think the reviewers were puzzled by the fact that at test time, the actual sequence needs to be known. While this may be problematic for generative modeling, I do not see why this would be a problem for language modeling, where the goal of the model is only to provide a log prob to evaluate how good a sequence of text is. Long before language modeling started being used to generate text, this was the main reason to use it - in speech recognition, spelling correction etc.."
https://openreview.net/forum?id=S1Jhfftgx,"['The paper attempted to solve an interesting problem. The main idea is to modify the weight of the neural network to find a feasible solution.'
 'The proposed approach looks more like a ""little hack"" to try to make it vaguely similar to Lagrangian relaxation methods. The accuracy reported in Table 1 needs to be explained.'
 ""This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-""]","['The paper attempted to solve an interesting problem. The main idea is to modify the weight of the neural network to find a feasible solution.'
 'The proposed approach looks more like a ""little hack"" to try to make it vaguely similar to Lagrangian relaxation methods. The accuracy reported in Table 1 needs to be explained.'
 ""This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-""]","The paper attempted to solve an interesting problem. The main idea is to modify the weight of the neural network to find a feasible solution.                                                                                                               0.585393
The proposed approach looks more like a ""little hack"" to try to make it vaguely similar to Lagrangian relaxation methods. The accuracy reported in Table 1 needs to be explained.                                                                           0.866760
This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-    0.812173
dtype: float32","The paper attempted to solve an interesting problem. The main idea is to modify the weight of the neural network to find a feasible solution.                                                                                                               1.098612
The proposed approach looks more like a ""little hack"" to try to make it vaguely similar to Lagrangian relaxation methods. The accuracy reported in Table 1 needs to be explained.                                                                           1.098612
This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-    1.098612
dtype: float32","{""The paper attempted to solve an interesting problem. The main idea is to modify the weight of the neural network to find a feasible solution."":{""This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is interesting, and it tries to solve an important problem. However, it seems to me the paper is not ready to publish yet.----------------Comments:----------------- The first section of the paper is clear and well-motivated. ----------------- The authors should report test running time. The proposed approach changes the weight matrix. As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7). This is actually pretty expensive in comparison to running the beam search or other inference methods. Therefore, I'm not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD).   ----------------- If I"":-9.5152578354,""This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems, while keeping inference unconstrained. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured prediction energy networks (SPENs), recently proposed by Belanger et al. ----------------Overall, I didn't find the approach very convincing and the paper has a few problems regarding the empirical evaluation. There's also some imprecisions throughout. The proposed approach (secs 6 and 7) looks more like a \""little hack\"" to try to make it vaguely similar to Lagrangian relaxation methods than something that is theoretically well motivated.----------------Before eq. 6: \""an exponential number of dual variables\"" -- why exponential? it's not one dual variable per output.----------------From the clarification questions:--------- The accuracy reported in Table 1 needs to be explained. --------- for the parsing experiments it would be good to report the"":-11.8284578323,""This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems.----------------Many things don't quite make sense to me:-------- 1. Most seq2seq models (such as those used for parsing) have substantially better performance when coupled with beam search than greedy search, and exact search is infeasible. This is because these models are trained to condition on discrete values of past outputs in each timestamp, and hence the problem of finding the highest-scoring total sequence of outputs is not solvable efficiently. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-of-the-art, specially when constraint-aware beam search is used. This comparison is specially interesting because both constrained beam search and this dual-decomposition-like approach require multiple computations of the model's score.-------- 2. It's unclear (to me at least) how to differentiate the constraint term"":-12.1595335007},""The proposed approach looks more like a \""little hack\"" to try to make it vaguely similar to Lagrangian relaxation methods. The accuracy reported in Table 1 needs to be explained."":{""This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is interesting, and it tries to solve an important problem. However, it seems to me the paper is not ready to publish yet.----------------Comments:----------------- The first section of the paper is clear and well-motivated. ----------------- The authors should report test running time. The proposed approach changes the weight matrix. As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7). This is actually pretty expensive in comparison to running the beam search or other inference methods. Therefore, I'm not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD).   ----------------- If I"":-7.8835144043,""This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems, while keeping inference unconstrained. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured prediction energy networks (SPENs), recently proposed by Belanger et al. ----------------Overall, I didn't find the approach very convincing and the paper has a few problems regarding the empirical evaluation. There's also some imprecisions throughout. The proposed approach (secs 6 and 7) looks more like a \""little hack\"" to try to make it vaguely similar to Lagrangian relaxation methods than something that is theoretically well motivated.----------------Before eq. 6: \""an exponential number of dual variables\"" -- why exponential? it's not one dual variable per output.----------------From the clarification questions:--------- The accuracy reported in Table 1 needs to be explained. --------- for the parsing experiments it would be good to report the"":-4.3844838142,""This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems.----------------Many things don't quite make sense to me:-------- 1. Most seq2seq models (such as those used for parsing) have substantially better performance when coupled with beam search than greedy search, and exact search is infeasible. This is because these models are trained to condition on discrete values of past outputs in each timestamp, and hence the problem of finding the highest-scoring total sequence of outputs is not solvable efficiently. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-of-the-art, specially when constraint-aware beam search is used. This comparison is specially interesting because both constrained beam search and this dual-decomposition-like approach require multiple computations of the model's score.-------- 2. It's unclear (to me at least) how to differentiate the constraint term"":-8.1929397583},""This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-"":{""This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, the idea presented in the paper is interesting, and it tries to solve an important problem. However, it seems to me the paper is not ready to publish yet.----------------Comments:----------------- The first section of the paper is clear and well-motivated. ----------------- The authors should report test running time. The proposed approach changes the weight matrix. As a result, it needs to reevaluate the values of hidden states and perform the greedy search for each iteration of optimizing Eq (7). This is actually pretty expensive in comparison to running the beam search or other inference methods. Therefore, I'm not convinced that the proposed approach is a right direction for solving this problem (In table, 1, the authors mention that they run 100 steps of SGD).   ----------------- If I"":-4.2803301811,""This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems, while keeping inference unconstrained. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured prediction energy networks (SPENs), recently proposed by Belanger et al. ----------------Overall, I didn't find the approach very convincing and the paper has a few problems regarding the empirical evaluation. There's also some imprecisions throughout. The proposed approach (secs 6 and 7) looks more like a \""little hack\"" to try to make it vaguely similar to Lagrangian relaxation methods than something that is theoretically well motivated.----------------Before eq. 6: \""an exponential number of dual variables\"" -- why exponential? it's not one dual variable per output.----------------From the clarification questions:--------- The accuracy reported in Table 1 needs to be explained. --------- for the parsing experiments it would be good to report the"":-4.1009078026,""This paper proposes a dual-decomposition-inspired technique for enforcing constraints in neural network prediction systems.----------------Many things don't quite make sense to me:-------- 1. Most seq2seq models (such as those used for parsing) have substantially better performance when coupled with beam search than greedy search, and exact search is infeasible. This is because these models are trained to condition on discrete values of past outputs in each timestamp, and hence the problem of finding the highest-scoring total sequence of outputs is not solvable efficiently. It's unclear what kind of model this paper is using which allows for greedy decoding, and how well it compares to the state-of-the-art, specially when constraint-aware beam search is used. This comparison is specially interesting because both constrained beam search and this dual-decomposition-like approach require multiple computations of the model's score.-------- 2. It's unclear (to me at least) how to differentiate the constraint term"":-0.8328582048}}","The program committee appreciates the authors' response to the clarifying questions. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work."
https://openreview.net/forum?id=S1OufnIlx,"['The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. The results of the experiments are interesting, but not likely to systematically improve our understanding of the adversarial example phenomenon.'
 "". Sharif's approach was constrained in an interesting way. The present work is less sensational and more methodical.""
 '. The paper investigates whether adversarial examples survive different geometric and photometric image transformations. The paper considers three different methods to generate adversarial examples.']","['The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. The results of the experiments are interesting, but not likely to systematically improve our understanding of the adversarial example phenomenon.'
 "". Sharif's approach was constrained in an interesting way. The present work is less sensational and more methodical.""
 '. The paper investigates whether adversarial examples survive different geometric and photometric image transformations. The paper considers three different methods to generate adversarial examples.']","The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. The results of the experiments are interesting, but not likely to systematically improve our understanding of the adversarial example phenomenon.    0.793360
. Sharif's approach was constrained in an interesting way. The present work is less sensational and more methodical.                                                                                                                                                                         0.901041
. The paper investigates whether adversarial examples survive different geometric and photometric image transformations. The paper considers three different methods to generate adversarial examples.                                                                                       0.738547
dtype: float32","The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. The results of the experiments are interesting, but not likely to systematically improve our understanding of the adversarial example phenomenon.    1.098612
. Sharif's approach was constrained in an interesting way. The present work is less sensational and more methodical.                                                                                                                                                                         1.098612
. The paper investigates whether adversarial examples survive different geometric and photometric image transformations. The paper considers three different methods to generate adversarial examples.                                                                                       1.098612
dtype: float32","{""The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. The results of the experiments are interesting, but not likely to systematically improve our understanding of the adversarial example phenomenon."":{""The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. ----------------Pros:--------1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.--------2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. ----------------Cons:--------1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration"":-0.7368252873,""In some sense, the Sharif et al. work \""scooped\"" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study."":-4.3279695511,""Description.--------The paper investigates whether adversarial examples survive different geometric and photometric image transformations,--------including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera.--------The paper considers three different methods to generate adversarial examples \u2014 images with added small amount of noise that changes the output of a classification neural network.  In the quantitative experiments the paper assumes available access to the neural network and its parameters. Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network.   ----------------Strong  points.--------- adversarial examples are an interesting phenomenon that is worth detailed investigation.--------- the paper is well written and presented.--------- Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting.--------- Experiments are well done and solid.----------------Weak points:--------- Probably the main negative point is the amount of"":-3.7351174355},"". Sharif's approach was constrained in an interesting way. The present work is less sensational and more methodical."":{""The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. ----------------Pros:--------1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.--------2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. ----------------Cons:--------1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration"":-17.5106487274,""In some sense, the Sharif et al. work \""scooped\"" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study."":-13.7318496704,""Description.--------The paper investigates whether adversarial examples survive different geometric and photometric image transformations,--------including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera.--------The paper considers three different methods to generate adversarial examples \u2014 images with added small amount of noise that changes the output of a classification neural network.  In the quantitative experiments the paper assumes available access to the neural network and its parameters. Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network.   ----------------Strong  points.--------- adversarial examples are an interesting phenomenon that is worth detailed investigation.--------- the paper is well written and presented.--------- Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting.--------- Experiments are well done and solid.----------------Weak points:--------- Probably the main negative point is the amount of"":-17.6749038696},"". The paper investigates whether adversarial examples survive different geometric and photometric image transformations. The paper considers three different methods to generate adversarial examples."":{""The paper is well motivated and well written. The setting of the experiments is to investigate a particular case. While the results of experiments are interesting, such investigation is not likely to systematically improve our understanding of the adversarial example phenomenon. Overall, the contribution of the paper seems incremental. ----------------Pros:--------1. This paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge.--------2. Some observations of the experiments are interesting. For example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. ----------------Cons:--------1. As noticed by the authors, some similar works exist in the literature. According to the authors, what differs this work from other existing works is that this paper tend to fool NN by making very small perturbations of the input. But based on the experiments and the demonstration"":-10.6448154449,""In some sense, the Sharif et al. work \""scooped\"" this paper, but as the authors indicate, the spirit of the work remains somewhat different. Sharif's approach was constrained in an interesting way (usable surface area limited to front portion of glasses frames) and also a bit gimmicky (focused on fooling a small scale face ID system to select among a set of celebrities). The present work is less sensational and more methodical in its study of physical manifestations of adversarial patterns for standard benchmark objects. I think the paper is at least a little above the bar since it poses an interesting question and carries out an informative empirical study."":-11.5921096802,""Description.--------The paper investigates whether adversarial examples survive different geometric and photometric image transformations,--------including a complex transformation where the image is printed on the paper and captured again by a cell-phone camera.--------The paper considers three different methods to generate adversarial examples \u2014 images with added small amount of noise that changes the output of a classification neural network.  In the quantitative experiments the paper assumes available access to the neural network and its parameters. Qualitative results are shown for a set-up where the network used to generate adversarial images is different from the test network.   ----------------Strong  points.--------- adversarial examples are an interesting phenomenon that is worth detailed investigation.--------- the paper is well written and presented.--------- Results showing (and quantifying) that adversarial examples can survive a complex image transformation such as printing and re-capturing are interesting.--------- Experiments are well done and solid.----------------Weak points:--------- Probably the main negative point is the amount of"":-8.0210847855}}","This paper studies an interesting aspect of adversarial training with important practical applications: its robustness against transformations that correspond to physical world constraints. The paper demonstrates how to construct adversarial examples such that they can be used to attack a machine-learning device through a physical interface such as a camera device.     The reviewers agreed that this is a well-written paper which clearly describes its contributions and offers a detailed experimental section. The authors took into account the reviewer's concerns and the rebuttal phase offered interesting discussions.     In light of the reviews, the main critique of this work is its lack of significance, relative to existing works in adversarial examples. The authors, however, did a good job during the rebuttal phase to highlight the empirical nature of the work and the potential practical significance of their findings in the design of ML models. The AC concludes that the potential practical applications, while not significant enough to be part of the conference proceedings, are worthy to be disseminated in the workshop. I therefore recommend submitting this work to the workshop track."
https://openreview.net/forum?id=S1RP6GLle,"['Amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results.'
 'This paper argues to approach Super-Resolution as amortised MAP estimation.'
 'The paper presents an amortised MAP estimation method for SR problems.']","['Amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results.'
 'This paper argues to approach Super-Resolution as amortised MAP estimation.'
 'The paper presents an amortised MAP estimation method for SR problems.']","Amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results.    0.887923
This paper argues to approach Super-Resolution as amortised MAP estimation.                                                                              0.811265
The paper presents an amortised MAP estimation method for SR problems.                                                                                   0.340523
dtype: float32","Amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results.    1.098612
This paper argues to approach Super-Resolution as amortised MAP estimation.                                                                              1.098612
The paper presents an amortised MAP estimation method for SR problems.                                                                                   1.098610
dtype: float32","{""Amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results."":{""The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments:----------------1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.----------------2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still"":-0.8170482516,""Sincere apologies for the late review.----------------This paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout. Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested. ----------------Summary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain. ----------------Manuscript should be proof-read once more,"":-4.8409881592,""The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.--------Results are nicely demonstrated on several datasets.----------------I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? "":-4.3783812523},""This paper argues to approach Super-Resolution as amortised MAP estimation."":{""The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments:----------------1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.----------------2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still"":-13.3435401917,""Sincere apologies for the late review.----------------This paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout. Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested. ----------------Summary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain. ----------------Manuscript should be proof-read once more,"":-9.9829015732,""The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.--------Results are nicely demonstrated on several datasets.----------------I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? "":-13.3235139847},""The paper presents an amortised MAP estimation method for SR problems."":{""The paper presents a new framework to solve the SR problem - amortized MAP inference and adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve the problem of minimizing cross-entropy. Generally, it is a great paper. However, I still have several comments:----------------1) The proposed amortized MAP inference is novel and different from the previous SR methods. Combined with GAN, this framework can obtain plausible and good results. Compared with another GAN-based SR methods - Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, question may arise as to what this new formulation adds to the latest state-of-the-art.----------------2) Using an affine projection architecture as a constraint, the model do not need any corresponding {HR, LR} image pairs for training. However, when training the affine projection layer, we still"":-14.7052278519,""Sincere apologies for the late review.----------------This paper argues to approach Super-Resolution as amortised MAP estimation. A projection step to keep consistent HR-LR dependencies is proposed and experimentally verified to obtain better results throughout. Further three different methods to solve the resulting cross-entropy problem in Eq.9 are proposed and tested. ----------------Summary: Very good paper, very well written and presented. Experimental results are sufficient, the paper presents well chosen toy examples and real world applications. From my understanding the contributions for the field of super-resolutions are novel (3.2,3.3,3.4), parts that are specific for the training of GANs may have appeared in different variants elsewhere (see also discussion). I believe that this paper will be relevant to future work on super-resolution, the finding that GAN based model training yields most visually appealing results suggests further work in this domain. ----------------Manuscript should be proof-read once more,"":-15.1942777634,""The paper presents an amortised MAP estimation method for SR problems. By learning a neural network which learns to project to an affine subspace of SR solutions which are consistent with the LR method the method enables finding propoer solutions with by using a variety of methods: GANs, noise assisted and density assisted optimisation.--------Results are nicely demonstrated on several datasets.----------------I like the paper all in all, though I feel the writing can be polished by quite a bit and presentation should be made clearer. It was hard to follow at times and considering the subject matter is quite complicated making it clearer would help. Also, I would love to see some more analysis of the resulting the networks - what kind of features to they learn? "":-13.2186832428}}","All the reviewers agreed that the paper is original, of high quality, and worth publishing."
https://openreview.net/forum?id=S1di0sfgl,"[' neural network. Each layer has different time scale, and the scale is not fixed but variable and determined by a neural network.'
 'The paper proposes a modified RNN architecture with multiple layers. Higher layers are only passed lower layer states if a FLUSH operation is predicted. The paper is well-motivated, exceptionally well-composed.'
 'This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data. Their approach does not require boundary information to segment the sequence.']","[' neural network. Each layer has different time scale, and the scale is not fixed but variable and determined by a neural network.'
 'The paper proposes a modified RNN architecture with multiple layers. Higher layers are only passed lower layer states if a FLUSH operation is predicted. The paper is well-motivated, exceptionally well-composed.'
 'This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data. Their approach does not require boundary information to segment the sequence.']"," neural network. Each layer has different time scale, and the scale is not fixed but variable and determined by a neural network.                                                                                     0.857277
The paper proposes a modified RNN architecture with multiple layers. Higher layers are only passed lower layer states if a FLUSH operation is predicted. The paper is well-motivated, exceptionally well-composed.    0.848637
This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data. Their approach does not require boundary information to segment the sequence.        0.821037
dtype: float32"," neural network. Each layer has different time scale, and the scale is not fixed but variable and determined by a neural network.                                                                                     1.098612
The paper proposes a modified RNN architecture with multiple layers. Higher layers are only passed lower layer states if a FLUSH operation is predicted. The paper is well-motivated, exceptionally well-composed.    1.098612
This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data. Their approach does not require boundary information to segment the sequence.        1.098612
dtype: float32","{"" neural network. Each layer has different time scale, and the scale is not fixed but variable and determined by a neural network."":{""This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.----------------Question) Can you extend it to bidirectional RNN? "":-8.7868738174,""The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.----------------Pros:--------- Paper is well-motivated, exceptionally well-composed--------- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation--------- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.--------Cons:--------- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a"":-12.6817426682,""This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).----------------Their model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. ----------------The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.----------------Overall this paper presents a strong and novel model with promising experimental results.--------------------------------"":-12.1279239655},""The paper proposes a modified RNN architecture with multiple layers. Higher layers are only passed lower layer states if a FLUSH operation is predicted. The paper is well-motivated, exceptionally well-composed."":{""This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.----------------Question) Can you extend it to bidirectional RNN? "":-4.1831011772,""The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.----------------Pros:--------- Paper is well-motivated, exceptionally well-composed--------- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation--------- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.--------Cons:--------- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a"":-0.7476332188,""This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).----------------Their model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. ----------------The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.----------------Overall this paper presents a strong and novel model with promising experimental results.--------------------------------"":-4.4072880745},""This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data. Their approach does not require boundary information to segment the sequence."":{""This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neural network framework, and shows the state-of-the-art performance on several benchmarks. The paper is well written.----------------Question) Can you extend it to bidirectional RNN? "":-7.0462589264,""The paper proposes a modified RNN architecture with multiple layers, where higher layers are only passed lower layer states if a FLUSH operation is predicted, consisting of passing up the state and reseting the lower layer's state. In order to select one of three operations at each time step, the authors propose using the straight-through estimator with a slope-annealing trick during training. Empirical results and visualizations illustrate that the modified architecture performs well at boundary detection.----------------Pros:--------- Paper is well-motivated, exceptionally well-composed--------- Provides promising initial results on learning hierarchical representations through visualizations and thorough experiments on language modeling and handwriting generation--------- The annealing trick with the straight-through estimator also seems potentially useful for other tasks containing discrete variables, and the trade-off in the flush operation is innovative.--------Cons:--------- In a couple cases the paper does not fully deliver. Empirical results on computational savings are not given, and hierarchy beyond a"":-6.9712924957,""This paper proposes a novel variant of recurrent networks that is able to learn the hierarchy of information in sequential data (e.g., character->word). Their approach does not require boundary information to segment the sequence in meaningful groups (like in Chung et al., 2016).----------------Their model is organized as a set of layers that aim at capturing the information form different \u201clevel of abstraction\u201d. The lowest level activate the upper one and decide when to update it based on a controller (or state cell, called c). A key feature of their model is that c is a discrete variable, allowing potentially fast inference time. However, this makes their model more challenging to learn, leading to the use of the straight-through estimator by Hinton, 2012. ----------------The experiment section is thorough and their model obtain competitive performance on several challenging tasks. The qualitative results show also that their model can capture natural boundaries.----------------Overall this paper presents a strong and novel model with promising experimental results.--------------------------------"":-3.6094126701}}","This extension to RNNs is clearly motivated, and the details of the proposed method are sensible. The paper would have benefitted from more experiments such as those in Figure 5 teasing out the representations learned by this model."
https://openreview.net/forum?id=S1j4RqYxg,"['The authors present a new algorithm for the effective calculation of polynomial features on Sparse Matrices.'
 'This paper proposes an algorithm for feature expansion on CSR matrices. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.'
 'The paper is beyond my expertise. The quality of writing requires improvement.']","['The authors present a new algorithm for the effective calculation of polynomial features on Sparse Matrices.'
 'This paper proposes an algorithm for feature expansion on CSR matrices. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.'
 'The paper is beyond my expertise. The quality of writing requires improvement.']","The authors present a new algorithm for the effective calculation of polynomial features on Sparse Matrices.                                                                                0.633701
This paper proposes an algorithm for feature expansion on CSR matrices. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.    0.782994
The paper is beyond my expertise. The quality of writing requires improvement.                                                                                                              0.805737
dtype: float32","The authors present a new algorithm for the effective calculation of polynomial features on Sparse Matrices.                                                                                1.098612
This paper proposes an algorithm for feature expansion on CSR matrices. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.    1.098612
The paper is beyond my expertise. The quality of writing requires improvement.                                                                                                              1.098612
dtype: float32","{""The authors present a new algorithm for the effective calculation of polynomial features on Sparse Matrices."":{""The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.----------------Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets. "":-7.4379878044,""This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.----------------The background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.----------------The experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as \""likely a"":-9.4778852463,""The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.----------------However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. "":-10.9801769257},""This paper proposes an algorithm for feature expansion on CSR matrices. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing."":{""The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.----------------Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets. "":-3.594473362,""This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.----------------The background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.----------------The experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as \""likely a"":-0.8312843442,""The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.----------------However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. "":-4.7054491043},""The paper is beyond my expertise. The quality of writing requires improvement."":{""The authors present here a new algorithm for the effective calculation of polynomial features on Sparse Matrices. The key idea is to use a proper mapping between matrices and their polynomial versions, in order to derive an effective CSR expansion algorithm. The authors analyse the time complexity in a convincing way with experiments.----------------Overall, the algorithm is definitely interesting, quite simple and nice, with many possible applications. The paper is however very superficial in terms of experiments, or applications of the proposed scheme. Most importantly, the fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets. "":-19.2721939087,""This paper proposes an algorithm for polynomial feature expansion on CSR matrices, which reduces the time complexity of the standard method by a factor d^k where d is the density of the sparse matrix. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing.----------------The background of the problem is not sufficiently introduced. There are only two references in the introduction part (overall only three papers are cited), which are from decades ago. Many more relevant papers should be cited from the recent literature.----------------The experiment part is very weak. This paper claims that the time complexity of their algorithm is O(d^k D^k), which is an improvement over standard method O(D^k) by a factor d^k. But in the experiments, when d=1, there is still a large gap (~14s vs. ~90s) between the proposed method and the standard one. The authors explain this as \""likely a"":-19.4201984406,""The paper is beyond my expertise. I cannot give any solid review comments regarding the techniques that are better than an educated guess.----------------However, it seems to me that the topic is not very relevant to the focus of ICLR. Also the quality of writing requires improvement, especially literature review and experiment analysis. "":-16.0204868317}}","The approach/problem seems interesting, and several reviewers commented on this. However, the experimental evaluation is quite preliminary and the paper would be helped a lot with a connection to a motivating application. All of the reviewers pointed out that the work is not written in the usual in the scope of ICLR papers, and putting these together at this time it makes sense to reject the paper."
https://openreview.net/forum?id=S1jmAotxg,"['This paper modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process.'
 'The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality.'
 'Stick-breaking priors does not generally improve upon spherically Gaussian priors in unsupervised setting. In a semi-supervised setting, the results are better.']","['This paper modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process.'
 'The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality.'
 'Stick-breaking priors does not generally improve upon spherically Gaussian priors in unsupervised setting. In a semi-supervised setting, the results are better.']","This paper modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process.        0.772081
The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality.    0.803803
Stick-breaking priors does not generally improve upon spherically Gaussian priors in unsupervised setting. In a semi-supervised setting, the results are better.                                           0.755845
dtype: float32","This paper modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process.        1.098612
The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality.    1.098612
Stick-breaking priors does not generally improve upon spherically Gaussian priors in unsupervised setting. In a semi-supervised setting, the results are better.                                           1.098612
dtype: float32","{""This paper modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process."":{""This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.----------------There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \""black-box\"" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for"":-0.7320871949,""The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.--------After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \""used\"" (which actually enable backpropagation) but the latent variables are parametrized differently (into ) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.--------With respect to the Figure 5b showing the decoder input weights:"":-3.4698696136,""Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.----------------In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.----------------Comments:--------- sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.--------- sec 2.2: two comma's--------- text flow eq 6: please refer to appendix with"":-4.5069932938},""The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality."":{""This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.----------------There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \""black-box\"" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for"":-4.7988066673,""The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.--------After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \""used\"" (which actually enable backpropagation) but the latent variables are parametrized differently (into ) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.--------With respect to the Figure 5b showing the decoder input weights:"":-2.0817797184,""Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.----------------In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.----------------Comments:--------- sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.--------- sec 2.2: two comma's--------- text flow eq 6: please refer to appendix with"":-6.424038887},""Stick-breaking priors does not generally improve upon spherically Gaussian priors in unsupervised setting. In a semi-supervised setting, the results are better."":{""This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. This is coupled with inference tailored to this model, specifically the Kumaraswamy distribution as an approximate variational posterior. The resulting model is named the SB-VAE which also has a semi-supervised extension, in similar vein to the original VAE paper.----------------There's a lot of interest in VAEs these days; many lines of work seek to achieve automatic \""black-box\"" inference in these models. For example, the authors themselves mention parallel work by Blei's lab (also others) towards this direction. However, there's a lot of merit in investigating more bespoke solutions to new models, which is what the authors are doing in this paper. Indeed, a (useful) side-effect of providing efficient inference for"":-6.135102272,""The paper attempts to combine Variational Auto-Encoders with the Stick-Breaking process. The motivation is to tackle the component collapsing and have a representation with stochastic dimensionality. To demonstrate the merit of their approach, the authors test this model on MNIST and SVHN in an unsupervised and semi-supervised fashion.--------After reading the paper in more detail, I find that the claim that the dimensionality of the latent variable is stochastic does not seem quite correct: all latent variables are \""used\"" (which actually enable backpropagation) but the latent variables are parametrized differently (into ) and the decoding process is altered as to give the impression of sparsity. The way all these latent variables are used does not involve any marginalization but is very similar to the common soft-gating mechanism already used in LSTM or attentional model.--------With respect to the Figure 5b showing the decoder input weights:"":-6.1488833427,""Summary: This is the first work to investigate stick-breaking priors, and corresponding inference methods, for use in VAEs. The background material is explained clearly, as well as the explanation of the priors and posteriors and their DNCP forms. The paper is really well written.----------------In experiments, they find that stick-breaking priors does not generally improve upon spherically Gaussian priors in the completely unsupervised setting, when measured w.r.t. log-likelihood. The fact that they do report this 'negative' result suggests good scientific taste. In a semi-supervised setting, the results are better.----------------Comments:--------- sec 2.1: There is plenty of previous work with non-Gaussian p(z): DRAW, the generative ResNet paper in the IAF paper, Ladder VAEs, etc.--------- sec 2.2: two comma's--------- text flow eq 6: please refer to appendix with"":-3.0431666374}}","This paper will make a positive contribution to the conference, especially since it is one of the first to look at stick-breaking as it applies to deep generative models. The paper will make a positive contribution to the conference."
https://openreview.net/forum?id=S1vyujVye,"['This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. The methods are technically similar to the “exemplar network’ (Dosovitskiy 2015)'
 'The proposed self supervised loss is formulated using a Siamese architecture. It encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image.'
 'A new paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches']","['This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. The methods are technically similar to the “exemplar network’ (Dosovitskiy 2015)'
 'The proposed self supervised loss is formulated using a Siamese architecture. It encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image.'
 'A new paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches']","This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. The methods are technically similar to the “exemplar network’ (Dosovitskiy 2015)    0.964132
The proposed self supervised loss is formulated using a Siamese architecture. It encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image.                0.868571
A new paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches                                        0.769192
dtype: float32","This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. The methods are technically similar to the “exemplar network’ (Dosovitskiy 2015)    1.098612
The proposed self supervised loss is formulated using a Siamese architecture. It encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image.                1.098612
A new paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches                                        1.098612
dtype: float32","{""This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. The methods are technically similar to the \u201cexemplar network\u2019 (Dosovitskiy 2015)"":{""This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. ----------------Strengths:----------------- The training objective is reasonable. In particular, high-level features show translation invariance. ----------------- The proposed methods are effective for initializing neural networks for supervised training on several datasets. ------------------------Weaknesses:----------------- The methods are technically similar to the \u201cexemplar network\u201d (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). ----------------- The paper"":-1.196305871,""The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches"":-5.8783812523,""This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:------------------------The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (\u201cprobability\u201d can be replaced with another word).----------------I would like to know more about how the"":-5.2889451981},""The proposed self supervised loss is formulated using a Siamese architecture. It encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image."":{""This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. ----------------Strengths:----------------- The training objective is reasonable. In particular, high-level features show translation invariance. ----------------- The proposed methods are effective for initializing neural networks for supervised training on several datasets. ------------------------Weaknesses:----------------- The methods are technically similar to the \u201cexemplar network\u201d (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). ----------------- The paper"":-6.8637857437,""The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches"":-3.4577257633,""This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:------------------------The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (\u201cprobability\u201d can be replaced with another word).----------------I would like to know more about how the"":-7.4176306725},""A new paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches"":{""This paper proposes an unsupervised training objective based on patch contrasting for visual representation learning using deep neural networks. In particular, the feature representations of the patches from the same image are encouraged to be closer than the those from different images. The distance ratios of positive training pairs are optimized. The proposed method are empirically shown to be effective as an initialization method for supervised training. ----------------Strengths:----------------- The training objective is reasonable. In particular, high-level features show translation invariance. ----------------- The proposed methods are effective for initializing neural networks for supervised training on several datasets. ------------------------Weaknesses:----------------- The methods are technically similar to the \u201cexemplar network\u201d (Dosovitskiy 2015). Cropping patches from a single image can be taken as a type of data augmentation, which is comparable to the data augmentation of positive sample (the exemplar) in (Dosovitskiy 2015). ----------------- The paper"":-3.6397173405,""The proposed self supervised loss is formulated using a Siamese architecture and encourages patches from the same image to lie closer in feature space than a contrasting patch taken from a different, random image. The loss is very similar in spirit to that of Doersch et al. ICCV 2015 and Isola et al. ICLR 2016 workshop. It seems that the proposed loss is actually a simplified version of Doersch et al. ICCV 2015 in that it does not make use of the spatial offset, a freely available self supervised signal in natural images. Intuitively, it seems that the self-supervised problem posed by this method is strictly simpler, and therefore less powerful, than that of the aforementioned work. I would like to see more discussion on the comparison of these two approaches. Nevertheless the proposed method seems to be effective in achieving good empirical results using this simple loss. Though more implementation details should be provided, such as the effect of patch size, overlap between sampled patches"":-4.181230545,""This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting (though likely applicable to fully-connected nets as well). The method is that of \u2018spatial constrasting\u2019, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patches from diferent images. The method is simple enough that I am surprised that no-one has tried this before (at least according to the previous work in the submission). Here are some comments:------------------------The usage of P(f_i^1 | f_i^2) in Section 4.1 is a bit odd. May be worth defining mathematically what kind of probability the authors are talking about, or just taking that part out (\u201cprobability\u201d can be replaced with another word).----------------I would like to know more about how the"":-0.7300976515}}","The paper proposes a formulation for unsupervised learning of ConvNets based on the distance between patches sampled from the same and different images. The novelty of the method is rather limited as it's similar to [Doersch et al. 2015] and [Dosovitsky et al. 2015]. The evaluation is only performed on the small datasets, which limits the potential impact of the contribution."
https://openreview.net/forum?id=S1xh5sYgx,"['Sqeezenet is a smaller CNN architecture designed for embedded CNN applications. Has x50 less memory usage than AlexNet, keeping similar accuracy.'
 'SqueezeNet is a smaller CNN architecture for embedded deployment. It is composed of fire modules. It has x50 less memory usage than AlexNet while keeping similar accuracy.'
 'Squeezenet paper has a series of reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition. The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet.']","['Sqeezenet is a smaller CNN architecture designed for embedded CNN applications. Has x50 less memory usage than AlexNet, keeping similar accuracy.'
 'SqueezeNet is a smaller CNN architecture for embedded deployment. It is composed of fire modules. It has x50 less memory usage than AlexNet while keeping similar accuracy.'
 'Squeezenet paper has a series of reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition. The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet.']","Sqeezenet is a smaller CNN architecture designed for embedded CNN applications. Has x50 less memory usage than AlexNet, keeping similar accuracy.                                                                                               0.573065
SqueezeNet is a smaller CNN architecture for embedded deployment. It is composed of fire modules. It has x50 less memory usage than AlexNet while keeping similar accuracy.                                                                     0.417608
Squeezenet paper has a series of reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition. The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet.    0.899346
dtype: float32","Sqeezenet is a smaller CNN architecture designed for embedded CNN applications. Has x50 less memory usage than AlexNet, keeping similar accuracy.                                                                                               1.098579
SqueezeNet is a smaller CNN architecture for embedded deployment. It is composed of fire modules. It has x50 less memory usage than AlexNet while keeping similar accuracy.                                                                     1.097490
Squeezenet paper has a series of reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition. The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet.    1.098612
dtype: float32","{""Sqeezenet is a smaller CNN architecture designed for embedded CNN applications. Has x50 less memory usage than AlexNet, keeping similar accuracy."":{""Strengths--------\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. --------\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.--------\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy --------\uf06e-- strong experimental results----------------Weaknesses--------\uf06e--Would be nice to test Sqeezenet on multiple tasks----------------\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more"":-7.4540009499,""Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.----------------Pros: --------Achieves x50 less memory usage than AlexNet while keeping similar accuracy.----------------Cons & Questions:--------Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?"":-9.1072540283,""The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.----------------Since the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.----------------On the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.----------------Oh"":-11.2380418777},""SqueezeNet is a smaller CNN architecture for embedded deployment. It is composed of fire modules. It has x50 less memory usage than AlexNet while keeping similar accuracy."":{""Strengths--------\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. --------\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.--------\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy --------\uf06e-- strong experimental results----------------Weaknesses--------\uf06e--Would be nice to test Sqeezenet on multiple tasks----------------\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more"":-5.700984478,""Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.----------------Pros: --------Achieves x50 less memory usage than AlexNet while keeping similar accuracy.----------------Cons & Questions:--------Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?"":-4.9053726196,""The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.----------------Since the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.----------------On the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.----------------Oh"":-8.8664121628},""Squeezenet paper has a series of reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition. The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet."":{""Strengths--------\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. --------\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.--------\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy --------\uf06e-- strong experimental results----------------Weaknesses--------\uf06e--Would be nice to test Sqeezenet on multiple tasks----------------\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more"":-4.6694121361,""Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.----------------Pros: --------Achieves x50 less memory usage than AlexNet while keeping similar accuracy.----------------Cons & Questions:--------Complex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?"":-5.2777647972,""The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.----------------Since the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.----------------On the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.----------------Oh"":-1.0928965807}}","The paper proposes a ConvNet architecture (""SqueezeNet"") and a building block (""Fire module"") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method."
https://openreview.net/forum?id=SJAr0QFxe,"['I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general.'
 'This paper studies the optimization issue of linear ResNet. The Hessian has condition number independent of depth.'
 'ResNet and other architectures that use shortcuts have shown empirical success in several domains. Some of the experiments in the paper are interesting.']","['I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general.'
 'This paper studies the optimization issue of linear ResNet. The Hessian has condition number independent of depth.'
 'ResNet and other architectures that use shortcuts have shown empirical success in several domains. Some of the experiments in the paper are interesting.']","I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general.    0.808574
This paper studies the optimization issue of linear ResNet. The Hessian has condition number independent of depth.                                                                           0.960572
ResNet and other architectures that use shortcuts have shown empirical success in several domains. Some of the experiments in the paper are interesting.                                     0.808378
dtype: float32","I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general.    1.098612
This paper studies the optimization issue of linear ResNet. The Hessian has condition number independent of depth.                                                                           1.098612
ResNet and other architectures that use shortcuts have shown empirical success in several domains. Some of the experiments in the paper are interesting.                                     1.098612
dtype: float32","{""I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general."":{""I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model."":-0.5475774407,""This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. ----------------This result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:----------------1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?----------------2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^"":-3.7773704529,""ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:----------------1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \""Singularity of Hessian in Deep Learning\"") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.----------------2- Hess"":-4.0013484955},""This paper studies the optimization issue of linear ResNet. The Hessian has condition number independent of depth."":{""I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model."":-16.3803291321,""This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. ----------------This result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:----------------1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?----------------2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^"":-10.97265625,""ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:----------------1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \""Singularity of Hessian in Deep Learning\"") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.----------------2- Hess"":-14.7099981308},""ResNet and other architectures that use shortcuts have shown empirical success in several domains. Some of the experiments in the paper are interesting."":{""I think the write-up can be improved. The results of the paper also might be somewhat misleading. The behavior for when weights are 0 is not revealing of how the model works in general. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model."":-10.0729427338,""This paper studies the optimization issue of linear ResNet, and shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I skimmed through the proof but have not checked them carefully. ----------------This result is a nice observation for training deep linear networks.  But I do not think the paper has fully resolved the linear vs nonlinear issue. Some question:----------------1. Though the revision has added some results using ReLU units, it seems it is only added to the mid positions of the network (sec 5.3), is this how it is typically done in ResNet? Moreover, ReLU is not differentiable at zero point, which does not satisfy the condition you had in Theorem 1. Why not use differentiable activations like sigmoid or tanh?----------------2. From equation (22) in the appendix, it seems for nonlinear activations, the condition number depends on the derivative \\sigma^"":-9.7452993393,""ResNet and other architectures that use shortcuts have shown empirical success in several domains and therefore, studying the optimization for such architectures is very valuable. This paper is an attempt to address some of the properties of networks that use shortcuts. Some of the experiments in the paper are interesting. However, there are two main issues with the current paper:----------------1- linear vs non-linear: I think studying linear networks is valuable but we should be careful not to extend the results to networks with non-linear activations without enough evidence. This is especially true for Hessian as the Hessian of non-linear networks have very large condition number (see the ICLR submission \""Singularity of Hessian in Deep Learning\"") even in cases where the optimization is not challenging. Therefore, I don't agree with the claims in the paper on non-linear networks. Moreover, one plot on MNIST is not enough to claim that non-linear networks behave similar to linear networks.----------------2- Hess"":-6.5634465218}}","This paper endeavors to offer theoretical explanations of the performance of ResNet. Providing better theoretical understanding of existing empirically powerful architectures is very important work and I commend the authors for tackling this. Unfortunately, this paper falls short in its current form: the particular choices and restrictions made (0 weights, linear regime) limit applicability to ResNet, and do not seem to offer insights sufficient to capture the causes of ResNet's performance."
https://openreview.net/forum?id=SJDaqqveg,"['The paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing.'
 'This paper introduces an actor-critic approach for sequence prediction and shows experiments on spelling correction and machine translation.'
 'Paper presents application of actor-critic method for conditional sequence prediction. Critic is trained conditional to target sequence output, while actor is conditional on input sequence.']","['The paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing.'
 'This paper introduces an actor-critic approach for sequence prediction and shows experiments on spelling correction and machine translation.'
 'Paper presents application of actor-critic method for conditional sequence prediction. Critic is trained conditional to target sequence output, while actor is conditional on input sequence.']","The paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing.                                                                       0.735142
This paper introduces an actor-critic approach for sequence prediction and shows experiments on spelling correction and machine translation.                                                     0.597220
Paper presents application of actor-critic method for conditional sequence prediction. Critic is trained conditional to target sequence output, while actor is conditional on input sequence.    0.773012
dtype: float32","The paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing.                                                                       1.098612
This paper introduces an actor-critic approach for sequence prediction and shows experiments on spelling correction and machine translation.                                                     1.098612
Paper presents application of actor-critic method for conditional sequence prediction. Critic is trained conditional to target sequence output, while actor is conditional on input sequence.    1.098612
dtype: float32","{""The paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing."":{""This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. --------In particular, experiments are shown in a synthetic denoising task as well as in machine translation. ----------------I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different?--------If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I'd like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline.----------------The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. ----------------While the idea makes sense, the"":-5.4419984818,""This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation.  While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs.  Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens.----------------The motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified. There are a number of details that are necessary for successful training, that are discussed well.  While the full Actor-Critic model does not show strong improvements over REINFORCE with critic"":-8.5045337677,""The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. ----------------The interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be"":-8.4063549042},""This paper introduces an actor-critic approach for sequence prediction and shows experiments on spelling correction and machine translation."":{""This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. --------In particular, experiments are shown in a synthetic denoising task as well as in machine translation. ----------------I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different?--------If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I'd like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline.----------------The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. ----------------While the idea makes sense, the"":-8.8866205215,""This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation.  While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs.  Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens.----------------The motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified. There are a number of details that are necessary for successful training, that are discussed well.  While the full Actor-Critic model does not show strong improvements over REINFORCE with critic"":-6.4164776802,""The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. ----------------The interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be"":-8.9681558609},""Paper presents application of actor-critic method for conditional sequence prediction. Critic is trained conditional to target sequence output, while actor is conditional on input sequence."":{""This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. --------In particular, experiments are shown in a synthetic denoising task as well as in machine translation. ----------------I like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different?--------If one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I'd like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline.----------------The authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. ----------------While the idea makes sense, the"":-4.6712498665,""This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation.  While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs.  Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens.----------------The motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified. There are a number of details that are necessary for successful training, that are discussed well.  While the full Actor-Critic model does not show strong improvements over REINFORCE with critic"":-3.3723316193,""The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. ----------------The interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be"":-0.712718308}}","Originality, Significance:    The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks.   A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.    Quality, Clarity:    The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal.     I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work."
https://openreview.net/forum?id=SJGPL9Dex,"['This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach'
 'LISTA originally proposes to accelerate sparse coding algorithms. The authors here propose a solid analysis of the acceleration performance of LISTA.'
 'This paper proposes a method for neural sparse coding inspired by LISTA. FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms']","['This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach'
 'LISTA originally proposes to accelerate sparse coding algorithms. The authors here propose a solid analysis of the acceleration performance of LISTA.'
 'This paper proposes a method for neural sparse coding inspired by LISTA. FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms']","This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach    0.858979
LISTA originally proposes to accelerate sparse coding algorithms. The authors here propose a solid analysis of the acceleration performance of LISTA.                                                                                                                  0.830297
This paper proposes a method for neural sparse coding inspired by LISTA. FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms                     0.678870
dtype: float32","This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA/FISTA iterations. Based on the results, the authors proposed a reparametrization approach    1.098612
LISTA originally proposes to accelerate sparse coding algorithms. The authors here propose a solid analysis of the acceleration performance of LISTA.                                                                                                                  1.098612
This paper proposes a method for neural sparse coding inspired by LISTA. FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms                     1.098612
dtype: float32","{""This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA\/FISTA iterations. Based on the results, the authors proposed a reparametrization approach"":{""This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA\/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below.----------------It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. ----------------Minor comments:----------------- E(z_k) in (3) and (4) are not defined.----------------- E_x in (19) is not defined.----------------- Forward referencing (\u201cEquation ("":-0.7697638869,""This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The authors here propose a solid analysis of the acceleration performance of LISTA, using a specific matrix factorisation of the dictionary. ----------------The analysis is well structured, and provides interesting insights. It would have been good to tie more closely these insights to specific properties of data or input distributions.----------------The learned dictionary results in Section 3.3 are not very clear: is the dictionary learned with a sort of alternating minimisation strategy that would include LISTA as sparse coding step? Or is it only the sparse coding that is studied, with a dictionary that has been learned a priori?----------------Overall, the paper does not propose a new algorithm and representation, but provides key insights on a well-known and interesting acceleration method on sparse coding. This is quite a nice work. The title seems however a bit confusing as 'neural sparse coding'"":-4.1694273949,""This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).----------------FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.----------------Overall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question\/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. ----------------Minor comments\/typos:--------- p. 6: \""memory taps\"" -> tapes?--------- sec 3."":-4.6047444344},""LISTA originally proposes to accelerate sparse coding algorithms. The authors here propose a solid analysis of the acceleration performance of LISTA."":{""This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA\/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below.----------------It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. ----------------Minor comments:----------------- E(z_k) in (3) and (4) are not defined.----------------- E_x in (19) is not defined.----------------- Forward referencing (\u201cEquation ("":-13.1749353409,""This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The authors here propose a solid analysis of the acceleration performance of LISTA, using a specific matrix factorisation of the dictionary. ----------------The analysis is well structured, and provides interesting insights. It would have been good to tie more closely these insights to specific properties of data or input distributions.----------------The learned dictionary results in Section 3.3 are not very clear: is the dictionary learned with a sort of alternating minimisation strategy that would include LISTA as sparse coding step? Or is it only the sparse coding that is studied, with a dictionary that has been learned a priori?----------------Overall, the paper does not propose a new algorithm and representation, but provides key insights on a well-known and interesting acceleration method on sparse coding. This is quite a nice work. The title seems however a bit confusing as 'neural sparse coding'"":-9.705997467,""This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).----------------FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.----------------Overall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question\/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. ----------------Minor comments\/typos:--------- p. 6: \""memory taps\"" -> tapes?--------- sec 3."":-13.1292667389},""This paper proposes a method for neural sparse coding inspired by LISTA. FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms"":{""This paper performs theoretical analysis to understand how sparse coding could be accelerated by neural networks. The neural networks are generated by unfolding the ISTA\/FISTA iterations. Based on the results, the authors proposed a reparametrization approach for the neural network architecture to enforce the factorization property and recovered the original gain of LISTA, which justified the theoretical analysis. My comments are listed below.----------------It is not clear about the purpose of Section 2.3.2. Adapting the factorization to the input distribution based on (15) would be time consuming because the overhead of solving (15) may not save the total time. In fact, the approach does not use (15) but back propagation to learn the factorization parameters. ----------------Minor comments:----------------- E(z_k) in (3) and (4) are not defined.----------------- E_x in (19) is not defined.----------------- Forward referencing (\u201cEquation ("":-3.9148275852,""This work presents an analysis of LISTA, which originally proposes to accelerate sparse coding algorithms with some prior on the structure of the problem. The authors here propose a solid analysis of the acceleration performance of LISTA, using a specific matrix factorisation of the dictionary. ----------------The analysis is well structured, and provides interesting insights. It would have been good to tie more closely these insights to specific properties of data or input distributions.----------------The learned dictionary results in Section 3.3 are not very clear: is the dictionary learned with a sort of alternating minimisation strategy that would include LISTA as sparse coding step? Or is it only the sparse coding that is studied, with a dictionary that has been learned a priori?----------------Overall, the paper does not propose a new algorithm and representation, but provides key insights on a well-known and interesting acceleration method on sparse coding. This is quite a nice work. The title seems however a bit confusing as 'neural sparse coding'"":-3.8121461868,""This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010). A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LISTA (via Theorem 2.2. and Corollary 2.3).----------------FacNet is a specialization of LISTA, sharing the same network architecture but with additional constraints on the parameters. In numerical experiments, LISTA outperforms FacNet, up to some optimization errors. It is not clear what is the advantage of using FacNet instead of LISTA.----------------Overall, the paper lacks clarity in several parts. It would be good to state beforehand what the main contribution is. As stated in the clarification question\/answer below, this paper would benefit from a more clear explanation about the connection of FacNet with LISTA. ----------------Minor comments\/typos:--------- p. 6: \""memory taps\"" -> tapes?--------- sec 3."":-1.0677047968}}","The work is fairly unique in that it provides a theoretical explanation for an empirical phenomenon in the world of sparse coding. The reviewers were overall favourable, although some reviewers thought parts of the paper were unclear or had confusion about the relationship to LISTA. I suspect the analysis here could also shed light on other problems."
https://openreview.net/forum?id=SJNDWNOlg,"['The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. It does not propose or compare to end-to-end learning frameworks.'
 'Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters.'
 'This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification.']","['The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. It does not propose or compare to end-to-end learning frameworks.'
 'Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters.'
 'This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification.']","The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. It does not propose or compare to end-to-end learning frameworks.                            0.674491
Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters.                                                         0.840200
This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification.    0.817564
dtype: float32","The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. It does not propose or compare to end-to-end learning frameworks.                            1.098612
Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters.                                                         1.098612
This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification.    1.098612
dtype: float32","{""The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. It does not propose or compare to end-to-end learning frameworks."":{""The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.----------------Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.----------------While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \""End-to-end Learning of Deep Visual Representations for Image Retrieval\""). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact,"":-1.8292019367,""Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:----------------I don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \""as large as possible\"" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).----------------The setting"":-4.614420414,""This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \""traditional wisdom\"".----------------Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. ----------------While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \""ignores\"" 2 major recent works that"":-4.60181427},""Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters."":{""The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.----------------Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.----------------While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \""End-to-end Learning of Deep Visual Representations for Image Retrieval\""). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact,"":-8.4672555923,""Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:----------------I don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \""as large as possible\"" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).----------------The setting"":-4.8240065575,""This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \""traditional wisdom\"".----------------Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. ----------------While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \""ignores\"" 2 major recent works that"":-8.1896095276},""This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification."":{""The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks.----------------Technically, the contribution is clear, particularly with the promised clarifications on how multiple scales are handled in the representation. However, I am still not entirely clear whether there would be a difference in the multi-scale settting for full and cropped queries.----------------While the paper focuses on comparing different baseline architectures for CNN-based image retrieval, several recent papers have proposed to learn end-to-end representations specific for this task, with very good result (see for instance the recent work by Gordo et al. \""End-to-end Learning of Deep Visual Representations for Image Retrieval\""). The authors clarify that their work is orthogonal to papers such as Gordo et al. as they assess instead the performance of networks pre-trained from image classification. In fact,"":-3.9112145901,""Authors investigate how to use pretrained CNNs for retrieval and perform an extensive evaluation of the influence of various parameters. For detailed comments on everything see the questions I posted earlier. The summary is here:----------------I don't think we learn much from this paper: we already knew that we should use the last conv layer, we knew we should use PCA with whitening, we knew we should use original size images (authors say Tolias didn't do this as they resized the images, but they did this exactly for the same reason as authors didn't evaluate on Holidays - the images are too big. So they basically used \""as large as possible\"" image sizes, which is what this paper effectively suggests as well), etc. This paper essentially concatenates methods that people have already used, and performs some more parameter tweaking to achieve the state-of-the-art (while the tweaking is actually performed on the test set of some of the tests).----------------The setting"":-4.4373240471,""This paper explores different strategies for instance-level image retrieval with deep CNNs. The approach consists of extracting features from a network pre-trained for image classification (e.g. VGG), and post-process them for image retrieval. In other words, the network is off-the-shelf and solely acts as a feature extractor. The post-processing strategies are borrowed from traditional retrieval pipelines relying on hand-crafted features (e.g. SIFT + Fisher Vectors), denoted by the authors as \""traditional wisdom\"".----------------Specifically, the authors examine where to extract features in the network (i.e. features are neurons activations of a convolution layer), which type of feature aggregation and normalization performs best, whether resizing images helps, whether combining multiple scales helps, and so on. ----------------While this type of experimental study is reasonable and well motivated, it suffers from a huge problem. Namely it \""ignores\"" 2 major recent works that"":-0.769128859}}","The paper conducts a detailed evaluation of different CNN architectures applied to visual instance retrieval. The authors consider various deep neural network architectures, with a focus on architectures pre-trained for image classification.     An important concern of the reviewers is the relevance of the evaluation given the recent impressive experimental results of deep neural networks trained end-to-end for visual instance retrieval by Gordo et al. ""End-to-end Learning of Deep Visual Representations for Image Retrieval"". Another concern is the novelty of the proposed evaluation given the evaluation of the performance for visual instance retrieval of deep neural network pre-trained for image classification performed in Paulin et al. ""Convolutional Patch Representations for Image Retrieval: An Unsupervised Approach"".     A revision of the paper, following the reviewers' suggestions, will generate a stronger submission to a future venue."
https://openreview.net/forum?id=SJQNqLFgl,"['The paper formulates a number of rules for designing convolutional neural network architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100,'
 ' in convolutional neural network design. The writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. The experimental portion of this paper feels scattered with many different approaches being presented'
 'The authors take on the task of figuring out a set of design patterns for current deep architectures. I am not too sure how the choice of these 14 patterns was made. The extensions proposed in Section 4 seem a bit off tune.']","['The paper formulates a number of rules for designing convolutional neural network architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100,'
 ' in convolutional neural network design. The writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. The experimental portion of this paper feels scattered with many different approaches being presented'
 'The authors take on the task of figuring out a set of design patterns for current deep architectures. I am not too sure how the choice of these 14 patterns was made. The extensions proposed in Section 4 seem a bit off tune.']","The paper formulates a number of rules for designing convolutional neural network architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100,                                 0.612556
 in convolutional neural network design. The writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. The experimental portion of this paper feels scattered with many different approaches being presented    0.405267
The authors take on the task of figuring out a set of design patterns for current deep architectures. I am not too sure how the choice of these 14 patterns was made. The extensions proposed in Section 4 seem a bit off tune.                                        0.493626
dtype: float32","The paper formulates a number of rules for designing convolutional neural network architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100,                                 1.098612
 in convolutional neural network design. The writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. The experimental portion of this paper feels scattered with many different approaches being presented    1.098611
The authors take on the task of figuring out a set of design patterns for current deep architectures. I am not too sure how the choice of these 14 patterns was made. The extensions proposed in Section 4 seem a bit off tune.                                        1.098612
dtype: float32","{""The paper formulates a number of rules for designing convolutional neural network architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100,"":{""The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.----------------I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an \""introdu"":-0.7764513493,""The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.----------------The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.----------------Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, \""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality\"" is presented as one of 14 core design principles without any further justification. Similarly \"""":-2.8485782146,""The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. ----------------There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the \""community service\"" aspect of helping someone who starts figure out the \""coordinate system\"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.----------------However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. ----------------Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU,"":-4.0123066902},"" in convolutional neural network design. The writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. The experimental portion of this paper feels scattered with many different approaches being presented"":{""The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.----------------I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an \""introdu"":-5.5988621712,""The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.----------------The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.----------------Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, \""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality\"" is presented as one of 14 core design principles without any further justification. Similarly \"""":-3.8511579037,""The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. ----------------There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the \""community service\"" aspect of helping someone who starts figure out the \""coordinate system\"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.----------------However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. ----------------Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU,"":-5.9480862617},""The authors take on the task of figuring out a set of design patterns for current deep architectures. I am not too sure how the choice of these 14 patterns was made. The extensions proposed in Section 4 seem a bit off tune."":{""The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. Essentially, it reads like a review paper about modern CNN architectures. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFAR-100, but seem to achieve relatively poor performance on these datasets (Table 1), so their merit is unclear to me.----------------I'm not sure if such a collection of rules extracted from prior work warrants publication as a research paper. It is not a bad idea to try and summarise some of these observations now that CNNs have been the model of choice for computer vision tasks for a few years, and such a summary could be useful for newcomers. However, a lot of it seems to boil down to common sense (e.g. #1, #3, #7, #11). The rest of it might be more suited for an \""introdu"":-4.5819411278,""The authors have grouped recent work in convolutional neural network design (specifically with respect to image classification) to identify core design principles guiding the field at large. The 14 principles they produce (along with associated references) include a number of useful and correct observations that would be an asset to anyone unfamiliar with the field. The authors explore a number of architectures on CIFAR-10 and CIFAR-100 guided by these principles.----------------The authors have collected a quality set of references on the subject and grouped them well which is valuable for young researchers. Clearly the authors explored a many of architectural changes as part of their experiments and publicly available code base is always nice.----------------Overall the writing seems to jump around a bit and the motivations behind some design principles feel lost in the confusion. For example, \""Design Pattern 4: Increase Symmetry argues for architectural symmetry as a sign of beauty and quality\"" is presented as one of 14 core design principles without any further justification. Similarly \"""":-4.3335642815,""The authors take on the task of figuring out a set of design patterns for current deep architectures - namely themes that are recurring in the literature.  If one may say so, a distributed representation of deep architectures. ----------------There are two aspects of the paper that I particularly valued: firstly, the excellent review of recent works, which made me realize how many things I have been missing myself.  Secondly, the \""community service\"" aspect of helping someone who starts figure out the \""coordinate system\"" for deep architectures - this could potentially be more important than introducing yet-another trick of the trade, as most other submissions may do.----------------However I think this work is still half-done, and even though working on this project is a great idea, the authors do not yet do it properly. ----------------Firstly, I am not too sure how the choice of these 14 patterns was made. Maxout for instance (pattern 14) is one of the many nonlinearities (PreLU,"":-2.2718756199}}",The authors agree with the reviewers that this manuscript is not yet ready.
https://openreview.net/forum?id=SJRpRfKxx,"['The authors formulate a recurrent deep neural network to predict human fixation locations in videos.'
 'This work proposes to a spatiotemporal saliency network that can mimic human fixation patterns.'
 'This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in L']","['The authors formulate a recurrent deep neural network to predict human fixation locations in videos.'
 'This work proposes to a spatiotemporal saliency network that can mimic human fixation patterns.'
 'This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in L']","The authors formulate a recurrent deep neural network to predict human fixation locations in videos.                                                                                                                                           0.878179
This work proposes to a spatiotemporal saliency network that can mimic human fixation patterns.                                                                                                                                                0.720567
This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in L    0.843376
dtype: float32","The authors formulate a recurrent deep neural network to predict human fixation locations in videos.                                                                                                                                           1.098612
This work proposes to a spatiotemporal saliency network that can mimic human fixation patterns.                                                                                                                                                1.098612
This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in L    1.098612
dtype: float32","{""The authors formulate a recurrent deep neural network to predict human fixation locations in videos."":{""The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.----------------quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?----------------You claim that your \""results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported"":-23.1270656586,""This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,--------thus helping to prune irrelevant information from the video and improve action recognition.----------------The work is interesting and has shown state-of-the-art results on predicting human attention on action videos.--------It has also shown promise for helping action clip classification.----------------The paper would benefit from a discussion on the role of context in attention.--------For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?----------------One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.--------The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.----------------Is there a way to combine the context and attention without concatenation?--------The"":-26.5385913849,""This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.----------------Overall, the idea in this paper is reasonable and the paper is well written. RNN\/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN\/LSTM for problems where the output is continuous like in this paper.----------------The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e"":-27.2256813049},""This work proposes to a spatiotemporal saliency network that can mimic human fixation patterns."":{""The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.----------------quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?----------------You claim that your \""results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported"":-21.7220745087,""This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,--------thus helping to prune irrelevant information from the video and improve action recognition.----------------The work is interesting and has shown state-of-the-art results on predicting human attention on action videos.--------It has also shown promise for helping action clip classification.----------------The paper would benefit from a discussion on the role of context in attention.--------For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?----------------One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.--------The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.----------------Is there a way to combine the context and attention without concatenation?--------The"":-19.1919116974,""This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.----------------Overall, the idea in this paper is reasonable and the paper is well written. RNN\/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN\/LSTM for problems where the output is continuous like in this paper.----------------The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e"":-22.7262306213},""This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in L"":{""The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.----------------quality: I am missing a more thorough evaluation of the fixation prediction performance. The center bias performance in Table 1 differs significantly from the on in Table 2. All the state-of-the-art models reported in Table 2 have a performance worse than the center bias performance reported in Table 1. Is there really no other model better than the center bias? Additionally I am missing details on how central bias and human performance are modelled. Is human performance cross-validated?----------------You claim that your \""results are very close to human performance (the difference is only 3.2%). This difference is actually larger than the difference between Central Bias and your model reported"":-4.4336047173,""This work proposes to a spatiotemporal saliency network that is able to mimic human fixation patterns,--------thus helping to prune irrelevant information from the video and improve action recognition.----------------The work is interesting and has shown state-of-the-art results on predicting human attention on action videos.--------It has also shown promise for helping action clip classification.----------------The paper would benefit from a discussion on the role of context in attention.--------For instance, if context is important, and people give attention to context, why is it not incorporated automatically in your model?----------------One weak point is the action recognition section, where the comparison between the two (1)(2) and (3) seems unfair.--------The attention weighted feature maps in fact reduce the classification performance, and only improve performance when doubling the feature and associated model complexity by concatenating the weighted maps with the original features.----------------Is there a way to combine the context and attention without concatenation?--------The"":-4.1719055176,""This paper proposes a new method for estimating visual attention in videos. The input clip is first processed by a convnet (in particular, C3D) to extract visual features. The visual features are then passed to LSTM. The hidden state at each time step in LSTM is used to generate the parameters in a Gaussian mixture model. Finally, the visual attention map is generated from the Gaussian mixture model.----------------Overall, the idea in this paper is reasonable and the paper is well written. RNN\/LSTM has been used in lots of vision problem where the outputs are discrete sequences, there has not been much work on using RNN\/LSTM for problems where the output is continuous like in this paper.----------------The experimental results have demonstrated the effectiveness of the proposed approach. In particular, it outperforms other state-of-the-art on the saliency prediction task on the Hollywood2 datasets. It also shows improvement over baselines (e"":-0.7821026444}}",The paper describes a model for video saliency prediction using a combination of spatio-temporal ConvNet features and LSTM. The proposed method outperforms the state of the art on the saliency prediction task and is shown to improve the performance of a baseline action classification model.
https://openreview.net/forum?id=SJU4ayYgl,"[' for graph node prediction.'
 'The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset.'
 'This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines. Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is']","[' for graph node prediction.'
 'The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset.'
 'This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines. Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is']"," for graph node prediction.                                                                                                                                                                                                            0.464388
The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset.              0.655806
This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines. Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is    0.243762
dtype: float32"," for graph node prediction.                                                                                                                                                                                                            1.098612
The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset.              1.098612
This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines. Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is    1.098575
dtype: float32","{"" for graph node prediction."":{""The paper develops a simple and reasonable algorithm for graph node prediction\/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. ----------------Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  "":-82.1548461914,""The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.----------------The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and"":-84.1289749146,""This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.----------------This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.----------------It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2"":-84.3746337891},""The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset."":{""The paper develops a simple and reasonable algorithm for graph node prediction\/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. ----------------Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  "":-5.449403286,""The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.----------------The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and"":-2.6155693531,""This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.----------------This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.----------------It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2"":-5.2127509117},""This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines. Compared to Duvenaud et al. 2015 and Li et al. 2016, the proposed method is"":{""The paper develops a simple and reasonable algorithm for graph node prediction\/classification. The formulations are very intuitive and lead to a simple CNN based training and can easily leverage existing GPU speedups. ----------------Experiments are thorough and compare with many reasonable baselines on large and real benchmark datasets. Although, I am not quite aware of the literature on other methods and there may be similar alternatives as link and node prediction is an old problem. I still think the approach is quite simple and reasonably supported by good evaluations.  "":-3.9672014713,""The paper introduces a method for semi-supervised learning in graphs that exploits the spectral structure of the graph in a convolutional NN implementation. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.----------------The paper is technically fine and clear, the algorithm seems to scale well, and the results on the different datasets compare very favorably with the different baselines. The algorithm is simple and training seems easy. Concerning the originality, the proposed algorithm is a simple adaptation of graph convolutional networks (ref Defferrard 2016 in the paper) to a semi-supervised transductive setting. This is clearly mentioned in the paper, but the authors could better highlight the differences and novelty wrt this reference paper. Also, there is no comparison with the family of iterative classifiers, which usually compare favorably, both in performance and"":-3.3851428032,""This paper proposes the graph convolutional networks, motivated from approximating graph convolutions.  In one propagation step, what the model does can be simplified as, first linearly transform the node representations for each node, and then multiply the transformed node representations with the normalized affinity matrix (with self-connections added), and then pass through nonlinearity.----------------This model is used for semi-supervised learning on graphs, and in the experiments it demonstrated quite impressive results compared to other baselines, outperforming them by a significant margin.  The evaluation of propagation model is also interesting, where different variants of the model and design decisions are evaluated and compared.----------------It is surprising that such a simple model works so much better than all the baselines.  Considering that the model used is just a two-layer model in most experiments, this is really surprising as a two-layer model is very local, and the output of a node can only be affected by nodes in a 2"":-2.2508819103}}",The reviewers are in agreement that this paper is well written and constitutes a solid contribution to graph-based semi-supervised learning based on variants of CNNs.
https://openreview.net/forum?id=SJqaCVLxx,"['The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network.'
 ""This paper is very difficult to understand. Much work is needed on the paper's writing before it can be understood well enough.""
 'The paper is still extremely poorly written and presented. The frequent spelling mistakes and incoherent sentences make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.']","['The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network.'
 ""This paper is very difficult to understand. Much work is needed on the paper's writing before it can be understood well enough.""
 'The paper is still extremely poorly written and presented. The frequent spelling mistakes and incoherent sentences make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.']","The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network.                                                                                                                                                                    0.647073
This paper is very difficult to understand. Much work is needed on the paper's writing before it can be understood well enough.                                                                                                                                                 0.019407
The paper is still extremely poorly written and presented. The frequent spelling mistakes and incoherent sentences make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.    0.766190
dtype: float32","The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network.                                                                                                                                                                    1.098559
This paper is very difficult to understand. Much work is needed on the paper's writing before it can be understood well enough.                                                                                                                                                 1.097755
The paper is still extremely poorly written and presented. The frequent spelling mistakes and incoherent sentences make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills.    1.098612
dtype: float32","{""The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network."":{""The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. ----------------Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages\/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? ----------------In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than"":-13.574133873,""Unfortunately, this paper is very difficult to understand.  The current version of this paper seems improved compared to the initial version, but still far from a finished level.  I'd encourage the authors to keep editing over the language and presentation.----------------I also think it would be good to also try answering some of the following questions very clearly in the paper:----------------- What is the advantage, if any, of the proposed algorithm over SGD?  What is the motivation and goal of the work beyond MNIST benchmarking?----------------- Why are few training examples used?  Is this a scenario in which the system might have an advantage?----------------- Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network.----------------- Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms.----------------- A single experiment on MNIST is too small to"":-15.7093601227,""The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills."":-17.0584182739},""This paper is very difficult to understand. Much work is needed on the paper's writing before it can be understood well enough."":{""The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. ----------------Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages\/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? ----------------In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than"":-11.1960802078,""Unfortunately, this paper is very difficult to understand.  The current version of this paper seems improved compared to the initial version, but still far from a finished level.  I'd encourage the authors to keep editing over the language and presentation.----------------I also think it would be good to also try answering some of the following questions very clearly in the paper:----------------- What is the advantage, if any, of the proposed algorithm over SGD?  What is the motivation and goal of the work beyond MNIST benchmarking?----------------- Why are few training examples used?  Is this a scenario in which the system might have an advantage?----------------- Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network.----------------- Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms.----------------- A single experiment on MNIST is too small to"":-10.7121019363,""The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills."":-10.966255188},""The paper is still extremely poorly written and presented. The frequent spelling mistakes and incoherent sentences make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills."":{""The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. ----------------Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is. Neither do the authors ever motivate why do such a training as opposed to the standard back-prop. What are its advantages\/dis-advantages? Furthermore the experimental section is equally unclear. The authors seem to have merged the training and validation set of the MNIST dataset and use only a subset of it. It is not clear why is that the case and what subset they use. In addition, to the best of my understanding, the results reported are RMSE as opposed to classification error. Why is that the case? ----------------In short, the paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than"":-3.8809301853,""Unfortunately, this paper is very difficult to understand.  The current version of this paper seems improved compared to the initial version, but still far from a finished level.  I'd encourage the authors to keep editing over the language and presentation.----------------I also think it would be good to also try answering some of the following questions very clearly in the paper:----------------- What is the advantage, if any, of the proposed algorithm over SGD?  What is the motivation and goal of the work beyond MNIST benchmarking?----------------- Why are few training examples used?  Is this a scenario in which the system might have an advantage?----------------- Concretely describe the genetic algorithms terminology used in the algorithm descriptions, and what each term means in the context of the convolutional network.----------------- Try to make sure that the method, as described, can be understood by a reader without much prior background on genetic algorithms.----------------- A single experiment on MNIST is too small to"":-3.4698369503,""The paper is still extremely poorly written and presented despite multiple reviewers asking to address that issue. The frequent spelling mistakes and incoherent sentences and unclear presentation make reading and understanding the paper very difficult and time consuming. Consider getting help from someone with good english and presentation skills."":-0.518281579}}","This paper is unfortunately quite unclear and unreadable and nowhere near ready for any conference.  I would advise the authors to 1) restructure their paper to present first some type of context and identify a problem that they are trying to solve, 2) explain what novel method they propose to solve the identified problem and why this method is promising and how it relates to existing methods, 3) explain what their experiments are trying to do and what the results of the experiments are, 4) enlist someone fluent in English to help with writing and re-reading.  A way to do this is to find a set of well-cited papers in the same domain with similar ideas and see how they are structured, then try to follow similar outlines."
https://openreview.net/forum?id=SJx7Jrtgl,"['This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario.'
 'The authors posit a mixture of Gaussian prior for variational auto-encoders. They also consider a regularization term motivated from information theory. The experiments are limited on toy data and only a few mixture components are considered.'
 '. The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. They perform experiments on a synthetic dataset,']","['This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario.'
 'The authors posit a mixture of Gaussian prior for variational auto-encoders. They also consider a regularization term motivated from information theory. The experiments are limited on toy data and only a few mixture components are considered.'
 '. The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. They perform experiments on a synthetic dataset,']","This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario.                                                                                                                             0.654044
The authors posit a mixture of Gaussian prior for variational auto-encoders. They also consider a regularization term motivated from information theory. The experiments are limited on toy data and only a few mixture components are considered.    0.493699
. The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. They perform experiments on a synthetic dataset,    0.787690
dtype: float32","This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario.                                                                                                                             1.098612
The authors posit a mixture of Gaussian prior for variational auto-encoders. They also consider a regularization term motivated from information theory. The experiments are limited on toy data and only a few mixture components are considered.    1.098612
. The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. They perform experiments on a synthetic dataset,    1.098612
dtype: float32","{""This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario."":{""This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). ----------------A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. ----------------Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the"":-11.3064260483,""The authors posit a mixture of Gaussian prior for variational--------auto-encoders. They also consider a regularization term motivated--------from information theory.----------------The modeling extension is simple and the inference follows--------mechanically from what's already standard in the literature. Instead--------of using discrete latent variable samples they collapse the expected--------KL; this works for few mixture components and has been considered--------before in more general contexts, e.g., Titsias and Lazaro-Gredilla--------(2015). It will not scale to many mixture components.----------------I find the discussion in Section 3.2.2 difficult to parse and, if I--------understood it correctly, not necessarily correct. Many arguments are--------introduced and few fleshed out. First, there is a claim that a--------multinomial prior with equal class probabilities assigns the same--------number of data points to each class on average; this is true a priori--------but certainly not"":-14.4335584641,""The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.--------The use of a mixture of VAE is an incremental idea if novel.--------I would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.--------The results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the \""Best Run\"". Was the best run maximizing the validation performance or"":-13.6732225418},""The authors posit a mixture of Gaussian prior for variational auto-encoders. They also consider a regularization term motivated from information theory. The experiments are limited on toy data and only a few mixture components are considered."":{""This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). ----------------A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. ----------------Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the"":-4.2047748566,""The authors posit a mixture of Gaussian prior for variational--------auto-encoders. They also consider a regularization term motivated--------from information theory.----------------The modeling extension is simple and the inference follows--------mechanically from what's already standard in the literature. Instead--------of using discrete latent variable samples they collapse the expected--------KL; this works for few mixture components and has been considered--------before in more general contexts, e.g., Titsias and Lazaro-Gredilla--------(2015). It will not scale to many mixture components.----------------I find the discussion in Section 3.2.2 difficult to parse and, if I--------understood it correctly, not necessarily correct. Many arguments are--------introduced and few fleshed out. First, there is a claim that a--------multinomial prior with equal class probabilities assigns the same--------number of data points to each class on average; this is true a priori--------but certainly not"":-1.9579409361,""The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.--------The use of a mixture of VAE is an incremental idea if novel.--------I would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.--------The results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the \""Best Run\"". Was the best run maximizing the validation performance or"":-4.0783023834},"". The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. They perform experiments on a synthetic dataset,"":{""This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term). ----------------A general positive point about this paper is that the model construction is kept simple. Indeed, the assumption about the mixture prior is simple but reasonable, and the inference follows the VAE framework where appropriate with only changing parts that do not conform with the clustering task. These changes are also motivated by some analysis. The presentation is also kept simple: the linking to VAE and related methods is made in an clear and honest way, so that it's easy to follow the paper and understand how everything fits together. ----------------Also, the regularization term is a well motivated and reasonable addition. Given the VAE context in this paper, I'd be interested in seeing a discussion on the variance of the"":-4.1614847183,""The authors posit a mixture of Gaussian prior for variational--------auto-encoders. They also consider a regularization term motivated--------from information theory.----------------The modeling extension is simple and the inference follows--------mechanically from what's already standard in the literature. Instead--------of using discrete latent variable samples they collapse the expected--------KL; this works for few mixture components and has been considered--------before in more general contexts, e.g., Titsias and Lazaro-Gredilla--------(2015). It will not scale to many mixture components.----------------I find the discussion in Section 3.2.2 difficult to parse and, if I--------understood it correctly, not necessarily correct. Many arguments are--------introduced and few fleshed out. First, there is a claim that a--------multinomial prior with equal class probabilities assigns the same--------number of data points to each class on average; this is true a priori--------but certainly not"":-4.2156925201,""The authors proposes to a variant of Variational Auto-Encoders using a mixture distribution to enable unsupervised clustering that they combine with an information-theoretical regularization. To demonstrate the merit of such approach, they perform experiments on a synthetic dataset, MNIST and SVHN.--------The use of a mixture of VAE is an incremental idea if novel.--------I would like to see the comparison with the more straightforward use of a mixture of gaussians prior. This model is more complex and I would like to see a justification of this additional complexity.--------The results in Table 1 are questionable. First of all, GMVAE+ seems to outperform other methods with M=1, there should be a run of GMVAE+ with M=10 for proper comparison. But what I find more disturbing in this table is the variance of the results, especially since you are taking the \""Best Run\"". Was the best run maximizing the validation performance or"":-0.9490895271}}","The reviewers have looked through both the responses, updates, and had much discussion. We agree that the paper is well executed and exposes ideas that are of value and interest. At the same same, the extent to which the methods can be applied in practice and scaled to different types of problems remains unclear, especially in high-dimensional settings. For this reason, we feel that the paper is not yet ready for acceptance in this years conference."
https://openreview.net/forum?id=Sk2iistgg,"['The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace.'
 'The paper considers an alternate formulation of Kernel PCA.'
 'Paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The paper contains errors and the experimental evaluation is not convincing.']","['The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace.'
 'The paper considers an alternate formulation of Kernel PCA.'
 'Paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The paper contains errors and the experimental evaluation is not convincing.']","The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace.    0.891687
The paper considers an alternate formulation of Kernel PCA.                                                                                                                                                              0.959204
Paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The paper contains errors and the experimental evaluation is not convincing.                0.847193
dtype: float32","The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace.    1.098612
The paper considers an alternate formulation of Kernel PCA.                                                                                                                                                              1.098612
Paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The paper contains errors and the experimental evaluation is not convincing.                1.098612
dtype: float32","{""The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace."":{""The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. ----------------The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b\/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first"":-0.6248281598,""This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating \u201ccausal factors\u201d, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  ----------------- Not sure what the authors mean by \u201ccausal factors\u201d. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition\/discussion.----------------- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each----------------- Authors outline two key disadvantages of the existing KPCA approach"":-4.3607420921,""This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. --------The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. ----------------The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. --------The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.----------------The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.------------------------The paper contains errors:----------------- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the"":-4.4864006042},""The paper considers an alternate formulation of Kernel PCA."":{""The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. ----------------The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b\/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first"":-38.4208335876,""This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating \u201ccausal factors\u201d, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  ----------------- Not sure what the authors mean by \u201ccausal factors\u201d. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition\/discussion.----------------- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each----------------- Authors outline two key disadvantages of the existing KPCA approach"":-33.5362968445,""This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. --------The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. ----------------The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. --------The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.----------------The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.------------------------The paper contains errors:----------------- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the"":-37.4465866089},""Paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The paper contains errors and the experimental evaluation is not convincing."":{""The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. The latent variables (or causal factors) corresponding to the observed data are assumed to lie near a low dimensional subspace in an RKHS induced by a predetermined kernel. The proposed regularizer can be seen as an extension of the linear low-rank assumption on the latent factors. A nuclear norm penalty on the Cholesky factor of the kernel matrix is used as a relaxation for the dimensionality of the subspace. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections -- and the proposed method is shown to outperform linear low-rank regularizer. ----------------The clarity of the paper has scope for improvement (particularly, Introduction) - the back and forth b\/w dimensionality reduction techniques and inverse problems is confusing at times. Clearly defining the ill-posed inverse problem first"":-7.3579235077,""This paper considers an alternate formulation of Kernel PCA with rank constraints incorporated as a regularization term in the objective. The writing is not clear. The focus keeps shifting from estimating \u201ccausal factors\u201d, to nonlinear dimensionality reduction to Kernel PCA to ill-posed inverse problems. The problem reformulation of Kernel PCA uses somewhat standard tricks and it is not clear what are the advantages of the proposed approach over the existing methods as there is no theoretical analysis of the overall approach or empirical comparison with existing state-of-the-art.  ----------------- Not sure what the authors mean by \u201ccausal factors\u201d. There is a reference to it in Abstract and in Problem formulation on page 3 without any definition\/discussion.----------------- In KPCA, I am not sure why one is interested in step (iii) outlined on page 2 of finding a pre-image for each----------------- Authors outline two key disadvantages of the existing KPCA approach"":-6.9498581886,""This paper presents an approach to non-linear kernel dimensionality reduction with a trace norm regularizer in the feature space. The authors proposed an iterative minimization approach in order to obtain a local optimum of a relaxed problem. --------The paper contains errors and the experimental evaluation is not convincing. Only old techniques are compared against in very toy datasets. ----------------The authors claim state-of-the-art, however, the oil dataset is not a real benchmark, and the comparisons are to very old approaches. --------The experimental evaluation should demonstrate robustness to more complex noise and outliers, as this was one of the motivations in the introduction.----------------The authors do not address the out-of-sample problem. This is a problem of kernel-based methods vs LVMs, and thus should be address here.------------------------The paper contains errors:----------------- The last paragraph of section 1 says that this paper proposes a closed form solution to robust KPCA. This is simply wrong, as the"":-3.6040537357}}","There is complete consensus among the reviewers that the KPCA formulation in this paper needs better motivation; that the paper has technical errors, and the experimental evaluation is not convincing. As such the paper is not up to ICLR standards. The authors are encouraged to revise the paper based on feedback from the reviews."
https://openreview.net/forum?id=Sk36NgFeg,"['This paper aims to characterize the perceptual ability of a neural network under different input conditions. It seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.'
 'The paper is interesting, but there are some issues with the analysis. The images in Figure 2 look very blurry.'
 ': Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks?']","['This paper aims to characterize the perceptual ability of a neural network under different input conditions. It seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.'
 'The paper is interesting, but there are some issues with the analysis. The images in Figure 2 look very blurry.'
 ': Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks?']","This paper aims to characterize the perceptual ability of a neural network under different input conditions. It seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.          0.820395
The paper is interesting, but there are some issues with the analysis. The images in Figure 2 look very blurry.                                                                                                          0.082852
: Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks?    0.337094
dtype: float32","This paper aims to characterize the perceptual ability of a neural network under different input conditions. It seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.          1.098612
The paper is interesting, but there are some issues with the analysis. The images in Figure 2 look very blurry.                                                                                                          1.097152
: Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks?    1.098582
dtype: float32","{""This paper aims to characterize the perceptual ability of a neural network under different input conditions. It seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions."":{""This paper aims to characterize the perceptual ability of a neural network under different input conditions.  This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.  MSE and qualitative results are shown and compared for the different input conditions.----------------Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.  For example, at the end of sec 4.4, \""This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....\"".  But this hypothesis is left dangling:  What detailed regions are needed, and from where?  For what sort of tasks?----------------Secondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input,"":-3.1158792973,""I like the idea the paper is exploring. Nevertheless I see some issues with the analysis:----------------- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.--------- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine"":-6.7266120911,""This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. ----------------I think the paper is well motivated. However, there are several concerns:--------1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as https:\/\/arxiv.org\/abs\/1609.04802--------2. Can the experiments based on AE support the idea that artificial neural networks"":-6.324174881},""The paper is interesting, but there are some issues with the analysis. The images in Figure 2 look very blurry."":{""This paper aims to characterize the perceptual ability of a neural network under different input conditions.  This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.  MSE and qualitative results are shown and compared for the different input conditions.----------------Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.  For example, at the end of sec 4.4, \""This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....\"".  But this hypothesis is left dangling:  What detailed regions are needed, and from where?  For what sort of tasks?----------------Secondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input,"":-12.786072731,""I like the idea the paper is exploring. Nevertheless I see some issues with the analysis:----------------- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.--------- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine"":-11.8016653061,""This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. ----------------I think the paper is well motivated. However, there are several concerns:--------1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as https:\/\/arxiv.org\/abs\/1609.04802--------2. Can the experiments based on AE support the idea that artificial neural networks"":-12.4085216522},"": Can the experiments based on AE support the idea that artificial neural networks can perceive an image from low fidelity? AE is only a kind of neural network, can the conclusion extend to other kind of networks?"":{""This paper aims to characterize the perceptual ability of a neural network under different input conditions.  This is done by manipulating the input image x in various ways (e.g. downsamplig, foveating), and training an auto-encoder to reconstruct the original full-resolution image.  MSE and qualitative results are shown and compared for the different input conditions.----------------Unfortunately, this paper seems to lack focus, presenting a set of preliminary inspections with few concrete conclusions.  For example, at the end of sec 4.4, \""This result is not surprising, given that FOV-R contains additional information .... These results suggests that a small number of foveations containing rich details might be all these neural networks need....\"".  But this hypothesis is left dangling:  What detailed regions are needed, and from where?  For what sort of tasks?----------------Secondly, it isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input,"":-4.1893358231,""I like the idea the paper is exploring. Nevertheless I see some issues with the analysis:----------------- To get a better understanding of the quality of the results, I think at least some state-of-the-art comparisons should be included (e.g. by setting d times d pixel patches too their average and applying a denoising autoencoder). If they perform significantly better, then this indicates that the presented model is not yet taking all the information from the input image that could be used.--------- SCT-R and FOV-R are supposed to test how much information can be restored from the Fovea alone as opposed to the Fovea together with low resolution periphery. However, there is an additional difference between the two conditions: According to the paper, in SCT-R, part of the image was set to zero, while in FOV-R it was removed alltogether. With only one or two hidden layers, I could easily imagine"":-4.416531086,""This paper is motivated by the ability that human's visual system can recognize contents of environment by from critical features, and tried to investigate whether neural networks can also have this kind of ability.  Specifically, the paper proposed to use Auto-Encoder (AE) as the network to reconstruct the low fidelity of visual input. Moreover, similar to Mnih et al. (2014),  the paper also proposed to use a recurrent fashion to mimic the sequential behavior the  human visual system. ----------------I think the paper is well motivated. However, there are several concerns:--------1. The baselines of the paper are too weak. Nearest neighbor, bilinear, bicubic and cubic interpolations without any learning procedure are of course performed worse than AE based models. The author should compare with the STOA methods such as https:\/\/arxiv.org\/abs\/1609.04802--------2. Can the experiments based on AE support the idea that artificial neural networks"":-2.5871515274}}","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, reviews are not leaning sufficiently towards acceptance. Reviewers find this direction of exploration to be interesting, but a bit preliminary at the moment. Authors are strongly encouraged to incorporate reviewer comments to make future iterations of the work stronger."
https://openreview.net/forum?id=SkBsEQYll,"['This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. I think the evaluation could be done better.'
 'This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words.'
 'This paper introduces a similarity encoder based on a standard feed-forward neural network. The approach is utilized to generate a simple extension of the CBOW word2vec model.']","['This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. I think the evaluation could be done better.'
 'This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words.'
 'This paper introduces a similarity encoder based on a standard feed-forward neural network. The approach is utilized to generate a simple extension of the CBOW word2vec model.']","This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. I think the evaluation could be done better.        0.754021
This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words.           0.694009
This paper introduces a similarity encoder based on a standard feed-forward neural network. The approach is utilized to generate a simple extension of the CBOW word2vec model.    0.797371
dtype: float32","This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. I think the evaluation could be done better.        1.098612
This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words.           1.098612
This paper introduces a similarity encoder based on a standard feed-forward neural network. The approach is utilized to generate a simple extension of the CBOW word2vec model.    1.098612
dtype: float32","{""This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. I think the evaluation could be done better."":{""This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.----------------Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word\/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \""A Simple Word Embedding Model for Lexical Substitution\"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.----------------In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: --------* http:\/\/veceval.com\/ (Nayak et al., RepEval 2016"":-1.9107755423,""this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. ----------------First, considering the related work [1,2] the proposed approach brings marginal novelty. Especially--------Context Encoders is just a small improvement over word2vec. ----------------Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].----------------[1] http:\/\/papers.nips.cc\/paper\/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions.pdf--------[2] http:\/\/deeplearning.cs.cmu.edu\/pdfs\/OJA.pca.pdf--------[3] http:\/\/www.anthology.aclweb.org\/P\/P10\/P10-1040."":-5.0642943382,""This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.----------------While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.----------------Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation"":-4.9418258667},""This paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words."":{""This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.----------------Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word\/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \""A Simple Word Embedding Model for Lexical Substitution\"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.----------------In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: --------* http:\/\/veceval.com\/ (Nayak et al., RepEval 2016"":-4.2960519791,""this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. ----------------First, considering the related work [1,2] the proposed approach brings marginal novelty. Especially--------Context Encoders is just a small improvement over word2vec. ----------------Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].----------------[1] http:\/\/papers.nips.cc\/paper\/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions.pdf--------[2] http:\/\/deeplearning.cs.cmu.edu\/pdfs\/OJA.pca.pdf--------[3] http:\/\/www.anthology.aclweb.org\/P\/P10\/P10-1040."":-1.0434163809,""This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.----------------While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.----------------Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation"":-3.5729386806},""This paper introduces a similarity encoder based on a standard feed-forward neural network. The approach is utilized to generate a simple extension of the CBOW word2vec model."":{""This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.----------------Although the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word\/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \""A Simple Word Embedding Model for Lexical Substitution\"" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.----------------In addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: --------* http:\/\/veceval.com\/ (Nayak et al., RepEval 2016"":-4.1706190109,""this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. ----------------First, considering the related work [1,2] the proposed approach brings marginal novelty. Especially--------Context Encoders is just a small improvement over word2vec. ----------------Experimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].----------------[1] http:\/\/papers.nips.cc\/paper\/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions.pdf--------[2] http:\/\/deeplearning.cs.cmu.edu\/pdfs\/OJA.pca.pdf--------[3] http:\/\/www.anthology.aclweb.org\/P\/P10\/P10-1040."":-3.7767944336,""This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.----------------While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.----------------Slightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation"":-0.6765524745}}","There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved."
https://openreview.net/forum?id=SkgSXUKxx,"['This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. The connection with Batch Normalization could have broader impact.'
 'This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm. The authors argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is'
 'The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. The proposed approach relates to Batch Norm and weight decay.']","['This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. The connection with Batch Normalization could have broader impact.'
 'This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm. The authors argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is'
 'The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. The proposed approach relates to Batch Norm and weight decay.']","This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. The connection with Batch Normalization could have broader impact.                                       0.603480
This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm. The authors argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is    0.825904
The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. The proposed approach relates to Batch Norm and weight decay.                         0.885605
dtype: float32","This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. The connection with Batch Normalization could have broader impact.                                       1.098612
This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm. The authors argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is    1.098612
The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. The proposed approach relates to Batch Norm and weight decay.                         1.098612
dtype: float32","{""This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. The connection with Batch Normalization could have broader impact."":{""Summary--------===--------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016. In that paper a regularizer was proposed which penalizes--------gradient magnitudes and it was shown to aid low-shot learning performance.--------This work shows that the previous regularizer is equivalent to a direct penalty--------on the magnitude of feature values weighted differently per example.----------------The analysis goes to to provide two examples where a feature penalty--------favors a better representation. The first example addresses the XOR--------problem, constructing a network where a feature penalty encourages--------a representation where XOR is linearly separable.--------The second example analyzes a 2 layer linear network, showing improved stability--------of a 2nd order optimizer when the feature penalty is added.--------One last bit of analysis shows how this regularizer can be interpreted as--------a Gaussian prior on both features and weights. Since the prior can be--------interpreted as having a"":-8.3649349213,""This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.----------------First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about \""results can be derived for \u03c6(x) with convex differentiable non-linear activation functions such as ReLU\"", both via analysis and experimentation to measure numerical stability.----------------Second, the authors again show an interesting correspond"":-10.5751609802,""The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.--------Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.--------The proposed approach relates to Batch Norm and weight decay.--------Experiments are given on \""low-shot\"" settting.--------There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?--------Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?--------Overall, the idea is simple but feels like preliminary: while it is supposed to be a \""soft BN\"", BN itself gets better performance than feature penalty, and both together give"":-11.2854509354},""This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm. The authors argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is"":{""Summary--------===--------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016. In that paper a regularizer was proposed which penalizes--------gradient magnitudes and it was shown to aid low-shot learning performance.--------This work shows that the previous regularizer is equivalent to a direct penalty--------on the magnitude of feature values weighted differently per example.----------------The analysis goes to to provide two examples where a feature penalty--------favors a better representation. The first example addresses the XOR--------problem, constructing a network where a feature penalty encourages--------a representation where XOR is linearly separable.--------The second example analyzes a 2 layer linear network, showing improved stability--------of a 2nd order optimizer when the feature penalty is added.--------One last bit of analysis shows how this regularizer can be interpreted as--------a Gaussian prior on both features and weights. Since the prior can be--------interpreted as having a"":-4.2284812927,""This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.----------------First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about \""results can be derived for \u03c6(x) with convex differentiable non-linear activation functions such as ReLU\"", both via analysis and experimentation to measure numerical stability.----------------Second, the authors again show an interesting correspond"":-0.7821183801,""The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.--------Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.--------The proposed approach relates to Batch Norm and weight decay.--------Experiments are given on \""low-shot\"" settting.--------There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?--------Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?--------Overall, the idea is simple but feels like preliminary: while it is supposed to be a \""soft BN\"", BN itself gets better performance than feature penalty, and both together give"":-4.1830019951},""The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net. The proposed approach relates to Batch Norm and weight decay."":{""Summary--------===--------This paper extends and analyzes the gradient regularizer of Hariharan and--------Girshick 2016. In that paper a regularizer was proposed which penalizes--------gradient magnitudes and it was shown to aid low-shot learning performance.--------This work shows that the previous regularizer is equivalent to a direct penalty--------on the magnitude of feature values weighted differently per example.----------------The analysis goes to to provide two examples where a feature penalty--------favors a better representation. The first example addresses the XOR--------problem, constructing a network where a feature penalty encourages--------a representation where XOR is linearly separable.--------The second example analyzes a 2 layer linear network, showing improved stability--------of a 2nd order optimizer when the feature penalty is added.--------One last bit of analysis shows how this regularizer can be interpreted as--------a Gaussian prior on both features and weights. Since the prior can be--------interpreted as having a"":-9.1056022644,""This paper proposes analysis of regularization, weight Froebius-norm and feature L2 norm, showing that it is equivalent to another proposed regularization, gradient magnitude loss. They then argue that: 1) it is helpful to low-shot learning, 2) it is numerically stable, 3) it is a soft version of Batch Normalization. Finally, they demonstrate experimentally that such a regularization improves performance on low-shot tasks.----------------First, this is a nice analysis of some simple models, and proposes interesting insights in some optimization issues. Unfortunately, the authors do not demonstrate, nor argue in a convincing manner, that such an analysis extends to deep non-linear computation structures. I feel like the authors could write a full paper about \""results can be derived for \u03c6(x) with convex differentiable non-linear activation functions such as ReLU\"", both via analysis and experimentation to measure numerical stability.----------------Second, the authors again show an interesting correspond"":-8.7579774857,""The paper proposes to use a last-layer feature penalty as regularization on the last layer of a neural net.--------Although the equations suggest a weighting per example, dropping this weight (alpha_i) works equally well.--------The proposed approach relates to Batch Norm and weight decay.--------Experiments are given on \""low-shot\"" settting.--------There seem to be two stories in the paper: feature penalty as a soft batch norm version, and low-shot learning; why is feature penalty specifically adapted to low-shot learning and not a more classical supervised task?--------Regarding your result on Omniglot, 91.5, I believe it is still about 2% worse than the Matching Networks, which you refer to but don't put in Table 1. Why?--------Overall, the idea is simple but feels like preliminary: while it is supposed to be a \""soft BN\"", BN itself gets better performance than feature penalty, and both together give"":-5.1620931625}}","The paper extends a regularizer on the gradients recently proposed by Hariharan and Girshick. I agree with the reviewers that while the analysis is interesting, it is unclear why this particular regularizer is especially relevant for low-shot learning. And the experimental validation is not strong enough to warrant acceptance."
https://openreview.net/forum?id=SkpSlKIel,"['This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units.'
 'The paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network.'
 '. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units. Lower bounds on the neural network size']","['This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units.'
 'The paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network.'
 '. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units. Lower bounds on the neural network size']","This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units.    0.642866
The paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network.                                                                                                                                                               0.938410
. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units. Lower bounds on the neural network size                                                                                                    0.839028
dtype: float32","This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units.    1.098612
The paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network.                                                                                                                                                               1.098612
. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1/e) layers and O(log 1/e) ReLU units. Lower bounds on the neural network size                                                                                                    1.098612
dtype: float32","{""This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units."":{""SUMMARY --------This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. ----------------PROS --------The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. ----------------CONS--------The main result appears to address only strongly convex univariate functions. ----------------SPECIFIC COMMENTS ----------------- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. --------I still have not read the revision and maybe you have already addressed some of these points there. ----------------- The problem statement is close to that from [Montufar, Pascanu, Cho, Beng"":-0.6762583852,""The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1\/eps)) layers and O(poly log(1\/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:----------------1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?----------------2- Is there an example of piecewise smooth function that requires at least poly(1\/eps) hidden units with a shallow network?"":-3.2591164112,""This paper shows:----------------  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1\/e) layers and O(log 1\/e) ReLU units.--------  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.--------  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.----------------The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance."":-3.4290399551},""The paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network."":{""SUMMARY --------This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. ----------------PROS --------The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. ----------------CONS--------The main result appears to address only strongly convex univariate functions. ----------------SPECIFIC COMMENTS ----------------- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. --------I still have not read the revision and maybe you have already addressed some of these points there. ----------------- The problem statement is close to that from [Montufar, Pascanu, Cho, Beng"":-14.866938591,""The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1\/eps)) layers and O(poly log(1\/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:----------------1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?----------------2- Is there an example of piecewise smooth function that requires at least poly(1\/eps) hidden units with a shallow network?"":-10.6509275436,""This paper shows:----------------  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1\/e) layers and O(log 1\/e) ReLU units.--------  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.--------  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.----------------The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance."":-14.7025823593},"". Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1\/e) layers and O(log 1\/e) ReLU units. Lower bounds on the neural network size"":{""SUMMARY --------This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of the paper is to show that approximating a strongly convex differentiable function is possible with much less units when using a network with one more hidden layer. ----------------PROS --------The paper presents an interesting combination of tools and arrives at a nice result on the exponential superiority of depth. ----------------CONS--------The main result appears to address only strongly convex univariate functions. ----------------SPECIFIC COMMENTS ----------------- Thanks for the comments on L. Still it would be a good idea to clarify this point as far as possible in the main part. Also, I would suggest to advertise the main result more prominently. --------I still have not read the revision and maybe you have already addressed some of these points there. ----------------- The problem statement is close to that from [Montufar, Pascanu, Cho, Beng"":-4.7966704369,""The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1\/eps)) layers and O(poly log(1\/eps)) hidden units where the activation functions can be either ReLU or binary step or any combination of them. The paper is well written and clear. The arguments and proofs are easy to follow. I only have two questions:----------------1- It would be great to have similar results without binary step units. To what extent do you find the binary step unit central to the proof?----------------2- Is there an example of piecewise smooth function that requires at least poly(1\/eps) hidden units with a shallow network?"":-4.3824710846,""This paper shows:----------------  1. Easy, constructive proofs to derive e-error upper-bounds on neural networks with O(log 1\/e) layers and O(log 1\/e) ReLU units.--------  2. Extensions of the previous results to more general function classes, such as smooth or vector-valued functions.--------  3. Lower bounds on the neural network size, as a function of its number of layers. The lower bound reveals the need of exponentially many more units to approximate functions using shallow architectures.----------------The paper is well written and easy to follow. The technical content, including the proofs in the Appendix, look correct. Although the proof techniques are simple (and are sometimes modifications of arguments by Gil, Telgarsky, or Dasgupta), they are brought together in a coherent manner to produce sharp results. Therefore, I am leaning toward acceptance."":-1.0831756592}}","The paper makes a solid technical contribution in proving that the deep networks are exponentially more efficient in function approximation compared to the shallow networks. They take the case of piecewise smooth networks, which is practically motivated (e.g. images have edges with smooth regions), and analyze the size of both the deep and shallow networks required to approximate it to the same degree.    The reviewers recommend acceptance of the paper and I am happy to go with their recommendation."
https://openreview.net/forum?id=SkqMSCHxe,"['A paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future. The architecture allows several RNNs to compete to make the best predictions.'
 'The paper is understandable but could benefit from some copy editing. Competitive learning creates several duplicates of the baseline neural architecture.'
 'Authors propose a competitive learning architecture that learn different RNN predictors independently. This work is applied to the task of predictive different driving behaviors from human drivers.']","['A paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future. The architecture allows several RNNs to compete to make the best predictions.'
 'The paper is understandable but could benefit from some copy editing. Competitive learning creates several duplicates of the baseline neural architecture.'
 'Authors propose a competitive learning architecture that learn different RNN predictors independently. This work is applied to the task of predictive different driving behaviors from human drivers.']","A paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future. The architecture allows several RNNs to compete to make the best predictions.    0.609664
The paper is understandable but could benefit from some copy editing. Competitive learning creates several duplicates of the baseline neural architecture.                                                                      0.868141
Authors propose a competitive learning architecture that learn different RNN predictors independently. This work is applied to the task of predictive different driving behaviors from human drivers.                           0.911403
dtype: float32","A paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future. The architecture allows several RNNs to compete to make the best predictions.    1.098612
The paper is understandable but could benefit from some copy editing. Competitive learning creates several duplicates of the baseline neural architecture.                                                                      1.098612
Authors propose a competitive learning architecture that learn different RNN predictors independently. This work is applied to the task of predictive different driving behaviors from human drivers.                           1.098612
dtype: float32","{""A paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future. The architecture allows several RNNs to compete to make the best predictions."":{""This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.---------------- It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the \u201cintegrated prediction\u201d obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.----------------It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger"":-0.7171747684,""This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. ----------------The competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it\u2019s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.----------------There is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of"":-3.2388648987,""Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.--------It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?--------The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited."":-3.2999415398},""The paper is understandable but could benefit from some copy editing. Competitive learning creates several duplicates of the baseline neural architecture."":{""This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.---------------- It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the \u201cintegrated prediction\u201d obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.----------------It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger"":-11.4521484375,""This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. ----------------The competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it\u2019s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.----------------There is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of"":-7.6103878021,""Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.--------It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?--------The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited."":-11.0975580215},""Authors propose a competitive learning architecture that learn different RNN predictors independently. This work is applied to the task of predictive different driving behaviors from human drivers."":{""This paper introduces a neural network architecture and training procedure for predicting the speed of a vehicle several seconds into the future based on video and vehicle state input. The architecture allows several RNNs to compete to make the best predictions, with only the best prediction receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.---------------- It is not clear how the best-performing RNN is chosen for each time point at test time. That is, how is the \u201cintegrated prediction\u201d obtained in Fig. 7? Is the prediction the one with minimum error over all of the output layers? If so, this means the prediction cannot be made until you already know the value to be predicted.----------------It seems possible that a larger generic RNN might be able to generate accurate predictions. If I understand correctly, the competitive architectures have many more parameters than the baseline. Is the improved performance here due to the competitive scheme, or just a larger"":-6.6093425751,""This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. Competitive learning creates several duplicates of the baseline neural architecture and during training only updates the architecture with minimum loss. The experiments compare the competitive learning approach to a single baseline architecture on a driving benchmark task. The paper is understandable but could benefit from some copy editing. ----------------The competitive learning approach seems rather adhoc and this paper feels quite incomplete without significant discussion and comparisons to ensembling. Much recent work has shown that duplicating and ensembling neural architectures can produce gains, and it\u2019s not clear why competitive learning is better than ensembling, it seems less theoretically sound to me.----------------There is a huge confound in the experiments due to the competitive learning architecture having many more free parameters than the baseline architecture. Again I think comparing to ensembling with the same number of architectures duplicated and perhaps comparing to a single baseline with larger hidden layers to make the total number of"":-6.6545710564,""Authors propose a competitive learning architecture that learn different RNN predictors independently, akin to a committee of experts which are chosen with a hard switch at run-time. This work is applied to the task of predictive different driving behaviors from human drivers, and combines behaviors at test time, often switching behaviors within seconds. Prediction loss is lower than the similar but non-competitive architecture used as a baseline.--------It is not very clear how to interpret the results, what is the real impact of the model. If behaviors switch very often, can this really be seen as choosing the best driving mode for a given situation? Maybe the motivation needs to be rephrased a little to be more convincing?--------The competitive approach presented is interesting but not really novel, thus the impact of this paper for a conference such as ICLR may be limited."":-2.7025461197}}","The authors present a prediction framework that involves multiple 'competitive' RNNs, and they claim that they are predicting human intention. It is unclear if this method, which seems quite ad-hoc, is any different from a simple ensemble approach, and it is unclear that the model is predicting human intention. The experiments do not adequately demonstrate either."
https://openreview.net/forum?id=Sy1rwtKxg,"['The paper proposes a parallel mechanism for stochastic gradient descent method (SGD) The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner.'
 'The proposed approach can only work for a small class of models. It is unclear why we need to develope parallel algorithm for linear regressio problems. Most datasets considered in the paper can be solved in a few second using a single core CPU'
 'This paper describes a correction technique to combine updates from multiple SGD.']","['The paper proposes a parallel mechanism for stochastic gradient descent method (SGD) The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner.'
 'The proposed approach can only work for a small class of models. It is unclear why we need to develope parallel algorithm for linear regressio problems. Most datasets considered in the paper can be solved in a few second using a single core CPU'
 'This paper describes a correction technique to combine updates from multiple SGD.']","The paper proposes a parallel mechanism for stochastic gradient descent method (SGD) The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner.                                                     0.866485
The proposed approach can only work for a small class of models. It is unclear why we need to develope parallel algorithm for linear regressio problems. Most datasets considered in the paper can be solved in a few second using a single core CPU    0.621834
This paper describes a correction technique to combine updates from multiple SGD.                                                                                                                                                                       0.812427
dtype: float32","The paper proposes a parallel mechanism for stochastic gradient descent method (SGD) The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner.                                                     1.098612
The proposed approach can only work for a small class of models. It is unclear why we need to develope parallel algorithm for linear regressio problems. Most datasets considered in the paper can be solved in a few second using a single core CPU    1.098612
This paper describes a correction technique to combine updates from multiple SGD.                                                                                                                                                                       1.098612
dtype: float32","{""The paper proposes a parallel mechanism for stochastic gradient descent method (SGD) The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner."":{""This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.----------------I feel that there might be some fundamental misunderstanding on SGD.----------------''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O("":-3.3828620911,""Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:----------------- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. ----------------- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). ----------------- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the"":-7.319437027,""This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.--------Comments--------1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.--------2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.--------3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.----------------Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the"":-6.7842555046},""The proposed approach can only work for a small class of models. It is unclear why we need to develope parallel algorithm for linear regressio problems. Most datasets considered in the paper can be solved in a few second using a single core CPU"":{""This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.----------------I feel that there might be some fundamental misunderstanding on SGD.----------------''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O("":-4.0758695602,""Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:----------------- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. ----------------- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). ----------------- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the"":-1.7367892265,""This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.--------Comments--------1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.--------2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.--------3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.----------------Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the"":-4.6260471344},""This paper describes a correction technique to combine updates from multiple SGD."":{""This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations (including least square linear regression and polynomial regression problems). The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. To make such combiner more efficient, the authors also use a randomized projection matrix to do dimension reduction. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.----------------I feel that there might be some fundamental misunderstanding on SGD.----------------''The combiner matrixM  generate above can be quite large and expensive to compute. The sequential SGD algorithm maintains and updates the weight vector w , and thus requires O(f)  space and time, where f  is the number of features. In contrast,M  is a f f  matrix and consequently, the space and time complexity of parallel SGD is O("":-31.7889976501,""Overall, the idea in this paper is interesting and the paper is well-written and well-motivated.  However, I think it is not ready to publish in ICLR for the following reasons:----------------- This paper is not related to representation learning. It may be more suitable for a general machine learning or data mining conference. ----------------- The proposed approach can only work for a small class of models and cannot apply to popular formulations,  such as SVM, logistic regression, and neural network. It is unclear why we want to use SGD for this specific type of formulations. For model like linear regression, the authors should compare their methods with linear programming approaches. Also, it is unclear why we need to develope parallel algorithm for linear regressio problems as they are relatively easy to solve unless the data are big (see next comment). ----------------- The dataset used in the paper are relatively small and can be only used for proving the concept. Most datasets considered in the"":-32.444896698,""This paper describes a correction technique to combine updates from multiple SGD to make it statistically equivalent to sequential technique.--------Comments--------1) The proposed method is novel and interesting to allow update to be corrected even when the update is delayed.--------2) The proposed theory can only be applied to square loss setting (with linear update rule), making it somewhat limited. This paper would be much more interesting to ICLR community, if the technique is applicable to general objective function and settings of deep neural networks.--------3) The resulting technique requires book-keeping of a dimensional reduced combiner matrix, which causes more computation in terms of complexity. The authors argue that the overhead can be canceled with SIMD support for symbolic update. However, the normal update of SGD might also benefit from SIMD, especially when the dataset is dense.----------------Overall, even though the practical value of this work is limited by 2) and 3), the technique(specifically the correction rule) proposed in the"":-28.7242774963}}","The reviewers largely agree that this paper is well written and presents an interesting, novel approach to parallelizing Stochastic Gradient Descent. However, the current formulation is restricted to linear regression models and requires sketching techniques to handle large number of features, although it is striking that very aggressive sketching (k~10) still works well. In this setting though, there are specialized randomized solvers such as Blendenpick (Avron et al) which sketch the data upfront to construct a high quality pre-conditioner for use with iterative methods.   The authors are encouraged to either compare with state of the art parallel randomized least squares solvers developed in the numerical linear algebra community (see papers by Michael Mahoney, Petros Drineas and others), or broaden the scope of the proposed methods to include models of current interest (e.g., DNNs). The latter would of course be of specific interest to the ICLR community."
https://openreview.net/forum?id=Sy7m72Ogg,"['The method is presented against popular adaptive first-order methods for training deep networks. The results are interesting but difficult to assess in a true apples-to-apples manner.'
 'Proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments on MNIST and CIFAR 10.'
 ' network architecture used for experiments on CIFAR-10 is quite outdated. Bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one.']","['The method is presented against popular adaptive first-order methods for training deep networks. The results are interesting but difficult to assess in a true apples-to-apples manner.'
 'Proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments on MNIST and CIFAR 10.'
 ' network architecture used for experiments on CIFAR-10 is quite outdated. Bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one.']","The method is presented against popular adaptive first-order methods for training deep networks. The results are interesting but difficult to assess in a true apples-to-apples manner.    0.708688
Proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments on MNIST and CIFAR 10.                                                                           0.912507
 network architecture used for experiments on CIFAR-10 is quite outdated. Bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one.     0.916681
dtype: float32","The method is presented against popular adaptive first-order methods for training deep networks. The results are interesting but difficult to assess in a true apples-to-apples manner.    1.098612
Proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments on MNIST and CIFAR 10.                                                                           1.098612
 network architecture used for experiments on CIFAR-10 is quite outdated. Bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one.     1.098612
dtype: float32","{""The method is presented against popular adaptive first-order methods for training deep networks. The results are interesting but difficult to assess in a true apples-to-apples manner."":{""The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:-----------------What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.---------Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.---------Since learning is a non"":-1.1965847015,""The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.----------------I have two main concerns. One is the lack of comparisons to similar recently proposed methods - \""Learning Step Size Controllers for Robust Neural Network Training\"" by Daniel et al. and \""Learning to learn by gradient descent by gradient descent\"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?----------------My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in"":-3.9409384727,""In the question response the authors mention and compare other works such as \""Learning to Learn by Gradient Descent by Gradient Descent\"", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.--------The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.--------As discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.--------In summary, the general idea"":-4.2858834267},""Proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments on MNIST and CIFAR 10."":{""The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:-----------------What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.---------Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.---------Since learning is a non"":-8.3901472092,""The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.----------------I have two main concerns. One is the lack of comparisons to similar recently proposed methods - \""Learning Step Size Controllers for Robust Neural Network Training\"" by Daniel et al. and \""Learning to learn by gradient descent by gradient descent\"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?----------------My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in"":-4.212767601,""In the question response the authors mention and compare other works such as \""Learning to Learn by Gradient Descent by Gradient Descent\"", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.--------The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.--------As discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.--------In summary, the general idea"":-7.9455418587},"" network architecture used for experiments on CIFAR-10 is quite outdated. Bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one."":{""The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc). The results are interesting but difficult to assess in a true apples-to-apples manner. Some specific comments:-----------------What is the computational overhead of the actor-critic algorithm relative to other algorithms? No plots with the wall-time of optimization are presented, even though the success of methods like Adagrad was due to their wall-time performance, not the number of iterations.---------Why was only a single learning rate learned? To accurately compare against other popular first order methods, why not train a separate RL model for each parameter, similar to how popular first-order methods adaptively change the learning rate for each parameter.---------Since learning is a non"":-5.0981416702,""The paper proposes using an actor-critic RL algorithm for training learning rate controllers for supervised learning. The proposed method outperforms standard optimizers like SGD, ADAM and RMSprop in experiments conducted on MNIST and CIFAR 10.----------------I have two main concerns. One is the lack of comparisons to similar recently proposed methods - \""Learning Step Size Controllers for Robust Neural Network Training\"" by Daniel et al. and \""Learning to learn by gradient descent by gradient descent\"" by Andrychowicz et al. The work of Daniel et al. is quite similar because it also proposes using a policy search RL method (REPS) and it is not clear what the downsides of their approach are. Their work does use more prior knowledge as the authors stated, but why is this a bad thing?----------------My second concern is with the experiments. Some of the numbers reported for the other methods are surprisingly low. For example, why is RMSprop so bad in"":-4.6026988029,""In the question response the authors mention and compare other works such as \""Learning to Learn by Gradient Descent by Gradient Descent\"", but the goal of current work and that work is quite different. That work is a new form of optimization algorithm which is not the case here. And bayesian hyper-parameter optimization methods aim for multiple hyper-parameters but this work only tune one hyper-parameter.--------The network architecture used for the experiments on CIFAR-10 is quite outdated and the performances are much poorer than any work that has published in last few years. So the comparison are not valid here, as if the paper claim the advantage of their method, they should use the state of the art network architecture and see if their claim still holds in that setting too.--------As discussed before, the extra cost of hyper-parameter optimizers are only justified if the method could push the SOTA results in multiple modern datasets.--------In summary, the general idea"":-0.8612838984}}","The authors use actor-critic reinforcement learning to adjust the step size of a supervised learning algorithm. There are no comparisons made to other, similar approaches, and the baselines are suspiciously weak, making the proposed method difficult to justify."
https://openreview.net/forum?id=SyCSsUDee,"['The paper introduces supervised deep learning with layer-wise reconstruction loss and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.'
 'The paper presents a new regularization technique for neural networks. It seeks to maximize correlation between input variables, latent variables and outputs.'
 'The paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. There are a number of technical issues with the paper.']","['The paper introduces supervised deep learning with layer-wise reconstruction loss and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.'
 'The paper presents a new regularization technique for neural networks. It seeks to maximize correlation between input variables, latent variables and outputs.'
 'The paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. There are a number of technical issues with the paper.']","The paper introduces supervised deep learning with layer-wise reconstruction loss and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.    1.022628
The paper presents a new regularization technique for neural networks. It seeks to maximize correlation between input variables, latent variables and outputs.                                                                                                0.615720
The paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. There are a number of technical issues with the paper.                                                                                           0.668038
dtype: float32","The paper introduces supervised deep learning with layer-wise reconstruction loss and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.    1.098612
The paper presents a new regularization technique for neural networks. It seeks to maximize correlation between input variables, latent variables and outputs.                                                                                                1.098612
The paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. There are a number of technical issues with the paper.                                                                                           1.098612
dtype: float32","{""The paper introduces supervised deep learning with layer-wise reconstruction loss and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively."":{""The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.----------------The derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.----------------Is \\sigma in Equation 8 trainable parameter or hyperparameter"":-0.5365607142,""The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is achieved by defining a measure of total correlation between these variables and decomposing it in terms of entropies and conditional entropies.----------------Authors explain that they do not actually maximize the total correlation, but a lower-bound of it that ignores simple entropy terms, and only considers conditional entropies. It is not clearly explained what is the rationale for discarding these entropy terms.----------------Entropies measures are applying to probability distributions (i.e. this implies that the variables in the model should be random). The link between the conditional entropy formulation and the reconstruction error is not made explicit. In order to link these two views, I would have expected, for example, a noise model for the units of the network.----------------Later in the paper, it is claimed that the original ladder network is not suitable for supervised learning with small samples"":-5.5403418541,""This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.----------------Technical issues:----------------The move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.----------------In (3) arbitrary balancing paramters lamda"":-5.6567282677},""The paper presents a new regularization technique for neural networks. It seeks to maximize correlation between input variables, latent variables and outputs."":{""The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.----------------The derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.----------------Is \\sigma in Equation 8 trainable parameter or hyperparameter"":-12.6310138702,""The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is achieved by defining a measure of total correlation between these variables and decomposing it in terms of entropies and conditional entropies.----------------Authors explain that they do not actually maximize the total correlation, but a lower-bound of it that ignores simple entropy terms, and only considers conditional entropies. It is not clearly explained what is the rationale for discarding these entropy terms.----------------Entropies measures are applying to probability distributions (i.e. this implies that the variables in the model should be random). The link between the conditional entropy formulation and the reconstruction error is not made explicit. In order to link these two views, I would have expected, for example, a noise model for the units of the network.----------------Later in the paper, it is claimed that the original ladder network is not suitable for supervised learning with small samples"":-10.0001144409,""This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.----------------Technical issues:----------------The move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.----------------In (3) arbitrary balancing paramters lamda"":-12.5158224106},""The paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations. There are a number of technical issues with the paper."":{""The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise for better representation learning. Total correlation measure and additional insights from auto-encoder are used to derive layer-wise reconstruction loss and is further combined with supervised loss. When combining with supervised loss the class-conditional additive noise model is proposed, which showed consistent improvement over the baseline model. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.----------------The derivation of Equation (3) from total correlation is hacky. Moreover, assuming graphical model between X, Y and Z, it should be more carefully derived to estimate H(X|Z) and H(Z|Y). The current proposal, encoding Z and Y from X and decoding from encoded representation is not really well justified.----------------Is \\sigma in Equation 8 trainable parameter or hyperparameter"":-9.3646430969,""The paper presents a new regularization technique for neural networks, which seeks to maximize correlation between input variables, latent variables and outputs. This is achieved by defining a measure of total correlation between these variables and decomposing it in terms of entropies and conditional entropies.----------------Authors explain that they do not actually maximize the total correlation, but a lower-bound of it that ignores simple entropy terms, and only considers conditional entropies. It is not clearly explained what is the rationale for discarding these entropy terms.----------------Entropies measures are applying to probability distributions (i.e. this implies that the variables in the model should be random). The link between the conditional entropy formulation and the reconstruction error is not made explicit. In order to link these two views, I would have expected, for example, a noise model for the units of the network.----------------Later in the paper, it is claimed that the original ladder network is not suitable for supervised learning with small samples"":-9.3451948166,""This paper introduces a maximum total correlation procedure, adds a target and then adds noise perturbations.----------------Technical issues:----------------The move from (1) to (2) is problematic. Yes it is a lower bound, but by igoring H(Z), equation (2) ignores the fact that H(Z) will potentially vary more significantly that H(Z|Y). As a result of removing H(Z), the objective (2) encourages Z that are low entropy as the H(Z) term is ignored, doubly so as low entropy Z results in low entropy Z|Y. Yes the -H(X|Z) mitigates against a complete entropy collapse for H(Z), but it still neglects critical terms. In fact one might wonder if this is the reason that semantic noise addition needs to be done anyway, just to push up the entropy of Z to stop it reducing too much.----------------In (3) arbitrary balancing paramters lamda"":-6.5994973183}}","The reviewers all expressed concerns with the technical quality of this work. In particular, the reviewers are concerned that ignoring certain entropy terms in the objective is problematic and would require significantly more justification theoretically and empirically. The reviewers believe that the authors had to resort to unjustified tricks such as adding noise in order to compensate for the missing terms in the objective. Some of the reviewers also had concerns with the choice of experiments, expressing that the authors did not choose the right baseline comparisons to compare to (e.g. convolutional networks vs. fully connected networks on MNIST). Hopefully the thorough feedback and lengthly discussion, along with the authors' responses (both in the text and additions to the paper and appendix), will lead to a stronger submission to a future conference."
https://openreview.net/forum?id=SyEiHNKxx,"['. A great tool. However, the paper lacks detail. It is impossible for someone to reimplement the research-mostly because of the lack of detail.'
 '.'
 'This paper creates a physics simulator using theano. It uses it to learn a neural network policy by back propagating gradients.']","['. A great tool. However, the paper lacks detail. It is impossible for someone to reimplement the research-mostly because of the lack of detail.'
 '.'
 'This paper creates a physics simulator using theano. It uses it to learn a neural network policy by back propagating gradients.']",". A great tool. However, the paper lacks detail. It is impossible for someone to reimplement the research-mostly because of the lack of detail.    0.737703
.                                                                                                                                                  0.163430
This paper creates a physics simulator using theano. It uses it to learn a neural network policy by back propagating gradients.                    0.940883
dtype: float32",". A great tool. However, the paper lacks detail. It is impossible for someone to reimplement the research-mostly because of the lack of detail.    1.098591
.                                                                                                                                                  1.069294
This paper creates a physics simulator using theano. It uses it to learn a neural network policy by back propagating gradients.                    1.098612
dtype: float32","{"". A great tool. However, the paper lacks detail. It is impossible for someone to reimplement the research-mostly because of the lack of detail."":{""I would definitely love to have this and use it for my research. A great tool.----------------However, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack of detail. However, replicability is a crucial part of science. Other publications proposing software (e.g. the tensorflow, theano and edward papers) come along with open source code. This is not the case here and therefore the picture is quite incomplete.----------------I am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more. Nevertheless, this is just an encouragement to the authors to interact with those communities."":-1.0450394154,""A differentiable physics engine is indeed a wonderful thing to have. ----------------The key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Besides describing the engine's speed in rather creative units (e.g. \""model seconds per day\""), the reader has no idea if this is fast or slow. Todorov's engine (my simulator of choice) computes a dynamics step and its derivatives wrt both states and controls (using finite-differences) in less than 1ms for a *full humanoid* model (his code is available here mujoco.org\/book\/programming.html#saDerivative). I think this actually faster than the engine described in this paper, but I can't be sure.----------------Because this engine is so limited in what it can collide (sphere\/sphere and sphere\/plane), it would be trivial to build the example models in several other popular engines (e.g"":-4.0011906624,""This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.----------------My two key reservations with the paper are as follows:--------1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.--------2. They key novelty\/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.----------------For the reader to fully"":-4.1381540298},""."":{""I would definitely love to have this and use it for my research. A great tool.----------------However, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack of detail. However, replicability is a crucial part of science. Other publications proposing software (e.g. the tensorflow, theano and edward papers) come along with open source code. This is not the case here and therefore the picture is quite incomplete.----------------I am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more. Nevertheless, this is just an encouragement to the authors to interact with those communities."":-135.813949585,""A differentiable physics engine is indeed a wonderful thing to have. ----------------The key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Besides describing the engine's speed in rather creative units (e.g. \""model seconds per day\""), the reader has no idea if this is fast or slow. Todorov's engine (my simulator of choice) computes a dynamics step and its derivatives wrt both states and controls (using finite-differences) in less than 1ms for a *full humanoid* model (his code is available here mujoco.org\/book\/programming.html#saDerivative). I think this actually faster than the engine described in this paper, but I can't be sure.----------------Because this engine is so limited in what it can collide (sphere\/sphere and sphere\/plane), it would be trivial to build the example models in several other popular engines (e.g"":-135.5243530273,""This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.----------------My two key reservations with the paper are as follows:--------1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.--------2. They key novelty\/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.----------------For the reader to fully"":-134.5188140869},""This paper creates a physics simulator using theano. It uses it to learn a neural network policy by back propagating gradients."":{""I would definitely love to have this and use it for my research. A great tool.----------------However, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack of detail. However, replicability is a crucial part of science. Other publications proposing software (e.g. the tensorflow, theano and edward papers) come along with open source code. This is not the case here and therefore the picture is quite incomplete.----------------I am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more. Nevertheless, this is just an encouragement to the authors to interact with those communities."":-7.189666748,""A differentiable physics engine is indeed a wonderful thing to have. ----------------The key selling point of the proposed software is its speed, however there is no comparison to other physics engines. Besides describing the engine's speed in rather creative units (e.g. \""model seconds per day\""), the reader has no idea if this is fast or slow. Todorov's engine (my simulator of choice) computes a dynamics step and its derivatives wrt both states and controls (using finite-differences) in less than 1ms for a *full humanoid* model (his code is available here mujoco.org\/book\/programming.html#saDerivative). I think this actually faster than the engine described in this paper, but I can't be sure.----------------Because this engine is so limited in what it can collide (sphere\/sphere and sphere\/plane), it would be trivial to build the example models in several other popular engines (e.g"":-7.2527952194,""This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.----------------My two key reservations with the paper are as follows:--------1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.--------2. They key novelty\/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.----------------For the reader to fully"":-3.0695726871}}","Originality, significance:   The paper implements a physics-based simulator directly using Theano. This avoids the type of finite differentiation that physics engines such as MuJoCo use to compute derivatives. It is quite an interesting idea, and is demonstrated using learned control for several models.     Quality, clarity:   The original version was somewhat loosely written; the current version is improved.    Pros:  - The nice idea of implementing a physics engine in a language such as Theano, and showing that this is quite feasible.  - May inspire further work in this direction.    Cons:  - The speed is not systematically evaluated, as compared to finite-difference-based engines. It is thought to be ""in the same ballpark"" as other more full-featured engines. It is not clear that those using simulators will care whether it uses the true derivatives or finite differences."
https://openreview.net/forum?id=SygvTcYee,"['The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between'
 'The paper proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.'
 'Proposed method is an extension of the MAC method. Subproblems are trained on a distributed cluster in a circular configuration.'
 'ParMAC is a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models. It is based on the composition of multiple processing layers (i.e., deep nets)']","['The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between'
 'The paper proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.'
 'Proposed method is an extension of the MAC method. Subproblems are trained on a distributed cluster in a circular configuration.'
 'ParMAC is a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models. It is based on the composition of multiple processing layers (i.e., deep nets)']","The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between    1.005196
The paper proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.                                                                                                                                                              0.951967
Proposed method is an extension of the MAC method. Subproblems are trained on a distributed cluster in a circular configuration.                                                                                                                                                       0.684701
ParMAC is a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models. It is based on the composition of multiple processing layers (i.e., deep nets)                                                                      0.928924
dtype: float32","The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between    1.386294
The paper proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.                                                                                                                                                              1.386294
Proposed method is an extension of the MAC method. Subproblems are trained on a distributed cluster in a circular configuration.                                                                                                                                                       1.386294
ParMAC is a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models. It is based on the composition of multiple processing layers (i.e., deep nets)                                                                      1.386294
dtype: float32","{""The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between"":{""The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.----------------My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely"":-1.1487799883,""UPDATE:--------I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.--------However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, \""...ICLR submission focus on the ParMAC algorithm...\"", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.----------------ORIGINAL REVIEW:--------The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.----------------Related Work: In the part"":-4.6744146347,""This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.----------------This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. ----------------There also seem to be a few ideas worth comparing, at least:--------- Circular vs. parameter server configurations--------- Decoupled sub-problems vs. parallel SGD----------------Parallel"":-4.7256727219,""This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are"":-4.5138645172},""The paper proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea."":{""The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.----------------My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely"":-14.6745681763,""UPDATE:--------I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.--------However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, \""...ICLR submission focus on the ParMAC algorithm...\"", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.----------------ORIGINAL REVIEW:--------The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.----------------Related Work: In the part"":-10.8958311081,""This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.----------------This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. ----------------There also seem to be a few ideas worth comparing, at least:--------- Circular vs. parameter server configurations--------- Decoupled sub-problems vs. parallel SGD----------------Parallel"":-15.0362901688,""This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are"":-13.3875780106},""Proposed method is an extension of the MAC method. Subproblems are trained on a distributed cluster in a circular configuration."":{""The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.----------------My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely"":-15.0894422531,""UPDATE:--------I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.--------However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, \""...ICLR submission focus on the ParMAC algorithm...\"", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.----------------ORIGINAL REVIEW:--------The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.----------------Related Work: In the part"":-15.1926517487,""This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.----------------This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. ----------------There also seem to be a few ideas worth comparing, at least:--------- Circular vs. parameter server configurations--------- Decoupled sub-problems vs. parallel SGD----------------Parallel"":-12.5447587967,""This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are"":-14.9546775818},""ParMAC is a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models. It is based on the composition of multiple processing layers (i.e., deep nets)"":{""The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) (Carreira-Perpinan and Wang, 2012). This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines. Relatively good speedup factors are reported especially on larger datasets and a theoretical model of performance is presented that matches with the experiments.----------------My main concern is that even though the method is presented as a general framework for nested functions, experiments focus on a restricted family of models (i.e. binary autoencoders with linear or kernel encoders and linear decoders) with only two components. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers and is not widely"":-3.9203498363,""UPDATE:--------I looked at the arxiv version of the paper. It is much longer and appears more rigorous. Fig 3 there is indeed more insightful.--------However, I am reviewing the submission and my overall assessment does not change. This is not a minor incremental contribution, and if you want to compress it into a conference submission of this type, I would recommend choosing message you want to convey, and focus on that. As you say, \""...ICLR submission focus on the ParMAC algorithm...\"", I would focus on this properly - and remove or move to appendix all extensions and theoretical remarks, and have an extra page on explaining the algorithm. Additionally, make sure to clearly explain the relation of the arxiv paper, in particular that the submission was a compressed version.----------------ORIGINAL REVIEW:--------The submission proposes ParMAC, based on MAC (Method of Auxiliary Coordinates), formulating a distributed variant of the idea.----------------Related Work: In the part"":-3.9069652557,""This paper proposes an extension of the MAC method in which subproblems are trained on a distributed cluster arranged in a circular configuration. The basic idea of MAC is to decouple the optimization between parameters and the outputs of sub-pieces of the model (auxiliary coordinates); optimization alternates between updating the coordinates given the parameters and optimizing the parameters given the outputs. In the circular configuration. Because each update is independent, they can be massively parallelized.----------------This paper would greatly benefit from more concrete examples of the sub-problems and how they decompose. For instance, can this be applied effectively for deep convolutional networks, recurrent models, etc? From a practical perspective, there's not much impact for this paper beyond showing that this particular decoupling scheme works better than others. ----------------There also seem to be a few ideas worth comparing, at least:--------- Circular vs. parameter server configurations--------- Decoupled sub-problems vs. parallel SGD----------------Parallel"":-4.4126634598,""This paper proposes a novel approach ParMAC, a parallel and distributed framework of MAC (the Method of Auxiliary Coordinates) to learn nested and non-convex models which is based on the composition of multiple processing layers (i.e., deep nets). The basic idea of MAC to optimise the nested objective function, which is traditionally learned using methods based on the chain-rule gradients but inconvenient and is hard to parallelise, is to break nested functional relationships judiciously by introducing new variables ( the auxiliary coordinates) as equality constraints, and then to optimise a penalised function using alternating optimisation over the original parameters (W step) and over the coordinates (Z step).  The minimisation (W step) updates the parameters by splitting the nested model into independent submodels and training them using existing algorithms, and the coordination (Z step) ensures that corresponding inputs and outputs of submodels eventually match.  In this paper, the basic assumptions of ParMAC are"":-0.8408522606}}","The work proposes a parallel/distributed variant of the MAC decomposition method. In presents some theoretical and experimental results supporting the parallelization strategy. The reviews are mixed and indeed a common concern among the reviewers was the choice of test problem. To me it is ok to only concentrate on a single class of problems, but in this case it needs to be a problem that the ICLR community identifies as being of central importance. Otherwise, if a more esoteric problem is chosen then I (and the reviewers) would rather see that the method is useful on multiple problems. Otherwise, it's basically impossible to extrapolate the experiments to new settings and we are forced to re-implement the algorithm. I'm not saying that the authors necessarily need to consider deep networks and there are many alternative possible models (sparse coding, collaborative filtering, etc.). But it should be noted that, without further experimental comparisons, it is impossible to verify the author's claims that the method is effective for deeply-nested models.    Other concerns brought up by the reviewers (beyond the clarity/presentation issues, which should also be addressed): the experimental comparison would be more convincing with a comparison to an existing approach like a parallel SGD method. I appreciate that the authors have done a lot of work already on this problem, but doing such obvious comparisons should be the job of the author instead of the reader (focusing purely on parallelization would be ok if the MAC model was extremely-widely-used already and parallelizing was an open problem, but my impression is that this is not the case). As a minor aside, the memory issue will be more serious for deeply-nested models, due to the use of the decomposition approach (we don't want to store the activations for all layers for all examples), and this doesn't arise in SGD."
https://openreview.net/forum?id=SypU81Ole,"['. This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. The techniques proposed are simple, well explained, and of immediate use to those working on gener'
 'This paper proposed a set of techniques that seem to produce visually good looking results. While this paper has some interesting ideas, it also has a number of problems.'
 '. Authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs.']","['. This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. The techniques proposed are simple, well explained, and of immediate use to those working on gener'
 'This paper proposed a set of techniques that seem to produce visually good looking results. While this paper has some interesting ideas, it also has a number of problems.'
 '. Authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs.']",". This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. The techniques proposed are simple, well explained, and of immediate use to those working on gener    0.632702
This paper proposed a set of techniques that seem to produce visually good looking results. While this paper has some interesting ideas, it also has a number of problems.                                                                   0.549676
. Authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs.                                                                                                 0.669723
dtype: float32",". This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. The techniques proposed are simple, well explained, and of immediate use to those working on gener    1.098612
This paper proposed a set of techniques that seem to produce visually good looking results. While this paper has some interesting ideas, it also has a number of problems.                                                                   1.098612
. Authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs.                                                                                                 1.098612
dtype: float32","{"". This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. The techniques proposed are simple, well explained, and of immediate use to those working on gener"":{""This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons \/ quantitative evaluations of the techniques proposed. Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work."":-0.9820480347,""This paper proposed a set of different things under the name of \""sampling generative models\"", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.----------------The spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances"":-3.8755691051,""In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.----------------I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide ("":-3.3835155964},""This paper proposed a set of techniques that seem to produce visually good looking results. While this paper has some interesting ideas, it also has a number of problems."":{""This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons \/ quantitative evaluations of the techniques proposed. Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work."":-8.0739603043,""This paper proposed a set of different things under the name of \""sampling generative models\"", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.----------------The spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances"":-5.8116230965,""In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.----------------I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide ("":-8.2670202255},"". Authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs."":{""This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper is somewhat challenging to assess since it doesn't propose a new algorithm, model, application etc. On the one hand these techniques will be highly relevant to the generative modeling community and I think this paper deserves a wide audience. The techniques proposed are simple, well explained, and of immediate use to those working on generative models. However, I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights into sampling generative models and there are no comparisons \/ quantitative evaluations of the techniques proposed. Overall, I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling. I would be willing to increase my core if the author could present a case for why ICLR is an appropriate venue for this work."":-12.4981842041,""This paper proposed a set of different things under the name of \""sampling generative models\"", focusing on analyzing the learned latent space and synthesizing desirable output images with certain properties for GANs.  This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.  While this paper has some interesting ideas, it also has a number of problems.----------------The spherical interpolation idea is interesting, but after a second thought this does not make much sense.  The proposed slerp interpolation equation (page 2) implicitly assumes that the two points q1 and q2 lie on the same sphere, in which case the parameter theta is the angle corresponding to the great arc connecting the two points on the sphere.  However, the latent space of a GAN, no matter trained with a uniform distribution or a Gaussian distribution, is not a distribution on a sphere, and many points have different distances"":-12.7203435898,""In this paper the authors propose various techniques to sample visualizations from generative models with high dimensional latent spaces like VAEs and GANs. For example, the authors highlight the well known but often not sufficiently appreciated fact that the probability mass of high dimensional Gaussian distributions concentrates near a thin hyper-shell with a certain radius. They therefore propose to use spherical interpolations (great arcs) instead of the commonly used linear interpolations. In a similar spirit they propose a visualisation for analogies and techniques to reinforce structure in VAE latent spaces.----------------I find it hard to give clear recommendation for this paper: On the one hand I enjoyed reading it and I might want use some of the proposals (e.g. spherical interpolations; J-diagrams) in future work of mine. On the other hand, it\u2019s obvious that this paper is not a typical machine learning paper; it does not propose a new model, or training method, or provide ("":-9.8441028595}}","This paper proposes some interesting ideas about visualizing latent-variable models. The paper is nicely written and presented, but the originality and importance of the work isn't enough. Also, neither the reviewers nor I were convinced that spherical interpolation makes more sense than linear interpolation."
https://openreview.net/forum?id=Sywh5KYex,"['Gated Residual Networks adds gating to shortcut connections with a scalar to regulate the gate. Authors claim that this approach will improve the training Residual Networks.'
 'This paper proposes to learn a single scalar gating parameter instead of a full gating tensor. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.'
 ' network. Gated residual networks can most easily learn identity mappings.']","['Gated Residual Networks adds gating to shortcut connections with a scalar to regulate the gate. Authors claim that this approach will improve the training Residual Networks.'
 'This paper proposes to learn a single scalar gating parameter instead of a full gating tensor. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.'
 ' network. Gated residual networks can most easily learn identity mappings.']","Gated Residual Networks adds gating to shortcut connections with a scalar to regulate the gate. Authors claim that this approach will improve the training Residual Networks.                            0.870591
This paper proposes to learn a single scalar gating parameter instead of a full gating tensor. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.    0.836833
 network. Gated residual networks can most easily learn identity mappings.                                                                                                                               0.798965
dtype: float32","Gated Residual Networks adds gating to shortcut connections with a scalar to regulate the gate. Authors claim that this approach will improve the training Residual Networks.                            1.098612
This paper proposes to learn a single scalar gating parameter instead of a full gating tensor. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.    1.098612
 network. Gated residual networks can most easily learn identity mappings.                                                                                                                               1.098612
dtype: float32","{""Gated Residual Networks adds gating to shortcut connections with a scalar to regulate the gate. Authors claim that this approach will improve the training Residual Networks."":{""This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.----------------It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC"":-3.260037899,""This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.----------------The basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \""shutting off\"" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.----------------Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual"":-7.1632971764,""The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). ----------------Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. ----------------Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments."":-6.7261161804},""This paper proposes to learn a single scalar gating parameter instead of a full gating tensor. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation."":{""This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.----------------It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC"":-4.1519608498,""This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.----------------The basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \""shutting off\"" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.----------------Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual"":-0.5392766595,""The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). ----------------Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. ----------------Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments."":-3.8976745605},"" network. Gated residual networks can most easily learn identity mappings."":{""This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The authors claim that this approach will improve the training Residual Networks.----------------It seems the authors could get competitive performance on CIFAR-10 to state of art models with only Wide Res Nets. Wide Gated ResNet requires much more parameters than DenseNet (and other Res Net variants) for obtaining a little improvement over Dense Net.  More importantly, the authors state that they obtained the best results on CIFAR-10 and CIFAR-100 but the updated version of DenseNet (Huang et al. (2016b)) has new results for a version called DenseNet-BC which outperforms all of the results that authors reported (3.46 for CIFAR-10 and 17.18 for CIFAR-100 with 25.6M parameters, DenseNet-BC"":-26.442363739,""This paper proposes to learn a single scalar gating parameter instead of a full gating tensor in highway networks. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation.----------------The basic idea of the paper is simple and is clearly presented. It is a natural simplification of highway networks to allow easily \""shutting off\"" layers while keeping number of additional parameters low. However, in this regard the paper leaves out a few key points. Firstly, it does not mention that the gates in highway networks are data-dependent which is potentially more powerful than learning a fixed gate for all units and independent of data. Secondly, it does not do a fair comparison with highway networks to show that this simpler formulation is indeed easier to learn.----------------Did the authors try their original design of u = g(k)f(x) + (1 - g(k))x where f(x) is a plain layer instead of a residual"":-27.0392074585,""The paper presents a layer architecture where a single parameter is used to  gate the output response of layer to amplify or suppress it. It is shown that such an architecture can ease optimization of a deep network as it is easy to learn identity mappings in layers helping in better gradient propagation to lower layers (better supervision). ----------------Using an introduced SDI metric it shown that gated residual networks can most easily learn identity mappings compared to other architectures. ----------------Although good theoretical reasoning is presented the observed experimental evidence of learned k values does not seem to strongly support the theory given that learned  k values are mostly very small and not varying much across layers. Also, experimental validation of the approach is not quite strong in terms of reported performances and number of large scale experiments."":-23.419303894}}","Although this was a borderline paper, the reviewers ultimately concluded that, given how easy it would be for a practitioner to independently devise the methodological trick of the paper, the paper did not demonstrate that the idea was sufficiently useful to merit acceptance."
https://openreview.net/forum?id=r10FA8Kxg,"['Experiments test whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset.'
 'The authors conducted experiments on the CIFAR10 dataset. They find a significant performance gap between the two approaches.']","['Experiments test whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset.'
 'The authors conducted experiments on the CIFAR10 dataset. They find a significant performance gap between the two approaches.']","Experiments test whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset.    0.292507
The authors conducted experiments on the CIFAR10 dataset. They find a significant performance gap between the two approaches.                                                                                    0.295598
dtype: float32","Experiments test whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset.    0.693146
The authors conducted experiments on the CIFAR10 dataset. They find a significant performance gap between the two approaches.                                                                                    0.693146
dtype: float32","{""Experiments test whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset."":{""Description.--------This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. ----------------Strong  points.--------- The experiments are carefully done with thorough selection of hyperparameters. --------- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).--------- The paper is well and clearly written.----------------Weak points:--------- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?----------------Originality:--------- This is"":-0.8637638092,""This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. --------To this end the authors conducted a series of experiments on the CIFAR10 dataset.--------They find that there is a significant performance gap between the two approaches, in favour of deep CNNs. --------The experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.--------They also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.----------------My take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The"":-2.6988668442},""The authors conducted experiments on the CIFAR10 dataset. They find a significant performance gap between the two approaches."":{""Description.--------This paper describes experiments testing whether deep convolutional networks can be replaced with shallow networks with the same number of parameters without loss of accuracy. The experiments are performed on he CIFAR 10 dataset where deep convolutional teacher networks are used to train shallow student networks using L2 regression on logit outputs.  The results show that similar accuracy on the same parameter budget can be only obtained when multiple layers of convolution are used. ----------------Strong  points.--------- The experiments are carefully done with thorough selection of hyperparameters. --------- The paper shows interesting results that go partially against conclusions from the previous work in this area (Ba and Caruana 2014).--------- The paper is well and clearly written.----------------Weak points:--------- CIFAR is still somewhat toy dataset with only 10 classes. It would be interesting to see some results on a more challenging problem such as ImageNet. Would the results for a large number of classes be similar?----------------Originality:--------- This is"":-10.3010082245,""This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification, given that both architectures use the same number of parameters. --------To this end the authors conducted a series of experiments on the CIFAR10 dataset.--------They find that there is a significant performance gap between the two approaches, in favour of deep CNNs. --------The experiments are well designed and involve a distillation training approach, and the results are presented in a comprehensive manner.--------They also observe (as others have before) that student models can be shallower than the teacher model from which they are trained for comparable performance.----------------My take on these results is that they suggest that using (deep) conv nets is more effective, since this model class encodes a form of a-prori or domain knowledge that images exhibit a certain degree of translation invariance in the way they should be processed for high-level recognition tasks. The"":-8.4516935349}}",The reviewers unanimously recommend accepting this paper.
https://openreview.net/forum?id=r1X3g2_xl,"['The authors propose to apply virtual adversarial training to semi-supervised classification. Theoretical and experimental comparison with past work is missing.'
 ' training is simple but non-trivial. The paper is worth to publish.'
 '. The paper applies adversarial and virtual adversarial training to LSTM for text classification.']","['The authors propose to apply virtual adversarial training to semi-supervised classification. Theoretical and experimental comparison with past work is missing.'
 ' training is simple but non-trivial. The paper is worth to publish.'
 '. The paper applies adversarial and virtual adversarial training to LSTM for text classification.']","The authors propose to apply virtual adversarial training to semi-supervised classification. Theoretical and experimental comparison with past work is missing.    0.355081
 training is simple but non-trivial. The paper is worth to publish.                                                                                                0.630538
. The paper applies adversarial and virtual adversarial training to LSTM for text classification.                                                                  0.282347
dtype: float32","The authors propose to apply virtual adversarial training to semi-supervised classification. Theoretical and experimental comparison with past work is missing.    1.098604
 training is simple but non-trivial. The paper is worth to publish.                                                                                                1.098612
. The paper applies adversarial and virtual adversarial training to LSTM for text classification.                                                                  1.098568
dtype: float32","{""The authors propose to apply virtual adversarial training to semi-supervised classification. Theoretical and experimental comparison with past work is missing."":{""The authors propose to apply virtual adversarial training to semi-supervised classification.----------------It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.----------------In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).----------------Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).----------------"":-2.1090457439,""This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Extending the idea of the adversarial training to the text tasks is simple but non-trivial. Overall the paper is worth to publish. ----------------I only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze. "":-3.9567668438,""*** Paper Summary ***----------------This paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.----------------*** Review Summary ***----------------The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.----------------*** Detailed Review ***----------------The paper reads well. I have only a few comments regarding experiments and link to prior resarch:----------------Experiments:----------------- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?--------- As another baseline, did you consider dropping words, i"":-3.7992222309},"" training is simple but non-trivial. The paper is worth to publish."":{""The authors propose to apply virtual adversarial training to semi-supervised classification.----------------It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.----------------In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).----------------Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).----------------"":-11.6103582382,""This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Extending the idea of the adversarial training to the text tasks is simple but non-trivial. Overall the paper is worth to publish. ----------------I only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze. "":-8.7552185059,""*** Paper Summary ***----------------This paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.----------------*** Review Summary ***----------------The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.----------------*** Detailed Review ***----------------The paper reads well. I have only a few comments regarding experiments and link to prior resarch:----------------Experiments:----------------- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?--------- As another baseline, did you consider dropping words, i"":-11.172914505},"". The paper applies adversarial and virtual adversarial training to LSTM for text classification."":{""The authors propose to apply virtual adversarial training to semi-supervised classification.----------------It is quite hard to assess the novelty on the algorithmic side at this stage: there is a huge available literature on semi-supervised learning (especially SVM-related literature, but some work were applied to neural networks too); unfortunately the authors do not mention it, nor relate their approach to it, and stick to the adversarial world.----------------In terms of novelty on the adversarial side, the authors propose to add perturbations at the level of words embeddings, rather than the input itself (having in mind applications to NLP).----------------Concerning the experimental section, authors focus on text classification methods. Again, comparison with the existing SVM-related literature is important to assess the viability of the proposed approach; for example (Wang et al, 2012) report 8.8% on IMBD with a very simple linear SVM (without transductive setup).----------------"":-8.7582893372,""This paper applies the idea of the adversarial training and virtual adversarial training to the LSTM-based model in the text context. The paper is in general well written and easy to follow. Extending the idea of the adversarial training to the text tasks is simple but non-trivial. Overall the paper is worth to publish. ----------------I only have a minor comment: it is also interesting to see how much adversarial training can help in the performance of RNN, which is a simpler model and may be easier to analyze. "":-7.9647397995,""*** Paper Summary ***----------------This paper applies adversarial and virtual adversarial training to LSTM for text classification. Since text inputs are discrete adversarial perturbation are applied to the (normalized) word embeddings. Extensive experiments are reported and demonstrate the advantage of these methods.----------------*** Review Summary ***----------------The paper reads well and has sufficent references. The application of adversarial training to text data is a simple but not trivial extension. The experimental section presents extensive experiments with comparison to alternative strategies. The proposed method is simple and effective and can be easily be applied after reading the paper.----------------*** Detailed Review ***----------------The paper reads well. I have only a few comments regarding experiments and link to prior resarch:----------------Experiments:----------------- In Table 2 (and for other datasets as well), could you include an SVM baseline? e.g. S Wang and C Manning 2012?--------- As another baseline, did you consider dropping words, i"":-6.8076725006}}","This paper is concerned with extending adversarial and virtual adversarial training to text classification tasks. The main technical contribution is to apply perturbations to word embeddings rather than discrete input symbols. Excellent empirical performance is reported across a variety of tasks.     The reviewers were consensual in acknowledging the clarity and significance of the contribution, highlighting the quality of the numerical experiments. Moreover, the authors were responsive in the rebuttal phase and updated their paper with reviewers suggestions (such as the svm-related comparisons).     The AC thus recommends accepting this work as a poster."
https://openreview.net/forum?id=r1aPbsFle,"['This work offers a theoretical justification for reusing the input word embedding in the output projection layer.'
 'This paper provides a theoretical framework for tying parameters between input and output word representations in the softmax.'
 'This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs.']","['This work offers a theoretical justification for reusing the input word embedding in the output projection layer.'
 'This paper provides a theoretical framework for tying parameters between input and output word representations in the softmax.'
 'This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs.']","This work offers a theoretical justification for reusing the input word embedding in the output projection layer.                 0.718549
This paper provides a theoretical framework for tying parameters between input and output word representations in the softmax.    0.751609
This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs.                0.794911
dtype: float32","This work offers a theoretical justification for reusing the input word embedding in the output projection layer.                 1.098612
This paper provides a theoretical framework for tying parameters between input and output word representations in the softmax.    1.098612
This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs.                1.098612
dtype: float32","{""This work offers a theoretical justification for reusing the input word embedding in the output projection layer."":{""This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.----------------It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection"":-3.2592978477,""This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.--------Experiments on PTB shows significant improvement.--------The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.--------I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.--------Have you considered using character or sub-word units in that context?"":-6.6601939201,""This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. ----------------I see two main drawbacks from this framework:--------The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. --------The augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. ----------------Nevertheless the argument is very interesting, and clearly written.--------The simulated results indeed validate the argument, and the PTB results seem promising.----------------Minor comments:--------Section 3:--------Can you clarify if y~ is conditioned on the t example or on the entire history.--------Eq."":-5.8520932198},""This paper provides a theoretical framework for tying parameters between input and output word representations in the softmax."":{""This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.----------------It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection"":-6.8578767776,""This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.--------Experiments on PTB shows significant improvement.--------The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.--------I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.--------Have you considered using character or sub-word units in that context?"":-3.416664362,""This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. ----------------I see two main drawbacks from this framework:--------The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. --------The augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. ----------------Nevertheless the argument is very interesting, and clearly written.--------The simulated results indeed validate the argument, and the PTB results seem promising.----------------Minor comments:--------Section 3:--------Can you clarify if y~ is conditioned on the t example or on the entire history.--------Eq."":-6.2055454254},""This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs."":{""This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input. However, the construction of the estimate of the true data distribution seems engineered to provide the weight tying justification in Eqs. 3.6 and 3.7.----------------It is not obvious why the projection matrix L in Eq 3.6 (let's rename it to L') should be the same as that in Eq. 2.1. For example, L' could be obtained through word2vec embeddings trained on a large dataset or it could be learned as an additional set of parameters. In the case that L' is a new learned matrix, it seems the result in Eq 4.5 is to use an independent matrix for the output projection"":-3.9628605843,""This paper provides a theoretical framework for tying parameters between input word embeddings and output word representations in the softmax.--------Experiments on PTB shows significant improvement.--------The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread), which I see as the main negative side of the paper. The proposed justification appears new to me though, and certainly interesting.--------I was concerned that results are only given on one dataset, PTB, which is now kind of old in that literature. I'm glad the authors tried at least one more dataset, and I think it would be nice to find a way to include these results in the paper if accepted.--------Have you considered using character or sub-word units in that context?"":-4.4012980461,""This paper gives a theoretical motivation for tieing the word embedding and output projection matrices in RNN LMs. The argument uses an augmented loss function which spreads the output probability mass among words with close word-embedding. ----------------I see two main drawbacks from this framework:--------The augmented loss function has no trainable parameters and is used for only for regularization. This is not expected to give gains with large enough datasets. --------The augmented loss is heavily \u201cengineered\u201d to produce the desired result of parameter tying. It\u2019s not clear what happens if you try to relax it a bit, by adding parameters, or estimating y~ in a different way. ----------------Nevertheless the argument is very interesting, and clearly written.--------The simulated results indeed validate the argument, and the PTB results seem promising.----------------Minor comments:--------Section 3:--------Can you clarify if y~ is conditioned on the t example or on the entire history.--------Eq."":-0.8933630586}}","pros:  - nice results on the tasks that justify acceptance of the paper    cons:  - In my opinion its a big stretch to describe this paper as a novel framework. The reasons for using the specific contrived augmented loss is based on the good results it produces. I view it more as regularization.  - The ""theoretical justification"" for coupling of the input and output layers is based on the premise that the above regularization is the correct thing to do. Since that's really not justified by some kind of theory, I think its questionable to call this simple observation a theoretical justification.  - Tying weights on the inputs and output layers is far from novel."
https://openreview.net/forum?id=r1fYuytex,"[""Experimental results look reasonable, validated on 3 tasks. Rumelhart's paper cited for back-propagation than the Deep Learning book.""
 'The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks.'
 'The authors reply still does not convince me.']","[""Experimental results look reasonable, validated on 3 tasks. Rumelhart's paper cited for back-propagation than the Deep Learning book.""
 'The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks.'
 'The authors reply still does not convince me.']","Experimental results look reasonable, validated on 3 tasks. Rumelhart's paper cited for back-propagation than the Deep Learning book.                                                                        1.024052
The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks.    0.852130
The authors reply still does not convince me.                                                                                                                                                                0.035933
dtype: float32","Experimental results look reasonable, validated on 3 tasks. Rumelhart's paper cited for back-propagation than the Deep Learning book.                                                                        1.098612
The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks.    1.098612
The authors reply still does not convince me.                                                                                                                                                                1.097236
dtype: float32","{""Experimental results look reasonable, validated on 3 tasks. Rumelhart's paper cited for back-propagation than the Deep Learning book."":{""Experimental results look reasonable, validated on 3 tasks. --------References could be improved, for example I would rather see--------Rumelhart's paper cited for back-propagation than the Deep Learning book."":-2.1378982067,""The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. --------The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.--------The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. --------This explanation would be clearer to"":-7.3004498482,""From my original comments:----------------The results looks good but the baselines proposed are quite bad.----------------For instance in the table 2 \""Misclassification rate for a 784-1024-1024-1024-10 \"" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \""significant\"" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...----------------In CIFAR-10 experiments, i do not understand  why \""Sparsely-Connected 90% + Single-Precision Floating-Point\"" is worse than \""Sparsely-Connected 90% + BinaryConnect\"". So it is better to use binary than float. ----------------Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian"":-7.1471891403},""The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks."":{""Experimental results look reasonable, validated on 3 tasks. --------References could be improved, for example I would rather see--------Rumelhart's paper cited for back-propagation than the Deep Learning book."":-4.7647576332,""The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. --------The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.--------The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. --------This explanation would be clearer to"":-1.0757770538,""From my original comments:----------------The results looks good but the baselines proposed are quite bad.----------------For instance in the table 2 \""Misclassification rate for a 784-1024-1024-1024-10 \"" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \""significant\"" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...----------------In CIFAR-10 experiments, i do not understand  why \""Sparsely-Connected 90% + Single-Precision Floating-Point\"" is worse than \""Sparsely-Connected 90% + BinaryConnect\"". So it is better to use binary than float. ----------------Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian"":-4.5218420029},""The authors reply still does not convince me."":{""Experimental results look reasonable, validated on 3 tasks. --------References could be improved, for example I would rather see--------Rumelhart's paper cited for back-propagation than the Deep Learning book."":-32.6560440063,""The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. --------The paper removes some of the connections in the fully connected layers and shows performance and computational efficiency increase in networks on three different datasets. It is also a good addition that the authors combine their method with binary and ternary connect studies and show further improvements.--------The paper was hard for me to understand because of this misleading statement: In this paper, we propose sparsely-connected networks by reducing the number of connections of fully-connected networks using linear-feedback shift registers (LFSRs). It led me to think that LFSRs reduced the connections by keeping some of the information in the registers. However, LFSR is only used as a random binary generator. Any random generator could be used but LFSR is chosen for the convenience in VLSI implementation. --------This explanation would be clearer to"":-32.1205406189,""From my original comments:----------------The results looks good but the baselines proposed are quite bad.----------------For instance in the table 2 \""Misclassification rate for a 784-1024-1024-1024-10 \"" the result for the FC with floating point is 1.33%. Well far from what we can obtain from this topology, near to 0.8%. I would like to see \""significant\"" compression levels on state of the art results or good baselines. I can get 0,6% with two FC hidden layers...----------------In CIFAR-10 experiments, i do not understand  why \""Sparsely-Connected 90% + Single-Precision Floating-Point\"" is worse than \""Sparsely-Connected 90% + BinaryConnect\"". So it is better to use binary than float. ----------------Again i think that in the experiments the authors are not using all the techniques that can be easily applied to float but not to binary (gaussian"":-31.9877986908}}","After discussion, the reviewers unanimously recommend accepting the paper."
https://openreview.net/forum?id=r1kGbydxg,"['The paper is straightforward, easy to read, and has clear results. The video is nice.'
 'Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies.'
 'This paper addresses a question that is often overlooked in reinforcement learning. Authors only consider a single neural network architecture and a single reward function.']","['The paper is straightforward, easy to read, and has clear results. The video is nice.'
 'Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies.'
 'This paper addresses a question that is often overlooked in reinforcement learning. Authors only consider a single neural network architecture and a single reward function.']","The paper is straightforward, easy to read, and has clear results. The video is nice.                                                                                                                                                                   0.598883
Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies.    1.001836
This paper addresses a question that is often overlooked in reinforcement learning. Authors only consider a single neural network architecture and a single reward function.                                                                            0.787930
dtype: float32","The paper is straightforward, easy to read, and has clear results. The video is nice.                                                                                                                                                                   1.098612
Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies.    1.098612
This paper addresses a question that is often overlooked in reinforcement learning. Authors only consider a single neural network architecture and a single reward function.                                                                            1.098612
dtype: float32","{""The paper is straightforward, easy to read, and has clear results. The video is nice."":{""The paper is straightforward, easy to read, and has clear results. ----------------Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?----------------Would we get the same result if there was no reference-pose cost, only a locomotion cost?----------------Would we get the same result if the task was to spin a top? My guess is no. ----------------This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.----------------The video is nice."":-9.4160289764,""Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. ----------------> Significance & Originality:----------------The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems\/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics \/ graphics community. ----------------> Clarity:----------------"":-12.2115097046,""This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.----------------My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. --------The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  --------My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \""DeepRL\"" seems arbitrary.----------------On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. --------It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work"":-11.6891736984},""Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies."":{""The paper is straightforward, easy to read, and has clear results. ----------------Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?----------------Would we get the same result if there was no reference-pose cost, only a locomotion cost?----------------Would we get the same result if the task was to spin a top? My guess is no. ----------------This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.----------------The video is nice."":-5.4464793205,""Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. ----------------> Significance & Originality:----------------The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems\/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics \/ graphics community. ----------------> Clarity:----------------"":-0.6643858552,""This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.----------------My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. --------The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  --------My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \""DeepRL\"" seems arbitrary.----------------On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. --------It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work"":-5.412214756},""This paper addresses a question that is often overlooked in reinforcement learning. Authors only consider a single neural network architecture and a single reward function."":{""The paper is straightforward, easy to read, and has clear results. ----------------Since all these parameterisations end up outputting torques, it seems like there shouldn't be much difference between them. There is a known function that convert from one representation to another (or at least to torques). Is it not possible that the only reason proportional control is a little better is that the tracking cost is a function of positions?----------------Would we get the same result if there was no reference-pose cost, only a locomotion cost?----------------Would we get the same result if the task was to spin a top? My guess is no. ----------------This work is interesting, but not likely to generalise to other scenarios, and in that sense is rather limited.----------------The video is nice."":-6.5176129341,""Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. ----------------> Significance & Originality:----------------The explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems\/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics \/ graphics community. ----------------> Clarity:----------------"":-6.6752381325,""This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.----------------My biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. --------The authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  --------My suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \""DeepRL\"" seems arbitrary.----------------On the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. --------It's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work"":-3.3539991379}}","After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.    The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the ""default"" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.    But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.    I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples."
https://openreview.net/forum?id=r1kQkVFgl,"['A paper presents an improved neural language models designed for selected long-term dependency.'
 'This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages.'
 'This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy. They release a Python open source dataset.']","['A paper presents an improved neural language models designed for selected long-term dependency.'
 'This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages.'
 'This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy. They release a Python open source dataset.']","A paper presents an improved neural language models designed for selected long-term dependency.                                                               0.934802
This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages.                             0.864046
This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy. They release a Python open source dataset.    0.856731
dtype: float32","A paper presents an improved neural language models designed for selected long-term dependency.                                                               1.098612
This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages.                             1.098612
This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy. They release a Python open source dataset.    1.098612
dtype: float32","{""A paper presents an improved neural language models designed for selected long-term dependency."":{""This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by:----------------1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. --------2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.----------------Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.----------------While the paper still misses some critical information"":-12.3744602203,""This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and\/or pointers truly show an advantage in capturing long term dependencies.----------------The sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.----------------The construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future"":-16.325340271,""This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. "":-16.6483039856},""This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages."":{""This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by:----------------1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. --------2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.----------------Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.----------------While the paper still misses some critical information"":-8.1050157547,""This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and\/or pointers truly show an advantage in capturing long term dependencies.----------------The sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.----------------The construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future"":-4.9149627686,""This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. "":-9.1880760193},""This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy. They release a Python open source dataset."":{""This paper presents an improved neural language models designed for selected long-term dependency, i.e., to predict more accurately the next identifier for the dynamic programming language such as Python. The improvements are obtained by:----------------1) replacing the fixed-widow attention with a pointer network, in which the memory only consists of context representation of the previous K identifies introduced for the entire history. --------2) a conventional neural LSTM-based language model is combined with such a sparse pointer network with a controller, which linearly combines the prediction of both components using a dynamic weights, decided by the input, hidden state, and the context representations at the time stamp.----------------Such a model avoids the the need of large window size of the attention to predict next identifier, which usually requires a long-term dependency in the programming language. This is partly validated by the python codebase (which is another contribution of this paper) experiments in the paper.----------------While the paper still misses some critical information"":-4.4216222763,""This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. Code suggestion seems an area where attention and\/or pointers truly show an advantage in capturing long term dependencies.----------------The sparse pointer method does seem to provide better results than attention for similar window sizes - specifically, comparing a window size of 20 for the attention and sparse pointer method shows the sparse pointer winning fairly definitively across the board. Given a major advantage of the pointer method is being able to use a large window size well thanks to the supervision the pointer provides, it was unfortunate (though understandable due to potential memory issues) not to see larger window sizes. Having a different batch size for the sparse pointer and attention models is unfortunate given it complicates an otherwise straight comparison between the two models.----------------The construction and filtering of the Python corpus sounds promising but as of now it is still inaccessible (listed in the paper as TODO). Given that code suggestion seems an interesting area for future"":-4.3877663612,""This paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. Additionally they release a Python open source dataset. As expected this augmentation, the fixed attention policy, improves the perplexity of the model. It seems important to dig a bit deeper into these results and show the contribution of different token types to the achieve perplexity. This is alluded to in the text, but a more thorough comparison would be welcome. The idea of an attention policy that takes advantage of expert knowledge is a nice contribution, but perhaps if limited novelty --- for example the Maddison and Tarlow 2014 paper, which the authors cite, has scoping rules that track previously used identifiers in scope. "":-0.8166339993}}","This paper augments language models with attention to to capture long range dependencies through a sparse pointer network that is restricted to previously introduced identifiers, and demonstrates the proposed architecture over a new, released large-scale code suggestion corpus of 41M lines of Python code. The addition of long range attention over 20 identifiers improves perplexity compared to an LSTM with an attentional context of 50 words, but degrades accuracy (hit @1), while improving hit@5.  The experimental validation however requires a more thorough analysis and more detailed ablation experiments and discussions, and more thorough comparison to related work. As is, many choices seem quite arbitrary and make it hard to determine if the model is really performing well (minibatch sizes, size of the memory for the LSTM, choice and number of identifiers for the sparse pointers, etc)."
https://openreview.net/forum?id=r1osyr_xg,"['This paper proposes a method for estimating the context sensitivity of paraphrases. It uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation.'
 'This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations.'
 'This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates.']","['This paper proposes a method for estimating the context sensitivity of paraphrases. It uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation.'
 'This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations.'
 'This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates.']","This paper proposes a method for estimating the context sensitivity of paraphrases. It uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation.            0.661173
This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations.                                                                                                                          0.703907
This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates.    0.758250
dtype: float32","This paper proposes a method for estimating the context sensitivity of paraphrases. It uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation.            1.098612
This paper tries to leverage an external lexicon / knowledge base to improve corpus-based word representations.                                                                                                                          1.098612
This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates.    1.098612
dtype: float32","{""This paper proposes a method for estimating the context sensitivity of paraphrases. It uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation."":{""This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.----------------On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.----------------Detailed\/minor points below:----------------1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very"":-1.1295278072,""This paper tries to leverage an external lexicon \/ knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.----------------I think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.----------------In addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).----------------Finally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations \/ lexicons"":-4.0710935593,""This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.----------------The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.----------------Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij)"":-3.6724672318},""This paper tries to leverage an external lexicon \/ knowledge base to improve corpus-based word representations."":{""This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.----------------On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.----------------Detailed\/minor points below:----------------1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very"":-16.2003326416,""This paper tries to leverage an external lexicon \/ knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.----------------I think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.----------------In addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).----------------Finally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations \/ lexicons"":-13.2964363098,""This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.----------------The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.----------------Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij)"":-16.1710510254},""This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates."":{""This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods (particularly those that do not require the paraphrase database!). Likewise, the model section did not convince me that this was the most obvious model formulation to try. The paper would be stronger if model choices were explained more convincingly or - better yet - alternatives were explored.----------------On balance I lean towards rejecting the paper and encouraging the authors to submit a revised and improved version at a near point in the future.----------------Detailed\/minor points below:----------------1) While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker. In its current form long sections are very"":-3.7308776379,""This paper tries to leverage an external lexicon \/ knowledge base to improve corpus-based word representations by determining (in a fuzzy way) which potential paraphrase is the most appropriate in a particular context.----------------I think this paper is a bit lost in translation. The grammatical and storytelling styles made it really difficult for me to concentrate, and even unintelligible at times. One of the most important criteria in a conference paper is to communicate one's ideas clearly; unfortunately, I do not feel that this paper meets that standard.----------------In addition, the evaluation is rather lacking. There are many ways to evaluate word representations, and Google's analogy dataset has many issues (see, for example, Linzen's paper from RepEval 2016, as well as Drozd et al., COLING 2016).----------------Finally, this work does not provide any qualitative result or motivation. Why does this method work better? Where does it fail? What have we learned about word representations \/ lexicons"":-3.7825438976,""This paper introduces the concept of fuzzy paraphrases to aid in the learning of distributed word representations from a corpus augmented by a lexicon or ontology. Sometimes polysemy is context-dependent, but prior approaches have neglected this fact when incorporating external paraphrase information during learning. The main idea is to introduce a function that essentially judges the context-sensitivity of paraphrase candidates, down-weighting those candidates that depend strongly on context. This function is inferred from bilingual translation agreement.----------------The main argumentation leading to the model selection is intuitive, and I believe that the inclusion of good paraphrases and the elimination of bad paraphrases during training should in principle improve word representation quality. However, the main questions are how well the proposed method achieves this goal, and, even if it achieves it well, whether it makes much difference in practical terms.----------------Regarding the first question, I am not entirely convinced that the parameterization of the control function f(x_ij)"":-0.6474308968}}",The reviewers agree that the paper's clarity and experimental evaluation can be improved.
https://openreview.net/forum?id=r1rhWnZkg,"['The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) for Visual Question Answering (VQA) The'
 'Results on the VQA task are good for this simple model. Missing are some explanations about the language embedding.'
 'The paper proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product. The full model archives a slight improvement over prior state-of-the-art.']","['The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) for Visual Question Answering (VQA) The'
 'Results on the VQA task are good for this simple model. Missing are some explanations about the language embedding.'
 'The paper proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product. The full model archives a slight improvement over prior state-of-the-art.']","The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) for Visual Question Answering (VQA) The                                         0.668659
Results on the VQA task are good for this simple model. Missing are some explanations about the language embedding.                                                                                     0.888594
The paper proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product. The full model archives a slight improvement over prior state-of-the-art.    0.815693
dtype: float32","The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) for Visual Question Answering (VQA) The                                         1.098612
Results on the VQA task are good for this simple model. Missing are some explanations about the language embedding.                                                                                     1.098612
The paper proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product. The full model archives a slight improvement over prior state-of-the-art.    1.098612
dtype: float32","{""The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) for Visual Question Answering (VQA) The"":{""Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3."":-2.5231556892,""Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. ----------------Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the--------paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible?  How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components"":-6.0799407959,""This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). --------This formulation is evaluated on the visual question answering (VQA) task together with several other model variants.----------------Strength:--------1. The paper discusses how the Hadamard product can be used to approximate the full outer product.--------2. The paper provides an extensive experimental evaluation of other model aspect for VQA.--------3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.----------------Weaknesses:--------1. Novelty: The paper presents only a new \u201cinterpretation\u201d of the Hadamard product which has previously been widely used for pooling, including for VQA.--------2. Experimental evaluation:--------2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar"":-4.7422561646},""Results on the VQA task are good for this simple model. Missing are some explanations about the language embedding."":{""Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3."":-13.5495815277,""Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. ----------------Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the--------paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible?  How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components"":-9.818441391,""This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). --------This formulation is evaluated on the visual question answering (VQA) task together with several other model variants.----------------Strength:--------1. The paper discusses how the Hadamard product can be used to approximate the full outer product.--------2. The paper provides an extensive experimental evaluation of other model aspect for VQA.--------3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.----------------Weaknesses:--------1. Novelty: The paper presents only a new \u201cinterpretation\u201d of the Hadamard product which has previously been widely used for pooling, including for VQA.--------2. Experimental evaluation:--------2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar"":-13.644153595},""The paper proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product. The full model archives a slight improvement over prior state-of-the-art."":{""Summary: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication). The paper implements low-rank bilinear pooling on an existing model (Kim et al., 2016b) and builds a model for Visual Question Answering (VQA) that outperforms the current state-of-art by 0.42%. The paper presents various ablation studies of the new VQA model they built.----------------Strengths:----------------1. The paper presents new insights into element-wise multiplication operation which has been previously used in VQA literature (such as Antol et al., ICCV 2015) without insights on why it should work. ----------------2. The paper presents a new model for the task of VQA that beats the current state-of-art by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).----------------3."":-3.9830875397,""Results on the VQA task are good for this simple model, the ablation study of table 1 gives some insights as to what is important. ----------------Missing are some explanations about the language embedding and the importance in deciding embedding dimension and final output dimension, equivalent to deciding the projected dimension in the compact bilinear model. Since the main contribution of the--------paper seems to be slightly better performance with fairly large reduction in parameters vs. compact bilinear something should be said about choice of those hyper parameters. If you increase embedded and output dimensions to equalize parameters to the compact bilinear model are further gains possible?  How is the question encoded? Is word order preserved in this encoding, the compact bilinear model compared to in table 1 mentions glove, the proposed model is using this as well? The meaning of visual attention in this model along with the number of glimpses should be tied to the sentence embedding, so now we are looking at particular spatial components"":-4.6654057503,""This work proposes to approximate the bilinear pooling (outer product) with a formulation which uses the Hadamard Product (element-wise product). --------This formulation is evaluated on the visual question answering (VQA) task together with several other model variants.----------------Strength:--------1. The paper discusses how the Hadamard product can be used to approximate the full outer product.--------2. The paper provides an extensive experimental evaluation of other model aspect for VQA.--------3. The full model archives a slight improvement over prior state-of-the-art on the challenging and large scale VQA challenge.----------------Weaknesses:--------1. Novelty: The paper presents only a new \u201cinterpretation\u201d of the Hadamard product which has previously been widely used for pooling, including for VQA.--------2. Experimental evaluation:--------2.1. An experimental direct comparison with MCB missing. Although the evaluated model is similar"":-0.9124623537}}","The program committee appreciates the authors' response to concerns raised in the reviews. While there are some concerns with the paper that the authors are strongly encouraged to address for the final version of the paper, overall, the work has contributions that are worth presenting at ICLR."
https://openreview.net/forum?id=r1tHvHKge,"['This paper presents a heuristic for avoiding large negative rewards. The paper is well written including some rather poetic language.'
 'The topic of keeping around highly rewarding or dangerous states is important. Overall, the approach presented is not very principled.'
 'The paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea)']","['This paper presents a heuristic for avoiding large negative rewards. The paper is well written including some rather poetic language.'
 'The topic of keeping around highly rewarding or dangerous states is important. Overall, the approach presented is not very principled.'
 'The paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea)']","This paper presents a heuristic for avoiding large negative rewards. The paper is well written including some rather poetic language.     0.777788
The topic of keeping around highly rewarding or dangerous states is important. Overall, the approach presented is not very principled.    0.944959
The paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea)                0.846544
dtype: float32","This paper presents a heuristic for avoiding large negative rewards. The paper is well written including some rather poetic language.     1.098612
The topic of keeping around highly rewarding or dangerous states is important. Overall, the approach presented is not very principled.    1.098612
The paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea)                1.098612
dtype: float32","{""This paper presents a heuristic for avoiding large negative rewards. The paper is well written including some rather poetic language."":{""This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \""danger model\"". The paper is well written including some rather poetic language [*].----------------The heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).----------------[*] this reviewer's favourite quotes:--------\""Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it\u2019s undesirable.\""--------\""The child can learn to adjust its behaviour without actually having to stab someone.\""--------\""... the catastrophe lurking just past the optimal shave.\"""":-2.3351259232,""- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. ----------------- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \""Reinforcement learning with unsupervised auxiliary tasks.\"", Blundell, Charles, et al. \""Model-free episodic control.\"" , Narasimhan et al. \""Language understanding for text-based games using deep reinforcement learning\""), which could perhaps reinforce the agent to avoid revisiting catastrophic states. ----------------- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? "":-5.7346253395,""This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).----------------1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.----------------2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for"":-5.3490552902},""The topic of keeping around highly rewarding or dangerous states is important. Overall, the approach presented is not very principled."":{""This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \""danger model\"". The paper is well written including some rather poetic language [*].----------------The heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).----------------[*] this reviewer's favourite quotes:--------\""Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it\u2019s undesirable.\""--------\""The child can learn to adjust its behaviour without actually having to stab someone.\""--------\""... the catastrophe lurking just past the optimal shave.\"""":-6.4016661644,""- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. ----------------- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \""Reinforcement learning with unsupervised auxiliary tasks.\"", Blundell, Charles, et al. \""Model-free episodic control.\"" , Narasimhan et al. \""Language understanding for text-based games using deep reinforcement learning\""), which could perhaps reinforce the agent to avoid revisiting catastrophic states. ----------------- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? "":-2.251553297,""This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).----------------1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.----------------2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for"":-6.4720845222},""The paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea)"":{""This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \""danger model\"". The paper is well written including some rather poetic language [*].----------------The heuristic is evaluated in two toy domains. I would think that in order to properly evaluate this one would use a well known benchmark e.g. Atari. Atari seems particularly apt since those games are full of catastrophes (i.e. sudden death).----------------[*] this reviewer's favourite quotes:--------\""Imagine a self-driving car that had to periodically hit a few pedestrians in order to remember that it\u2019s undesirable.\""--------\""The child can learn to adjust its behaviour without actually having to stab someone.\""--------\""... the catastrophe lurking just past the optimal shave.\"""":-4.3593201637,""- The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. ----------------- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \""Reinforcement learning with unsupervised auxiliary tasks.\"", Blundell, Charles, et al. \""Model-free episodic control.\"" , Narasimhan et al. \""Language understanding for text-based games using deep reinforcement learning\""), which could perhaps reinforce the agent to avoid revisiting catastrophic states. ----------------- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? "":-4.393799305,""This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).----------------1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.----------------2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for"":-0.8446894884}}","This paper presents a few interesting ideas, namely the idea of keeping around a set of ""danger states"" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.    However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.    Pros:  + Interesting idea of keeping around danger states and injecting them into training    Cons:  - Algorithm doesn't seem that well motivated  - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.  - Experiments aren't that convincing (better after revisions, but still need work)"
https://openreview.net/forum?id=r1xUYDYgg,"['The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable.'
 'While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience.'
 'This paper presents a JavaScript framework for training and deploying deep neural networks. It is possible to reach competitive speeds with this technology.']","['The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable.'
 'While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience.'
 'This paper presents a JavaScript framework for training and deploying deep neural networks. It is possible to reach competitive speeds with this technology.']","The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable.                                                              1.020755
While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience.       0.838241
This paper presents a JavaScript framework for training and deploying deep neural networks. It is possible to reach competitive speeds with this technology.    0.703450
dtype: float32","The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable.                                                              1.098612
While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience.       1.098612
This paper presents a JavaScript framework for training and deploying deep neural networks. It is possible to reach competitive speeds with this technology.    1.098612
dtype: float32","{""The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable."":{""Validity:--------The presented work seems technically valid. Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.----------------https:\/\/mil-tokyo.github.io\/sukiyaki2\/examples\/mnist\/ was fun, but seemed very slow (5 mnist images per second). The demo page would be more interesting if it showed what model was being trained, which implementation was being used (pure js or webcl?), which hardware was being used for the computation, and how that compared with other people who logged into the page. As it is, the demo is kind of unclear as to what is happening.----------------Relevance:----------------The grand vision of a DLTraining@Home is exciting. While much work remains, having a solid WebCL foundation seems valuable. The big advantage of javascript is that it runs everywhere, especially on idle desktops and laptops around the world"":-8.1975364685,""While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. --------There is no new algorithm here, nor is there any UI\/meta-design improvement to make it easier for non-experts to design and train neural network systems. ----------------I think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. --------I have no significant objection to the presentation or methodology of the paper. "":-13.7077188492,""This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs. While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.----------------My main points of criticism are:--------1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).----------------2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many \""low performance\"" clients, the setting"":-12.883026123},""While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience."":{""Validity:--------The presented work seems technically valid. Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.----------------https:\/\/mil-tokyo.github.io\/sukiyaki2\/examples\/mnist\/ was fun, but seemed very slow (5 mnist images per second). The demo page would be more interesting if it showed what model was being trained, which implementation was being used (pure js or webcl?), which hardware was being used for the computation, and how that compared with other people who logged into the page. As it is, the demo is kind of unclear as to what is happening.----------------Relevance:----------------The grand vision of a DLTraining@Home is exciting. While much work remains, having a solid WebCL foundation seems valuable. The big advantage of javascript is that it runs everywhere, especially on idle desktops and laptops around the world"":-4.216126442,""While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. --------There is no new algorithm here, nor is there any UI\/meta-design improvement to make it easier for non-experts to design and train neural network systems. ----------------I think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. --------I have no significant objection to the presentation or methodology of the paper. "":-0.8124203086,""This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs. While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.----------------My main points of criticism are:--------1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).----------------2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many \""low performance\"" clients, the setting"":-4.3885817528},""This paper presents a JavaScript framework for training and deploying deep neural networks. It is possible to reach competitive speeds with this technology."":{""Validity:--------The presented work seems technically valid. Code for matrix library sushi2 and DL library sukiyaki2 are on github, including live demos that run in your browser.----------------https:\/\/mil-tokyo.github.io\/sukiyaki2\/examples\/mnist\/ was fun, but seemed very slow (5 mnist images per second). The demo page would be more interesting if it showed what model was being trained, which implementation was being used (pure js or webcl?), which hardware was being used for the computation, and how that compared with other people who logged into the page. As it is, the demo is kind of unclear as to what is happening.----------------Relevance:----------------The grand vision of a DLTraining@Home is exciting. While much work remains, having a solid WebCL foundation seems valuable. The big advantage of javascript is that it runs everywhere, especially on idle desktops and laptops around the world"":-8.3948659897,""While it is interesting that this can be done, and it will be useful for some, it does seem like the audience is not really the mainstream ICLR audience, who will not be afraid to use a conventional ML toolkit. --------There is no new algorithm here, nor is there any UI\/meta-design improvement to make it easier for non-experts to design and train neural network systems. ----------------I think there will be relatively little interest at ICLR in such a paper that doesn't really advance the state of the art. --------I have no significant objection to the presentation or methodology of the paper. "":-8.0067081451,""This paper presents a JavaScript framework including WebCL components for training and deploying deep neural networks. The authors show that it is possible to reach competitive speeds with this technology, even higher speed than a compiled application with ViennaCL on AMD GPUs. While remaining a little more than factor three slower than compiled high performance software on NVIDIA GPUs, it offers compelling possibilities for easily deployable training and application settings for deep learning.----------------My main points of criticism are:--------1. In Tab. 4 different batch sizes are used. Even if this is due to technical limits for the Javascript library, it would only be fair to use the smaller batch sizes for the other frameworks as well (on the GPUs probably in favor of the presented framework).----------------2. In Fig. 6, why not include more information in the graphs? Especially, as stated in the question, why not include the node.js values? While I do see the possible application with one server and many \""low performance\"" clients, the setting"":-5.3017859459}}","In the § Related work: ""deeplearning4j 2 provides distributed computing of deep learning framework that runs on the distributed computing Hadoop. However, Hadoop must be installed in all computing nodes, thereby imposing high deployment and maintenance costs.""  This is inexact, Deeplearning4j's most basic mode of operation is on a single machine, with Java installed. A GPU is used if available but is not a requirement (Deeplearning4j documentation: https://deeplearning4j.org/quickstart )"
https://openreview.net/forum?id=r1y1aawlg,"['.'
 'This is an important idea that is not currently playing much of a role in neural net models. The model could be interpreted as a globally normalized, undirected translation model trained using a pseudo-likelihood objective.'
 'This paper is generally well written. drawings illustrating the architectures would help understand how the different algorithms relate to one another.'
 'This work proposes to improve a sentence that has been generated from another MT system. The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word. During testing, the gold words']","['.'
 'This is an important idea that is not currently playing much of a role in neural net models. The model could be interpreted as a globally normalized, undirected translation model trained using a pseudo-likelihood objective.'
 'This paper is generally well written. drawings illustrating the architectures would help understand how the different algorithms relate to one another.'
 'This work proposes to improve a sentence that has been generated from another MT system. The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word. During testing, the gold words']",".                                                                                                                                                                                                                                                      0.233761
This is an important idea that is not currently playing much of a role in neural net models. The model could be interpreted as a globally normalized, undirected translation model trained using a pseudo-likelihood objective.                        0.661944
This paper is generally well written. drawings illustrating the architectures would help understand how the different algorithms relate to one another.                                                                                                1.036118
This work proposes to improve a sentence that has been generated from another MT system. The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word. During testing, the gold words    1.127968
dtype: float32",".                                                                                                                                                                                                                                                      1.382488
This is an important idea that is not currently playing much of a role in neural net models. The model could be interpreted as a globally normalized, undirected translation model trained using a pseudo-likelihood objective.                        1.386284
This paper is generally well written. drawings illustrating the architectures would help understand how the different algorithms relate to one another.                                                                                                1.386294
This work proposes to improve a sentence that has been generated from another MT system. The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word. During testing, the gold words    1.386294
dtype: float32","{""."":{""This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.----------------The paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several"":-207.8465118408,""This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and\/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF"":-210.2633514404,""Disclosure: I am not an expert in machine translation algorithms.----------------Summary: A human translator does not come up with the final translation right--------away. Instead, (s)he uses an iterative process, starting with a rough draft--------which is corrected little by little. The idea behind this paper is to--------implement a similar framework for an automated system. ----------------This paper is generally well written. ----------------It is my opinion however that drawings illustrating the architectures would help--------understanding how the different algorithms relate to one another.----------------I like a lot that you report on a preliminary experiment to give an--------intuition of how difficult the task is. You should highlight the links--------between the task of finding the errors in a guess translation and the task--------of iterative refinement. Could you use post-edited text to have a more--------solid ground-truth?----------------My main concern with this paper is that in the experimental section the --------iterative approach tries to improve upon"":-208.5331573486,""This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.----------------Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \""I went to the fridge even though I was not hungry\"" to \""Although I was not hungry, I went to the fridge\""). The fact that only 0.6 words are edited on average supports this. ----------------Specific comments:--------- It would be interesting to see what the improvements are if the baseline model is a neural system.--------- It seems"":-207.6876831055},""This is an important idea that is not currently playing much of a role in neural net models. The model could be interpreted as a globally normalized, undirected translation model trained using a pseudo-likelihood objective."":{""This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.----------------The paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several"":-5.6853265762,""This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and\/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF"":-3.2887442112,""Disclosure: I am not an expert in machine translation algorithms.----------------Summary: A human translator does not come up with the final translation right--------away. Instead, (s)he uses an iterative process, starting with a rough draft--------which is corrected little by little. The idea behind this paper is to--------implement a similar framework for an automated system. ----------------This paper is generally well written. ----------------It is my opinion however that drawings illustrating the architectures would help--------understanding how the different algorithms relate to one another.----------------I like a lot that you report on a preliminary experiment to give an--------intuition of how difficult the task is. You should highlight the links--------between the task of finding the errors in a guess translation and the task--------of iterative refinement. Could you use post-edited text to have a more--------solid ground-truth?----------------My main concern with this paper is that in the experimental section the --------iterative approach tries to improve upon"":-5.9186887741,""This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.----------------Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \""I went to the fridge even though I was not hungry\"" to \""Although I was not hungry, I went to the fridge\""). The fact that only 0.6 words are edited on average supports this. ----------------Specific comments:--------- It would be interesting to see what the improvements are if the baseline model is a neural system.--------- It seems"":-5.6937379837},""This paper is generally well written. drawings illustrating the architectures would help understand how the different algorithms relate to one another."":{""This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.----------------The paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several"":-16.9343070984,""This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and\/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF"":-17.1411628723,""Disclosure: I am not an expert in machine translation algorithms.----------------Summary: A human translator does not come up with the final translation right--------away. Instead, (s)he uses an iterative process, starting with a rough draft--------which is corrected little by little. The idea behind this paper is to--------implement a similar framework for an automated system. ----------------This paper is generally well written. ----------------It is my opinion however that drawings illustrating the architectures would help--------understanding how the different algorithms relate to one another.----------------I like a lot that you report on a preliminary experiment to give an--------intuition of how difficult the task is. You should highlight the links--------between the task of finding the errors in a guess translation and the task--------of iterative refinement. Could you use post-edited text to have a more--------solid ground-truth?----------------My main concern with this paper is that in the experimental section the --------iterative approach tries to improve upon"":-13.5374240875,""This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.----------------Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \""I went to the fridge even though I was not hungry\"" to \""Although I was not hungry, I went to the fridge\""). The fact that only 0.6 words are edited on average supports this. ----------------Specific comments:--------- It would be interesting to see what the improvements are if the baseline model is a neural system.--------- It seems"":-17.3889160156},""This work proposes to improve a sentence that has been generated from another MT system. The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word. During testing, the gold words"":{""This paper proposes a method for iteratively improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution, in this case using an attention-based model.  It is motivated by the method in which (it is assumed) human translators operate.----------------The paper is interesting and imaginative.  However, in general terms, I am somewhat sceptical of this kind of approach -- whereby a machine learning method is used to identify and correct the predictions of another method, or itself -- because in the first case, if the new method is better, why not use it from the outset in place of the other method?  And in the second case, since the method has no new information compared to previously, why is it more likely to identify more past mistakes and correct them, than identify past correct terms and turn them into new errors?  That is unless there is a specific reason that an iterative approach can be shown to converge to a better solution when run over several"":-4.7711582184,""This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on \u201cleft context\u201d, but also on \u201cright context\u201d, and potentially enabling more rapid and\/or accurate decoding. The motivation given is that often translators (and text generators generally) use a process of refinement in generating outputs.----------------This is an important idea that is not currently playing much of a role in neural net models, so this paper is a welcome contribution. However, while I think this is an important first step, I do feel that the lack of in depth analysis suggests this paper is not quite ready for a final publication version. For example, there are many possible connections to prior work in NLP, MT, and other parts of ML that could better contextualize this work (see specifics below). More substantively, the model in Section 3 could be interpreted as a globally normalized, undirected (~CRF"":-4.7014017105,""Disclosure: I am not an expert in machine translation algorithms.----------------Summary: A human translator does not come up with the final translation right--------away. Instead, (s)he uses an iterative process, starting with a rough draft--------which is corrected little by little. The idea behind this paper is to--------implement a similar framework for an automated system. ----------------This paper is generally well written. ----------------It is my opinion however that drawings illustrating the architectures would help--------understanding how the different algorithms relate to one another.----------------I like a lot that you report on a preliminary experiment to give an--------intuition of how difficult the task is. You should highlight the links--------between the task of finding the errors in a guess translation and the task--------of iterative refinement. Could you use post-edited text to have a more--------solid ground-truth?----------------My main concern with this paper is that in the experimental section the --------iterative approach tries to improve upon"":-4.8799176216,""This work proposes to iteratively improve a sentence that has been generated from another MT system (in this case, a phrase-based system). The authors use a neural net that takes in the source sentence and a window of (gold) words around the current target word, and predicts the current target word. During testing, the gold words are replaced with the generated words. While this is an interesting area of research, I am not convinced by the proposed approach, and experimental evidence is lacking.----------------Under the current framework, it is all but impossible for the model to do anything more than a rudimentary word replacement (e.g. it cannot change \""I went to the fridge even though I was not hungry\"" to \""Although I was not hungry, I went to the fridge\""). The fact that only 0.6 words are edited on average supports this. ----------------Specific comments:--------- It would be interesting to see what the improvements are if the baseline model is a neural system.--------- It seems"":-0.764333725}}","The paper is an interesting first step, but the reviewers found flaws in the overall argument. Further, the method is not contextualized well enough in relation to prior work."
https://openreview.net/forum?id=rJEgeXFex,"['This is a well written, organized, and presented paper that I enjoyed reading. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture'
 'This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient. The problem is formulated as a multilabel sequence classification.'
 '. The authors compared GRUs, LSTMs, feed-forward networks and random forests.']","['This is a well written, organized, and presented paper that I enjoyed reading. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture'
 'This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient. The problem is formulated as a multilabel sequence classification.'
 '. The authors compared GRUs, LSTMs, feed-forward networks and random forests.']","This is a well written, organized, and presented paper that I enjoyed reading. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture    0.880336
This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient. The problem is formulated as a multilabel sequence classification.                                                 0.707405
. The authors compared GRUs, LSTMs, feed-forward networks and random forests.                                                                                                                                                          0.924296
dtype: float32","This is a well written, organized, and presented paper that I enjoyed reading. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture    1.098612
This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient. The problem is formulated as a multilabel sequence classification.                                                 1.098612
. The authors compared GRUs, LSTMs, feed-forward networks and random forests.                                                                                                                                                          1.098612
dtype: float32","{""This is a well written, organized, and presented paper that I enjoyed reading. Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture"":{""This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical\/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:-----------------The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that"":-0.6571606994,""In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.-------------------------------------This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and"":-4.3779459,""This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.----------------The authors also did address the questions of the reviewers.----------------My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR."":-4.3888607025},""This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient. The problem is formulated as a multilabel sequence classification."":{""This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical\/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:-----------------The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that"":-10.207775116,""In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.-------------------------------------This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and"":-7.47564888,""This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.----------------The authors also did address the questions of the reviewers.----------------My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR."":-10.568985939},"". The authors compared GRUs, LSTMs, feed-forward networks and random forests."":{""This is a well written, organized, and presented paper that I enjoyed reading.  I commend the authors on their attention to the narrative and the explanations.  While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes.  The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.  That said, the investigation of those results was not as deep as I thought it should have been in an empirical\/applications paper.  Despite their focus on the application, I was encouraged to see the authors use cutting edge choices (eg Keras, adadelta, etc) in their architecture.  A few points of criticism:-----------------The numerical results are in my view too brief.  Fig 4 is anecdotal, Fig 5 is essentially a negative result (tSNE is only in some places interpretable), so that"":-22.7632770538,""In light of the detailed author responses and further updates to the manuscript, I am raising my score to an 8 and reiterating my support for this paper. I think it will be among the strongest non-traditional applied deep learning work at ICLR and will receive a great deal of interest and attention from attendees.-------------------------------------This paper describes modern deep learning approach to the problem of predicting the medications taken by a patient during a period of time based solely upon the sequence of ICD-9 codes assigned to the patient during that same time period. This problem is formulated as a multilabel sequence classification (in contrast to language modeling, which is multiclass classification). They propose to use standard LSTM and GRU architectures with embedding layers to handle the sparse categorical inputs, similar to that described in related work by Choi, et al. In experiments using a cohort of ~610K patient records, they find that RNN models outperform strong baselines including an MLP and"":-21.9351921082,""This is a well-conducted and well-written study on the prediction of medication from diagnostic codes. The authors compared GRUs, LSTMs, feed-forward networks and random forests (making a case for why random forests should be used, instead of SVMs) and analysed the predictions and embeddings.----------------The authors also did address the questions of the reviewers.----------------My only negative point is that this work might be more relevant for a data science or medical venue rather than at ICLR."":-18.2642173767}}","This paper applies RNNs to predict medications from billing costs. While this paper does not have technical novelty, it is well done and well organized. It demonstrates a creative use of recent models in a very important domain, and I think many people in our community are interested and inspired by well-done applications that branch to socially important domains. Moreover, I think an advantage to accepting it at ICLR is that it gives our ""expert"" stamp of approval -- I see a lot of questionable / badly applied / antiquated machine learning methods in domain conferences, so I think it would be helpful for those domains to have examples of application papers that are considered sound."
https://openreview.net/forum?id=rJLS7qKel,"[""New algorithm was the winner of the Visual Doom AI competition. Uses low-dimensional 'measurements' for prediction, Monte Carlo estimation and parametrized goals.""
 'This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. I would have liked to see failure modes of this approach.'
 'The paper presents an on-policy method to predict future intrinsic measurements. The results are impressive, as this model won the 2016 vizDoom competition.']","[""New algorithm was the winner of the Visual Doom AI competition. Uses low-dimensional 'measurements' for prediction, Monte Carlo estimation and parametrized goals.""
 'This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. I would have liked to see failure modes of this approach.'
 'The paper presents an on-policy method to predict future intrinsic measurements. The results are impressive, as this model won the 2016 vizDoom competition.']","New algorithm was the winner of the Visual Doom AI competition. Uses low-dimensional 'measurements' for prediction, Monte Carlo estimation and parametrized goals.    0.349496
This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. I would have liked to see failure modes of this approach.              0.895607
The paper presents an on-policy method to predict future intrinsic measurements. The results are impressive, as this model won the 2016 vizDoom competition.          0.885287
dtype: float32","New algorithm was the winner of the Visual Doom AI competition. Uses low-dimensional 'measurements' for prediction, Monte Carlo estimation and parametrized goals.    1.098611
This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. I would have liked to see failure modes of this approach.              1.098612
The paper presents an on-policy method to predict future intrinsic measurements. The results are impressive, as this model won the 2016 vizDoom competition.          1.098612
dtype: float32","{""New algorithm was the winner of the Visual Doom AI competition. Uses low-dimensional 'measurements' for prediction, Monte Carlo estimation and parametrized goals."":{""Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition.----------------The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.----------------There have been a number"":-2.5555875301,""This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. ----------------- The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract\/intro----------------- I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past. ----------------- Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)----------------Overall, I think this paper does show clear empirical advantage of using"":-4.4763889313,""The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win\/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \""goal\"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform\/guide exploration. At test time, one can act by maximizing the long term goal only.----------------The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:-------- - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal"":-4.1442241669},""This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. I would have liked to see failure modes of this approach."":{""Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition.----------------The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.----------------There have been a number"":-7.4321613312,""This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. ----------------- The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract\/intro----------------- I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past. ----------------- Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)----------------Overall, I think this paper does show clear empirical advantage of using"":-3.3444905281,""The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win\/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \""goal\"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform\/guide exploration. At test time, one can act by maximizing the long term goal only.----------------The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:-------- - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal"":-6.9450349808},""The paper presents an on-policy method to predict future intrinsic measurements. The results are impressive, as this model won the 2016 vizDoom competition."":{""Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition.----------------The key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.----------------There have been a number"":-6.1930131912,""This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. ----------------- The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract\/intro----------------- I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past. ----------------- Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)----------------Overall, I think this paper does show clear empirical advantage of using"":-6.4165353775,""The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win\/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \""goal\"" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform\/guide exploration. At test time, one can act by maximizing the long term goal only.----------------The results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:-------- - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal"":-2.5434412956}}","This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-written paper."
https://openreview.net/forum?id=rJRhzzKxl,"['This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset.'
 'The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The idea is straight-forward, albeit fairly heuristic'
 'Paper describes technique for leveraging multiple teachers in teacher-student framework. Central idea relies on using similarity measure between source and target domains. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.']","['This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset.'
 'The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The idea is straight-forward, albeit fairly heuristic'
 'Paper describes technique for leveraging multiple teachers in teacher-student framework. Central idea relies on using similarity measure between source and target domains. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.']","This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset.                                                                                                   0.940240
The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The idea is straight-forward, albeit fairly heuristic                   0.909123
Paper describes technique for leveraging multiple teachers in teacher-student framework. Central idea relies on using similarity measure between source and target domains. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.    0.967469
dtype: float32","This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset.                                                                                                   1.098612
The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The idea is straight-forward, albeit fairly heuristic                   1.098612
Paper describes technique for leveraging multiple teachers in teacher-student framework. Central idea relies on using similarity measure between source and target domains. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.    1.098612
dtype: float32","{""This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset."":{""This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset.----------------The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.----------------I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).----------------I think this paper would make an interesting ICLR paper."":-8.9583721161,""The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements. To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model. ----------------The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source  models, and performs comparable to the proposed method. --------Questions: --------1. Why did you choose to use different combination schemes in equation (3) and (5)? For example,"":-12.9248743057,""Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation. The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.----------------What exactly constitutes the learned representation h used in MCD measure? I assume those are the top level pre-softmax activations - is this the case? Those tend to be typically more task related, would not the intermediate ones work better?----------------One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework. As such, approach is rater constrained in scope.-------- --------Authors shall refer in the related-work to similar ideas proposed in the"":-13.3120594025},""The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The idea is straight-forward, albeit fairly heuristic"":{""This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset.----------------The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.----------------I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).----------------I think this paper would make an interesting ICLR paper."":-4.5393433571,""The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements. To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model. ----------------The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source  models, and performs comparable to the proposed method. --------Questions: --------1. Why did you choose to use different combination schemes in equation (3) and (5)? For example,"":-0.8096708655,""Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation. The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.----------------What exactly constitutes the learned representation h used in MCD measure? I assume those are the top level pre-softmax activations - is this the case? Those tend to be typically more task related, would not the intermediate ones work better?----------------One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework. As such, approach is rater constrained in scope.-------- --------Authors shall refer in the related-work to similar ideas proposed in the"":-4.9354319572},""Paper describes technique for leveraging multiple teachers in teacher-student framework. Central idea relies on using similarity measure between source and target domains. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis."":{""This paper studies the problem of transfer learning in the context of domain adaptation. They propose to study it in the framework of knowledge distillation. Several settings are presented along with experiments on the Amazon Reviews dataset.----------------The paper is nicely written and the problem studied is very important towards progress in AI. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. Of course, the experimental setting is rather limited but the benchmarks are competitive enough to be meaningful.----------------I had concerns regarding discussion of previous work but the extensive responses helped clarify this point (the authors should turn the arguments used in this thread into an appendix).----------------I think this paper would make an interesting ICLR paper."":-7.2670230865,""The work extends knowledge distillation to domain adaptation scenario, the student model (for the target domain) is learned to mimic the prediction of the teacher model, learned on the source domain. The authors extends the idea to multi-source domain settings, proposing to weight predictions of teacher model using several domain similarity measurements. To improve the performance of proposed method when only a single source domain is available, the authors propose to use maximum cluster difference to inject pseudo-supervised examples labeled by the teacher model to train the student model. ----------------The paper is well written and easy to follow. The idea is straight-forward, albeit fairly heuristic in several cases. It is not clear what is the advantage of the proposed method vs existing feature learning techniques for domain adaptation, which also does not require re-train source  models, and performs comparable to the proposed method. --------Questions: --------1. Why did you choose to use different combination schemes in equation (3) and (5)? For example,"":-6.5794825554,""Paper describes technique for leveraging multiple teachers in teacher-student framework and performing unsupervised and semi-supervised domain adaptation. The central idea relies on using similarity measure between source and target domains to weight the corresponding trustfulness of particular teachers, when using their prediction as soft targets in student training. Authors provide an experimental validation on a single benchmark corpora for sentiment analysis.----------------What exactly constitutes the learned representation h used in MCD measure? I assume those are the top level pre-softmax activations - is this the case? Those tend to be typically more task related, would not the intermediate ones work better?----------------One not entirely clear aspect concerns types of distributions applicable to proposed learning - it assumes the vocabulary (or decision space) between tasks spans same categories, as otherwise one cannot derive the KL based objective, often used in TS framework. As such, approach is rater constrained in scope.-------- --------Authors shall refer in the related-work to similar ideas proposed in the"":-2.4922916889}}","The paper introduces a reasonable but somewhat heurstic and straightforward approach to domain adaptation. Especially, since the approach is not so principled, it does not seem sufficient to evaluate it on a single benchmark (document classification, specifically sentiment polarity prediction).     + the results on the sentiment dataset are strong  + the paper is easy to follow    - relatively straightforward  - the novel aspects are a bit heuristic  - extra evaluation is needed"
https://openreview.net/forum?id=rJXTf9Bxg,"['This submission proposes method for class-conditional generative image modeling. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution.'
 '. They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples. They show this metric correlates with a discriminability metric ('
 'This paper introduces a class-conditional GAN as a generative model for images. Experiments are conducted on the CIFAR-10 and ImageNet datasets.']","['This submission proposes method for class-conditional generative image modeling. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution.'
 '. They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples. They show this metric correlates with a discriminability metric ('
 'This paper introduces a class-conditional GAN as a generative model for images. Experiments are conducted on the CIFAR-10 and ImageNet datasets.']","This submission proposes method for class-conditional generative image modeling. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution.                     0.894252
. They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples. They show this metric correlates with a discriminability metric (    0.850787
This paper introduces a class-conditional GAN as a generative model for images. Experiments are conducted on the CIFAR-10 and ImageNet datasets.                                                                       0.668799
dtype: float32","This submission proposes method for class-conditional generative image modeling. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution.                     1.098612
. They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples. They show this metric correlates with a discriminability metric (    1.098612
This paper introduces a class-conditional GAN as a generative model for images. Experiments are conducted on the CIFAR-10 and ImageNet datasets.                                                                       1.098612
dtype: float32","{""This submission proposes method for class-conditional generative image modeling. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution."":{""Apologies for the late review.----------------This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.----------------Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).----------------The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).----------------Discriminability:"":-3.1082527637,""This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows:--------- Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real\/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.--------- Training different models on different subsets of imagenet classes improves performance.--------- They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)--------- They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .----------------The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the"":-7.2814660072,""This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.----------------Pros:--------+ The paper is clear and well-written.--------+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.--------+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.----------------Cons:--------- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.--------- Diversity metric is of limited use for training non class-conditional GANs.--------- No experimental comparison of AC-GAN to other class-conditional models.----------------To my knowledge training GANs"":-6.6374487877},"". They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples. They show this metric correlates with a discriminability metric ("":{""Apologies for the late review.----------------This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.----------------Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).----------------The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).----------------Discriminability:"":-4.9026136398,""This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows:--------- Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real\/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.--------- Training different models on different subsets of imagenet classes improves performance.--------- They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)--------- They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .----------------The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the"":-1.0168902874,""This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.----------------Pros:--------+ The paper is clear and well-written.--------+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.--------+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.----------------Cons:--------- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.--------- Diversity metric is of limited use for training non class-conditional GANs.--------- No experimental comparison of AC-GAN to other class-conditional models.----------------To my knowledge training GANs"":-4.3036851883},""This paper introduces a class-conditional GAN as a generative model for images. Experiments are conducted on the CIFAR-10 and ImageNet datasets."":{""Apologies for the late review.----------------This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the (real or sampled) class label corresponding to the sample.----------------Figure 2. nicely illustrates related methods - this particular method bears similarities to InfoGANs and Semi-supervised GANs. Compared to infogans, this method also encourages correspondence between the latent c and the real class labels for the real examples (whereas infogans are presented as fully unsupervised).----------------The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples. It is found - not surprisingly - that higher resolution improves discriminability (because more information is present).----------------Discriminability:"":-6.0713324547,""This is a clear, easy to read, highly relevant paper that improves GAN training for images and explores evaluation criterion on GANs. The main contributions are as follows:--------- Adding an auxiliary classifier head to a GAN discriminator and training a classification objective in addition to the real\/fake objective improves performance. Generator is conditioned on 1-hot encoding of class and is trained to generate the specified class.--------- Training different models on different subsets of imagenet classes improves performance.--------- They motivate evaluating GAN images by using a perceptual similarity metric (MS-SSIM) on pairs of samples to quantify diversity in the samples (and detect mode collapse)--------- They show this metric correlates with a discriminability metric (classification accuracy of pre-trained imagenet model on generated samples) .----------------The overall novelty of this approach is somewhat lacking in that previous methods have proposed training a classifier head on the discriminator and the discriminability metric proposed is simply the"":-6.57725811,""This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.----------------Pros:--------+ The paper is clear and well-written.--------+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.--------+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.----------------Cons:--------- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.--------- Diversity metric is of limited use for training non class-conditional GANs.--------- No experimental comparison of AC-GAN to other class-conditional models.----------------To my knowledge training GANs"":-3.5472593307}}","Ratings summary:  3: Clear rejection  6: Marginally above acceptance threshold  6: Marginally above acceptance threshold    Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The authorÕs point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.    Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference."
https://openreview.net/forum?id=rJfMusFll,"['This paper extends neural conversational models into the batch reinforcement learning setting. The primary dataset used is very small (6000 conversations)'
 'The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.'
 'The paper discuss a ""batch"" method for RL setup to improve chat-bots.']","['This paper extends neural conversational models into the batch reinforcement learning setting. The primary dataset used is very small (6000 conversations)'
 'The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.'
 'The paper discuss a ""batch"" method for RL setup to improve chat-bots.']","This paper extends neural conversational models into the batch reinforcement learning setting. The primary dataset used is very small (6000 conversations)    1.000908
The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.                                                        0.702429
The paper discuss a ""batch"" method for RL setup to improve chat-bots.                                                                                         0.745352
dtype: float32","This paper extends neural conversational models into the batch reinforcement learning setting. The primary dataset used is very small (6000 conversations)    1.098612
The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.                                                        1.098612
The paper discuss a ""batch"" method for RL setup to improve chat-bots.                                                                                         1.098612
dtype: float32","{""This paper extends neural conversational models into the batch reinforcement learning setting. The primary dataset used is very small (6000 conversations)"":{""This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.----------------While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. ----------------My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able"":-1.0071017742,""The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.--------The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \""clarification regarding batch vs. online setting\"").--------The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. "":-5.6267008781,""The paper discuss a \""batch\"" method for RL setup to improve chat-bots.--------The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. ----------------I find the writing clear, and the algorithm a natural extension of the online version.----------------Below are some constructive remarks:--------- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:--------- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.--------- section 2.2:--------   sentence before last"":-5.9093322754},""The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots."":{""This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.----------------While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. ----------------My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able"":-3.084600687,""The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.--------The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \""clarification regarding batch vs. online setting\"").--------The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. "":-0.6434992552,""The paper discuss a \""batch\"" method for RL setup to improve chat-bots.--------The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. ----------------I find the writing clear, and the algorithm a natural extension of the online version.----------------Below are some constructive remarks:--------- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:--------- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.--------- section 2.2:--------   sentence before last"":-4.1380329132},""The paper discuss a \""batch\"" method for RL setup to improve chat-bots."":{""This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Thus, it is natural to use off-policy learning \u2013 training a base policy on unsupervised data, deploying that policy to collect human scores, and then learning off-line from those scores.----------------While the overall contribution is modest (extending off-policy actor-critic to the application of dialogue generation), the approach is well-motivated, and the paper is written clearly and is easy to understand. ----------------My main concern is that the primary dataset used (restaurant recommendations) is very small (6000 conversations). In fact, it is several orders of magnitude smaller than other datasets used in the literature (e.g. Twitter, the Ubuntu Dialogue Corpus) for dialogue generation. It is a bit surprising to me that RNN chatbots (with no additional structure) are able"":-9.7895460129,""The author propose to use a off-policy actor-critic algorithm in a batch-setting to improve chat-bots.--------The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version (see comments on \""clarification regarding batch vs. online setting\"").--------The artificial experiments are instructive, and the real-world experiments were performed very thoroughly although the results show only modest improvement. "":-8.9358959198,""The paper discuss a \""batch\"" method for RL setup to improve chat-bots.--------The authors provide nice overview of the RL setup they are using and present an algorithm which is similar to previously published on line setup for the same problem. They make a comparison to the online version and explore several modeling choices. ----------------I find the writing clear, and the algorithm a natural extension of the online version.----------------Below are some constructive remarks:--------- Comparison of the constant vs. per-state value function: In the artificial experiment there was no difference between the two while on the real-life task there was. It will be good to understand why, and add this to the discussion. Here is one option:--------- For the artificial task it seems like you are giving the constant value function an unfair advantage, as it can update all the weights of the model, and not just the top layer, like the per-state value function.--------- section 2.2:--------   sentence before last"":-6.2500691414}}","This is an interesting and timely paper combining off-policy learning with seq2seq models to train a chatbot on a restaurant reservation task, using labels collected through Amazon Mechanical Turk while using the bot with a baseline maximum likelihood policy.   The paper is clear, well-written and well-executed. Although the improvements are modest and the actual novelty of the paper is limited (combining known pieces in a rather straightforward way), this is still an interesting and informative read, and will probably be of interest to many people at ICLR."
https://openreview.net/forum?id=rJo9n9Feg,"['CNN is trained from a visual rendering of the game board to these 18 possible outputs.'
 'This paper presents tic-tac-toe as toy problem for investigating CNNs. A CNN is trained to label boards according to the player who can win (2 choices) and the position they may move (9 choices) to win'
 '1029 boards are legal boards where the next legal play can end the game. CNN is trained to predict these 18 categories and can do so with 100% accuracy.']","['CNN is trained from a visual rendering of the game board to these 18 possible outputs.'
 'This paper presents tic-tac-toe as toy problem for investigating CNNs. A CNN is trained to label boards according to the player who can win (2 choices) and the position they may move (9 choices) to win'
 '1029 boards are legal boards where the next legal play can end the game. CNN is trained to predict these 18 categories and can do so with 100% accuracy.']","CNN is trained from a visual rendering of the game board to these 18 possible outputs.                                                                                                                       0.889786
This paper presents tic-tac-toe as toy problem for investigating CNNs. A CNN is trained to label boards according to the player who can win (2 choices) and the position they may move (9 choices) to win    0.718112
1029 boards are legal boards where the next legal play can end the game. CNN is trained to predict these 18 categories and can do so with 100% accuracy.                                                     0.849860
dtype: float32","CNN is trained from a visual rendering of the game board to these 18 possible outputs.                                                                                                                       1.098612
This paper presents tic-tac-toe as toy problem for investigating CNNs. A CNN is trained to label boards according to the player who can win (2 choices) and the position they may move (9 choices) to win    1.098612
1029 boards are legal boards where the next legal play can end the game. CNN is trained to predict these 18 categories and can do so with 100% accuracy.                                                     1.098612
dtype: float32","{""CNN is trained from a visual rendering of the game board to these 18 possible outputs."":{""Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. ----------------Authors claim that this:--------1. is a very interesting finding. --------2. CNN has figured out game rules. --------3. Cross modal supervision is applicable to higher-level semantics. ----------------I don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \""one\"" stage of a game - i.e. last move that is considered. Further, the results are on the"":-21.8179016113,""Summary--------===--------This paper presents tic-tac-toe as toy problem for investigating CNNs.--------A dataset is created containing tic-tac-toe boards where one player is one--------move away from winning and a CNN is trained to label boards according--------to (1) the player who can win (2 choices) and (2) the position they may move--------to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper--------performs perfectly at the task and the paper's goal is to inspect how the--------CNN works.----------------The fundamental mechanism for this inspection is Class Activation--------Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention--------in the CNN. These implicit attention maps (localization heat maps) are used to--------derive actions (which square each player should move). The attention maps  ----------------(1) attend to squares in the tic-tac"":-25.5422725677,""1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \""If you place a black square in the middle right, black will win\"" or \""if you place a white square in the upper left, white will win\"". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.----------------The focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more"":-25.6666908264},""This paper presents tic-tac-toe as toy problem for investigating CNNs. A CNN is trained to label boards according to the player who can win (2 choices) and the position they may move (9 choices) to win"":{""Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. ----------------Authors claim that this:--------1. is a very interesting finding. --------2. CNN has figured out game rules. --------3. Cross modal supervision is applicable to higher-level semantics. ----------------I don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \""one\"" stage of a game - i.e. last move that is considered. Further, the results are on the"":-3.941439867,""Summary--------===--------This paper presents tic-tac-toe as toy problem for investigating CNNs.--------A dataset is created containing tic-tac-toe boards where one player is one--------move away from winning and a CNN is trained to label boards according--------to (1) the player who can win (2 choices) and (2) the position they may move--------to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper--------performs perfectly at the task and the paper's goal is to inspect how the--------CNN works.----------------The fundamental mechanism for this inspection is Class Activation--------Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention--------in the CNN. These implicit attention maps (localization heat maps) are used to--------derive actions (which square each player should move). The attention maps  ----------------(1) attend to squares in the tic-tac"":-0.9889609814,""1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \""If you place a black square in the middle right, black will win\"" or \""if you place a white square in the upper left, white will win\"". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.----------------The focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more"":-3.9254448414},""1029 boards are legal boards where the next legal play can end the game. CNN is trained to predict these 18 categories and can do so with 100% accuracy."":{""Game of tic-tac-toe is considered. 1029 tic-tac-toe board combinations are chosen so that a single move will result into victory of either the black or the white player. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations. ----------------Authors claim that this:--------1. is a very interesting finding. --------2. CNN has figured out game rules. --------3. Cross modal supervision is applicable to higher-level semantics. ----------------I don't think (2) be can be claimed because the knowledge of game rules is not tested by any experiment. There is only \""one\"" stage of a game - i.e. last move that is considered. Further, the results are on the"":-9.1895532608,""Summary--------===--------This paper presents tic-tac-toe as toy problem for investigating CNNs.--------A dataset is created containing tic-tac-toe boards where one player is one--------move away from winning and a CNN is trained to label boards according--------to (1) the player who can win (2 choices) and (2) the position they may move--------to win (9 choices), resulting in 18 labels. The CNN evaluated in this paper--------performs perfectly at the task and the paper's goal is to inspect how the--------CNN works.----------------The fundamental mechanism for this inspection is Class Activation--------Mapping (CAM) (Zhou et. al. 2016), which identifies regions of implicit attention--------in the CNN. These implicit attention maps (localization heat maps) are used to--------derive actions (which square each player should move). The attention maps  ----------------(1) attend to squares in the tic-tac"":-9.6704311371,""1029 tic-tac-toe boards are rendered (in various ways). These 1029 boards are legal boards where the next legal play can end the game. There are 18 categories of such boards -- 9 for the different locations of the next play, and 2 for the color of the next play. The supervision is basically saying \""If you place a black square in the middle right, black will win\"" or \""if you place a white square in the upper left, white will win\"". A CNN is trained to predict these 18 categories and can do so with 100% accuracy.----------------The focus of the paper is using Zhou et al's Class Activation Mapping to show where the CNN focuses when making it's decision. As I understand it, an input to CAM is the class of interest. So let's say it is class 1 (black wins with a play to the bottom right square, if I've deciphered figure 2 correctly. Figure 2 should really be more"":-5.8599252701}}","The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work."
https://openreview.net/forum?id=rJzaDdYxx,"['Interior gradients for analysing feature importance in deep neural networks.'
 'The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The paper presents a very simple idea, it is far too long.'
 '. Overall, this presented visualizations are interesting, however the approach is very ad hoc.']","['Interior gradients for analysing feature importance in deep neural networks.'
 'The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The paper presents a very simple idea, it is far too long.'
 '. Overall, this presented visualizations are interesting, however the approach is very ad hoc.']","Interior gradients for analysing feature importance in deep neural networks.                                                                                                                                0.762561
The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The paper presents a very simple idea, it is far too long.    0.452898
. Overall, this presented visualizations are interesting, however the approach is very ad hoc.                                                                                                              0.803780
dtype: float32","Interior gradients for analysing feature importance in deep neural networks.                                                                                                                                1.098612
The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The paper presents a very simple idea, it is far too long.    1.098612
. Overall, this presented visualizations are interesting, however the approach is very ad hoc.                                                                                                              1.098612
dtype: float32","{""Interior gradients for analysing feature importance in deep neural networks."":{""This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.----------------While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-"":-25.4236888885,""The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images \u201ccounterfactuals\u201d which seems like quite an"":-28.2874126434,""This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. ----------------One particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?----------------With regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.----------------Finally, does this model use batch normalization?"":-28.8702869415},""The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. The paper presents a very simple idea, it is far too long."":{""This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.----------------While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-"":-3.7254374027,""The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images \u201ccounterfactuals\u201d which seems like quite an"":-1.7534812689,""This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. ----------------One particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?----------------With regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.----------------Finally, does this model use batch normalization?"":-3.9033734798},"". Overall, this presented visualizations are interesting, however the approach is very ad hoc."":{""This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks.  The interior gradient is the gradient measured on a scaled version of the input.  The integrated gradient is the integral of interior gradients over all scaling factors.  Visualizations comparing integrated gradients with standard gradients on real images input to the Inception CNN show that integrated gradients correspond to an intuitive notion of feature importance.----------------While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.  Only the baseline (simply the standard gradient) is presented as reference for qualitative comparison.  Yet, the paper cites numerous other works (DeepLift, layer-wise relevance propagation, guided backpropagation) that all attack the same problem of feature importance.  Lack of comparison to any of these methods is a major weakness of the paper.  I do not believe it is fit for publication without such comparisons.  My pre-"":-20.3265666962,""The authors propose to measure \u201cfeature importance\u201d, or specifically, which pixels contribute most to a network\u2019s classification of an image. A simple (albeit not particularly effective) heuristic for measuring feature importance is to measure the gradients of the predicted class wrt each pixel in an input image I. This assigns a score to each pixel in I (that ranks how much the output prediction would change if a given pixel were to change). In this paper, the authors build on this and propose to measure feature importance by computing gradients of the output wrt scaled version of the input image, alpha*I, where alpha is a scalar between 0 and 1, then summing across all values of alpha to obtain their feature importance score. Here the scaling is simply linear scaling of the pixel values (alpha=0 is all black image, alpha=1 is original image). The authors call these scaled images \u201ccounterfactuals\u201d which seems like quite an"":-20.9612159729,""This work proposes to use visualization of gradients to further understand the importance of features (i.e. pixels) for visual classification. Overall, this presented visualizations are interesting, however, the approach is very ad hoc. The authors do not explain why visualizing regular gradients isn't correlated with the importance of features relevant to the given visual category and proceed to the interior gradient approach. ----------------One particular question with regular gradients at features that form the spatial support of the visual class. Is it the case that the gradients of the features that are confident of the prediction remain low, while those with high uncertainty will have strong gradients?----------------With regards to the interior gradients, it is unclear how the scaling parameter \\alpha affects the feature importance and how it is related to attention.----------------Finally, does this model use batch normalization?"":-17.2955532074}}","This paper was reviewed by 3 experts. All 3 seem unconvinced of the contributions, point to several shortcomings, and recommend rejection. I see no basis for overturning their recommendation. To be clear, the problem of achieving insight into the inner workings of deep networks is of significant importance and I encourage the authors to use the feedback to improve the manuscript."
https://openreview.net/forum?id=rkGabzZgl,"['The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a'
 'This paper introduces dropout as a latent variable model (LVM) Leveraging this formulation authors analyze the dropout “inference gap’. They propose use of per-sample based inference gap as a regularizer.'
 ': ""Dropout as a bayesian approximation""']","['The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a'
 'This paper introduces dropout as a latent variable model (LVM) Leveraging this formulation authors analyze the dropout “inference gap’. They propose use of per-sample based inference gap as a regularizer.'
 ': ""Dropout as a bayesian approximation""']","The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a    0.908794
This paper introduces dropout as a latent variable model (LVM) Leveraging this formulation authors analyze the dropout “inference gap’. They propose use of per-sample based inference gap as a regularizer.                                               0.854270
: ""Dropout as a bayesian approximation""                                                                                                                                                                                                                    0.834056
dtype: float32","The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a    1.098612
This paper introduces dropout as a latent variable model (LVM) Leveraging this formulation authors analyze the dropout “inference gap’. They propose use of per-sample based inference gap as a regularizer.                                               1.098612
: ""Dropout as a bayesian approximation""                                                                                                                                                                                                                    1.098612
dtype: float32","{""The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a"":{""summary----------------The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.----------------The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).--------This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.----------------Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.----------------"":-0.8299511671,""This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.----------------One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic"":-4.6715660095,""This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.----------------Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.-------- "":-4.8142437935},""This paper introduces dropout as a latent variable model (LVM) Leveraging this formulation authors analyze the dropout \u201cinference gap\u2019. They propose use of per-sample based inference gap as a regularizer."":{""summary----------------The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.----------------The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).--------This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.----------------Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.----------------"":-4.6457104683,""This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.----------------One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic"":-1.4026588202,""This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.----------------Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.-------- "":-5.4098796844},"": \""Dropout as a bayesian approximation\"""":{""summary----------------The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.----------------The paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).--------This framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.----------------Finally a new regularisation term is introduced to account for minimisation of the inference gap during learning.----------------"":-44.6274299622,""This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.----------------One relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic"":-44.671497345,""This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.----------------Their proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.-------- "":-41.1838951111}}","This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper ""Variational Dropout and the Local Reparameterization Trick"" by Diederik P. Kingma, Tim Salimans, Max Welling."
https://openreview.net/forum?id=rkKCdAdgx,"['This paper aims to compress binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings""'
 'Bloom filter encodings could be used to reduce the size of the network. The advantage of the Bloom filter approach is its simplicity and its wide applicability.'
 ""The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. The idea is simple enough to implement that it can easily be added to any practitioner's toolbox.""]","['This paper aims to compress binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings""'
 'Bloom filter encodings could be used to reduce the size of the network. The advantage of the Bloom filter approach is its simplicity and its wide applicability.'
 ""The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. The idea is simple enough to implement that it can easily be added to any practitioner's toolbox.""]","This paper aims to compress binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings""                                                                                            0.738017
Bloom filter encodings could be used to reduce the size of the network. The advantage of the Bloom filter approach is its simplicity and its wide applicability.                                               0.094354
The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. The idea is simple enough to implement that it can easily be added to any practitioner's toolbox.    0.576378
dtype: float32","This paper aims to compress binary inputs and outputs of neural network models with unsupervised ""Bloom embeddings""                                                                                            1.098612
Bloom filter encodings could be used to reduce the size of the network. The advantage of the Bloom filter approach is its simplicity and its wide applicability.                                               1.098450
The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. The idea is simple enough to implement that it can easily be added to any practitioner's toolbox.    1.098612
dtype: float32","{""This paper aims to compress binary inputs and outputs of neural network models with unsupervised \""Bloom embeddings\"""":{""Description:----------------This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \""Bloom embeddings\"".----------------The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.----------------Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs.----------------Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.----------------The paper simply applies this idea, testing it"":-9.8140439987,""The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \""Hash Kernels for Structured Data\"" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \""Robust Bloom Filters for Large MultiLabel Classification Tasks\"". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.----------------The main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow"":-12.7697610855,""The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.----------------Pros:----------------- Can be applied to practically any model, either at the input or hte output.--------- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. ----------------Cons:----------------- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.--------- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.----------------This seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental."":-12.910323143},""Bloom filter encodings could be used to reduce the size of the network. The advantage of the Bloom filter approach is its simplicity and its wide applicability."":{""Description:----------------This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \""Bloom embeddings\"".----------------The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.----------------Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs.----------------Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.----------------The paper simply applies this idea, testing it"":-5.6517438889,""The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \""Hash Kernels for Structured Data\"" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \""Robust Bloom Filters for Large MultiLabel Classification Tasks\"". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.----------------The main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow"":-4.7105565071,""The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.----------------Pros:----------------- Can be applied to practically any model, either at the input or hte output.--------- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. ----------------Cons:----------------- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.--------- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.----------------This seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental."":-5.5133442879},""The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. The idea is simple enough to implement that it can easily be added to any practitioner's toolbox."":{""Description:----------------This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \""Bloom embeddings\"".----------------The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.----------------Inputs and outputs are assumed to be sparse. The nonzero elements of an input are then simply encoded by Bloom filters onto the same binary array. The neural network is run with the embedded inputs.----------------Desired outputs are assumed to be a softmax-type ranking of different alternatives. a sort of back-projection step is needed to recover a probability ranking of the desired ground truth alternatives from the lower-dimensional output. For each ground-truth class, this is simply approximated as a product of the output values at the Bloom-filtered hash positions of that class.----------------The paper simply applies this idea, testing it"":-2.9707837105,""The paper presents the idea of using Bloom filter encodings on the input features and the output layer of deep network to reduce the size of the network. Bloom-filter like encodings were proposed in Shi et al. (JMLR 2009) \""Hash Kernels for Structured Data\"" (see Theorem 2 and its proof), whereas it was proposed to encode the outputs in the context of multilabel classification in Cisse et al. (NIPS 2013) \""Robust Bloom Filters for Large MultiLabel Classification Tasks\"". The paper still joins both ideas together, applies it to ranking problems, and presents extensive experiments in the context of deep neural networks and language modelling.----------------The main motivation of the paper is the reduction of model size for deep neural networks. There is a whole body of literature on this topic,  that is not mentioned at all in the paper, where the baselines are based on weight quantization (with an available implementation in TensorFlow"":-3.1506931782,""The authors propose a simple scheme based on Bloom filters to generate embeddings for inputs and outputs.  This reduces memory while introducing limited computational overhead, and it is simple enough to implement that it can easily be added to any practitioner's toolbox.----------------Pros:----------------- Can be applied to practically any model, either at the input or hte output.--------- Method is very straightforward: apply multiple hashes, instead of single one. The algorithm for decoding is nice too. ----------------Cons:----------------- The paper is a bit difficult to read; the idea is really simple so it doesn't seem like it warrants such a complex description.--------- The novelty of the approach is a bit limited since, as other reviewers have mentioned, Bloom filters are in use in a lot of areas including multi-label classification.----------------This seems like it would be a good short paper. There's a lot of well fleshed out experiments, but the core of the paper is a bit incremental."":-0.6164944768}}","The authors present a simple scheme based on Bloom filters to generate embeddings for inputs and outputs. On the one hand, the scheme is simple, it reduces memory while introducing limited computational overhead. On the other hand, reviewers were concerned with the limited novelty of the approach, which diminishes quite a bit its impact. Overall, this paper is just a pinch too borderline."
https://openreview.net/forum?id=rkpACe1lx,"['A paper proposes to use different parameters for different types of examples. Using different parameters for different types of examples has the potential to greatly reduce underfitting.'
 'Although the trainable parameters might be reduced, unfortunately the training and recognition speech cannot be reduced in this way.'
 'The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task.'
 'The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation.']","['A paper proposes to use different parameters for different types of examples. Using different parameters for different types of examples has the potential to greatly reduce underfitting.'
 'Although the trainable parameters might be reduced, unfortunately the training and recognition speech cannot be reduced in this way.'
 'The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task.'
 'The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation.']","A paper proposes to use different parameters for different types of examples. Using different parameters for different types of examples has the potential to greatly reduce underfitting.    0.588975
Although the trainable parameters might be reduced, unfortunately the training and recognition speech cannot be reduced in this way.                                                          1.163102
The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task.                                     0.892792
The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation.                                      0.923856
dtype: float32","A paper proposes to use different parameters for different types of examples. Using different parameters for different types of examples has the potential to greatly reduce underfitting.    1.386294
Although the trainable parameters might be reduced, unfortunately the training and recognition speech cannot be reduced in this way.                                                          1.386294
The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task.                                     1.386294
The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation.                                      1.386294
dtype: float32","{""A paper proposes to use different parameters for different types of examples. Using different parameters for different types of examples has the potential to greatly reduce underfitting."":{""A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics.  For example, recognizing animals will likely require different features than categorizing flowers.  Using different parameters for different types of examples has the potential to greatly reduce underfitting.  This can be seen in recent results with generative models, where image quality is much better for less diverse datasets.  However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network).  ----------------The hypernetworks paper cleverly proposes to get around this problem by adapting different \""parameters\"" for different time steps in recurrent networks and different.  The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so"":-1.1617058516,""Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.--------Unfortunately, as the results show, the authors could not get better results with less parameters.--------However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.----------------The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.--------E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.----------------Could the authors provide the num. of trainable parameters for Table 6?----------------Probably presenting less results could also improve the readability.--------Only marginal accept due to the writing style."":-3.365380764,""This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    ------------------pros----------------This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.------------------cons----------------The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters."":-3.5215072632,""*** Paper Summary ***----------------The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.----------------*** Review Summary ***----------------Pros: --------- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. --------- LM and MT results are excellent.----------------Cons:  --------- The paper could be better written. It is too long for the conference format and need refocussing. --------- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way"":-3.4824919701},""Although the trainable parameters might be reduced, unfortunately the training and recognition speech cannot be reduced in this way."":{""A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics.  For example, recognizing animals will likely require different features than categorizing flowers.  Using different parameters for different types of examples has the potential to greatly reduce underfitting.  This can be seen in recent results with generative models, where image quality is much better for less diverse datasets.  However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network).  ----------------The hypernetworks paper cleverly proposes to get around this problem by adapting different \""parameters\"" for different time steps in recurrent networks and different.  The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so"":-9.5923395157,""Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.--------Unfortunately, as the results show, the authors could not get better results with less parameters.--------However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.----------------The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.--------E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.----------------Could the authors provide the num. of trainable parameters for Table 6?----------------Probably presenting less results could also improve the readability.--------Only marginal accept due to the writing style."":-5.2339539528,""This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    ------------------pros----------------This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.------------------cons----------------The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters."":-8.9526586533,""*** Paper Summary ***----------------The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.----------------*** Review Summary ***----------------Pros: --------- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. --------- LM and MT results are excellent.----------------Cons:  --------- The paper could be better written. It is too long for the conference format and need refocussing. --------- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way"":-9.9957351685},""The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task."":{""A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics.  For example, recognizing animals will likely require different features than categorizing flowers.  Using different parameters for different types of examples has the potential to greatly reduce underfitting.  This can be seen in recent results with generative models, where image quality is much better for less diverse datasets.  However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network).  ----------------The hypernetworks paper cleverly proposes to get around this problem by adapting different \""parameters\"" for different time steps in recurrent networks and different.  The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so"":-7.56965065,""Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.--------Unfortunately, as the results show, the authors could not get better results with less parameters.--------However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.----------------The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.--------E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.----------------Could the authors provide the num. of trainable parameters for Table 6?----------------Probably presenting less results could also improve the readability.--------Only marginal accept due to the writing style."":-7.2655177116,""This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    ------------------pros----------------This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.------------------cons----------------The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters."":-4.4689235687,""*** Paper Summary ***----------------The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.----------------*** Review Summary ***----------------Pros: --------- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. --------- LM and MT results are excellent.----------------Cons:  --------- The paper could be better written. It is too long for the conference format and need refocussing. --------- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way"":-7.972091198},""The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation."":{""A well known limitation in deep neural networks is that the same parameters are typically used for all examples, even though different examples have very different characteristics.  For example, recognizing animals will likely require different features than categorizing flowers.  Using different parameters for different types of examples has the potential to greatly reduce underfitting.  This can be seen in recent results with generative models, where image quality is much better for less diverse datasets.  However, it is difficult to use different parameters for different examples because we typically train using minibatches, which relies on using the same parameters for all examples in a minibatch (i.e. doing matrix multiplies in a fully-connected network).  ----------------The hypernetworks paper cleverly proposes to get around this problem by adapting different \""parameters\"" for different time steps in recurrent networks and different.  The basic insight is that a minibatch will always include many different examples from the same time step or spatial position, so"":-5.591583252,""Although the trainable parameters might be reduced significantly, unfortunately the training and recognition speech cannot be reduced in this way.--------Unfortunately, as the results show, the authors could not get better results with less parameters.--------However, the proposed structure with even more number of parameters shows significant gain e.g. in LM.----------------The paper should be reorganized, and shortened. It is sometimes difficult to follow and sometimes inconsistent.--------E.g.: the weights of the feedforward network depend only on an embedding vector (see also my previous comments on linear bottlenecks), whereas in recurrent network the generated weights also depend on the input observation or its hidden representation.----------------Could the authors provide the num. of trainable parameters for Table 6?----------------Probably presenting less results could also improve the readability.--------Only marginal accept due to the writing style."":-5.2257885933,""This paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. In particular, the hyperLSTM with non-shared weights can achieve excellent results compared to conventional LSTM and its variants on a couple of LM talks, which is very inspiring.    ------------------pros----------------This work demonstrates that it is possible to generate the neural network model parameters using another network that can achieve competitive results by a few relative large scale experiments. The idea itself is very inspiring, and the experiments are very solid.------------------cons----------------The paper would be much stronger if it was more focused. In particular, it is unclear what is the key advantage of this hypernetwork approach. It is argued that in the paper that can achieve competitive results using smaller number of trainable model parameters."":-5.6643414497,""*** Paper Summary ***----------------The paper proposes to a new neural network architecture. The layer weights of a classical network are computed as a function of a latent representation associated with the layer. Two instances are presented (i) a CNN where each layer weight is computed from a lower dimensional layer embedding vector; (ii) an RNN where each layer weight is computed from a secondary RNN state.----------------*** Review Summary ***----------------Pros: --------- I like the idea of bringing multiplicative RNNs and their predecessors back into the spotlight. --------- LM and MT results are excellent.----------------Cons:  --------- The paper could be better written. It is too long for the conference format and need refocussing. --------- On related work, the relation with multiplicative RNN and their generic tensor product predecessor (Order 2 networks, wrt C. Lee Giles definition) should be mentioned in the related work section and the differences with earlier research need to be explained and motivated (by the way"":-2.2765522003}}","The paper contains an interesting idea, and after the revision of 3rd Jan, the presentation is clear enough as well. (Although I find it now contains an odd repetition where related work is presented first in section 2, and then later in section 3.2)."
https://openreview.net/forum?id=ry54RWtxx,"['A paper develops a toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used.'
 'The authors are trying to understand whether static analysis can be learned. LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems.'
 'The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem. The toy task presented in this paper is too simple to warrant an ICLR submission.']","['A paper develops a toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used.'
 'The authors are trying to understand whether static analysis can be learned. LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems.'
 'The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem. The toy task presented in this paper is too simple to warrant an ICLR submission.']","A paper develops a toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used.                                                            0.589478
The authors are trying to understand whether static analysis can be learned. LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems.                                                      0.753995
The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem. The toy task presented in this paper is too simple to warrant an ICLR submission.    0.811823
dtype: float32","A paper develops a toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used.                                                            1.098612
The authors are trying to understand whether static analysis can be learned. LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems.                                                      1.098612
The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem. The toy task presented in this paper is too simple to warrant an ICLR submission.    1.098612
dtype: float32","{""A paper develops a toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used."":{""This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.----------------One further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model"":-6.8561878204,""The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.----------------LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it."":-9.6321268082,""The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.----------------While the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these bas"":-9.0891342163},""The authors are trying to understand whether static analysis can be learned. LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems."":{""This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.----------------One further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model"":-8.1293725967,""The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.----------------LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it."":-5.0176625252,""The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.----------------While the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these bas"":-8.0881490707},""The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem. The toy task presented in this paper is too simple to warrant an ICLR submission."":{""This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. The paper tries a variety of off-the-shelf sequence classification models and develops a new model that makes use of a ``differentiable set'' to keep track of which variables have been defined so far. Result show that an LSTM model can achieve 98% accuracy, and the differentiable set model can achieve 99.3% accuracy with sequence-level supervision and 99.7% accuracy with strong token-level supervision. An additional result is used whereby an LSTM language model is trained over correct code, and then low probability (where a threshold to determine low is tuned by hand) tokens are highlighted as sources of possible error.----------------One further question is if the authors could clarify what reasoning patterns are needed to solve these problems. Does the model"":-3.9953258038,""The authors are trying to understand whether static analysis can be learned. As I hinted in my question, I think that all of the interesting complexity of static analysis has been removed in the toy language --- extraordinarily simple logic using a set can solve the problem posed, and an LSTM (unsurprisingly) can learn the extraordinarily simple logic (when given a differentiable set object). This extreme simplicity gives me no confidence that a more realistic static analysis problem can be solved.----------------LSTMs (and deep learning) have had remarkable successes in solving messy real-world language problems. It's certainly possible that LSTMs could solve static analysis -- but being technically timid is not the right way to go about it."":-3.7063448429,""The authors explore the idea of deep-learning a static analyzer. They do it with a toy programming language and a very simplified analysis problem -- just checking if all variables are initalized.----------------While the idea is interesting and might be developped into a tool in the future, the toy task presented in this paper is too simple to warrant an ICLR submission. Just detecting whether a variable is initialized in a string is a toy algorihtmic task, similar to the ones solved in a number of paper in recent years by models such as the Neural Turing Machine, Stack RNNs, Neural GPU, or Differentiable Neural Computer. All these architectures perform almost perfectly on a number of algorithmic tasks, so it is highly probable that they would also solve this one. Unluckily, the authors only compare to much more basic models, such as HMMs. Since the code for many of the above-mentioned models is available online, a paper without these bas"":-0.4904210567}}","There is a general consensus that, though the idea is interesting, the work is not mature enough for a conference publication (e.g., the problem is too toy, not clear that really solves any, even artificial problem, better than existing techniques)."
https://openreview.net/forum?id=ryAe2WBee,"[' and EUrlex.'
 'The paper presents the semantic embedding model for multi-label prediction.'
 '. This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network.']","[' and EUrlex.'
 'The paper presents the semantic embedding model for multi-label prediction.'
 '. This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network.']"," and EUrlex.                                                                                                                                                                              0.844988
The paper presents the semantic embedding model for multi-label prediction.                                                                                                               0.628751
. This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network.    0.940044
dtype: float32"," and EUrlex.                                                                                                                                                                              1.098612
The paper presents the semantic embedding model for multi-label prediction.                                                                                                               1.098612
. This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network.    1.098612
dtype: float32","{"" and EUrlex."":{""The paper proposes a semantic embedding based approach to multilabel classification. --------Conversely to previous proposals, SEM considers the underlying parameters determining the--------observed labels are low-rank rather than that the observed label matrix is itself low-rank. --------However, It is not clear to what extent the difference between the two assumptions is significant----------------SEM models the labels for an instance as draws from a multinomial distribution--------parametrized by nonlinear functions of the instance features. As such, it is a neural network.--------The proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. ----------------The paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. "":-82.3684082031,""The paper presents the semantic embedding model for multi-label prediction.--------In my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!--------I was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.--------Regarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.--------One last question: why is it called \""semantic\"" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper."":-86.0215988159,""This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.----------------Given that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well"":-85.7727432251},""The paper presents the semantic embedding model for multi-label prediction."":{""The paper proposes a semantic embedding based approach to multilabel classification. --------Conversely to previous proposals, SEM considers the underlying parameters determining the--------observed labels are low-rank rather than that the observed label matrix is itself low-rank. --------However, It is not clear to what extent the difference between the two assumptions is significant----------------SEM models the labels for an instance as draws from a multinomial distribution--------parametrized by nonlinear functions of the instance features. As such, it is a neural network.--------The proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. ----------------The paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. "":-29.4816570282,""The paper presents the semantic embedding model for multi-label prediction.--------In my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!--------I was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.--------Regarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.--------One last question: why is it called \""semantic\"" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper."":-27.2497768402,""This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.----------------Given that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well"":-30.3591880798},"". This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network."":{""The paper proposes a semantic embedding based approach to multilabel classification. --------Conversely to previous proposals, SEM considers the underlying parameters determining the--------observed labels are low-rank rather than that the observed label matrix is itself low-rank. --------However, It is not clear to what extent the difference between the two assumptions is significant----------------SEM models the labels for an instance as draws from a multinomial distribution--------parametrized by nonlinear functions of the instance features. As such, it is a neural network.--------The proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. ----------------The paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. "":-4.9547028542,""The paper presents the semantic embedding model for multi-label prediction.--------In my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!--------I was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.--------Regarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.--------One last question: why is it called \""semantic\"" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper."":-5.121199131,""This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.----------------Given that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well"":-0.8908751607}}","This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions."
https://openreview.net/forum?id=ryMxXPFex,"['Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds. Directed hierarchical continuous component models actual manifolds.'
 'The paper presents a way of training deep generative models with discrete hidden variables. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance.'
 'The paper is very rich with ideas. The framework does become a little complex.']","['Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds. Directed hierarchical continuous component models actual manifolds.'
 'The paper presents a way of training deep generative models with discrete hidden variables. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance.'
 'The paper is very rich with ideas. The framework does become a little complex.']","Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds. Directed hierarchical continuous component models actual manifolds.    0.977888
The paper presents a way of training deep generative models with discrete hidden variables. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance.                           0.748630
The paper is very rich with ideas. The framework does become a little complex.                                                                                                                                                                                           0.685756
dtype: float32","Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds. Directed hierarchical continuous component models actual manifolds.    1.098612
The paper presents a way of training deep generative models with discrete hidden variables. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance.                           1.098612
The paper is very rich with ideas. The framework does become a little complex.                                                                                                                                                                                           1.098612
dtype: float32","{""Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds. Directed hierarchical continuous component models actual manifolds."":{""Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). ----------------Overall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:----------------Graham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the"":-2.5306301117,""This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. ----------------The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!----------------The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)----------------I\u2019m a bit concerned about the variance of the"":-6.9194698334,""This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). --------A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. --------The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.--------Overall, the paper is very rich with ideas so I think it would be a great contribution to the conference. "":-7.1307983398},""The paper presents a way of training deep generative models with discrete hidden variables. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance."":{""Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). ----------------Overall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:----------------Graham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the"":-3.6384849548,""This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. ----------------The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!----------------The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)----------------I\u2019m a bit concerned about the variance of the"":-0.5849147439,""This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). --------A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. --------The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.--------Overall, the paper is very rich with ideas so I think it would be a great contribution to the conference. "":-3.6682293415},""The paper is very rich with ideas. The framework does become a little complex."":{""Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables). In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters. The training procedure for such models is also presented and is quite involved. Experiments illustrate state-of-the-art performance on public datasets (including MNIST, Omniglot, Caltech-101). ----------------Overall the model is interesting and could be useful in a variety of applications and domains. The approach is complex and somewhat mathematically involved. It's not exactly clear how the model compares or relates to other RBM formulations, particularly those that contain discrete latent variables and continuous outputs. As a prime example:----------------Graham Taylor and Geoffrey Hinton. Factored conditional restricted Boltzmann machines for modeling motion style. In Proc. of the"":-25.0420665741,""This paper presents a way of training deep generative models with discrete hidden variables using the reparameterization trick. It then applies it to a particular DBN-like architecture, and shows that this architecture achieves state-of-the-art density modeling performance on MNIST and similar datasets. ----------------The paper is well written, and the exposition is both thorough and precise. There are several appendices which justify various design decisions in detail. I wish more papers in our field would take this degree of care with the exposition!----------------The log-likelihood results are quite strong, especially given that most of the competitive algorithms are based on continuous latent variables. Probably the main thing missing from the experiments is some way to separate out the contributions of the architecture and the inference algorithm. (E.g., what if a comparable architecture is trained with VIMCO, or if the algorithm is applied to a previously published discrete architecture?)----------------I\u2019m a bit concerned about the variance of the"":-24.7646007538,""This is an interesting paper on how to handle reparameterization in VAEs when you have discrete variables. The idea is to introduce a smoothing transformation that is shared between the generative model and the recognition model (leading to cancellations). --------A second contribution is to introduce an RBM as the prior model P(z) and to use autoregressive connections in generative and recognition models. The whole package becomes a bit entangled and complex and it is hard to figure out what causes the claimed good performance. Experiments that study these contributions separately would have been nice. --------The framework does become a little complex but this should not be a problem if nice software is delivered that can be used in a plug and play mode.--------Overall, the paper is very rich with ideas so I think it would be a great contribution to the conference. "":-22.0770187378}}","The authors present a novel reparameterization framework for VAEs with discrete random variables. The idea is to carry out symmetric projections of the approximate posterior and the prior into a continuous space and evaluating the autoencoder term in that space by marginalizing out the discrete variables. They consider the KL divergence between the approximating posterior and the true prior in the original discrete space and show that due to the symmetry of the projection into the continuous space, it does not  contribute to the KL term.     One question that warrants further investigation is whether this framework can be extended to GANs and what empirical success they would have.    The reviewers have presented a strong case for the acceptance of the paper and I go with their recommendation."
https://openreview.net/forum?id=ryUPiRvge,"['Thank you for your perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.'
 '.'
 'Theory: Multiplication units can also be represented by a neural network in the usual sense. EQL is very interesting from the perspective of interpretability, which is crucial for data analysis.']","['Thank you for your perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.'
 '.' '.']","Thank you for your perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.    0.922501
.                                                                                                                                                                                                                                                          0.821020
Theory: Multiplication units can also be represented by a neural network in the usual sense. EQL is very interesting from the perspective of interpretability, which is crucial for data analysis.                                                         0.750996
dtype: float32","Thank you for your perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.    1.097698
.                                                                                                                                                                                                                                                          0.439999
Theory: Multiplication units can also be represented by a neural network in the usual sense. EQL is very interesting from the perspective of interpretability, which is crucial for data analysis.                                                         0.440928
dtype: float32","{""Thank you for your perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc."":{""Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.                                                                                                                                                  --------       "":-0.7016319036,""The authors attempt to extract analytical equations governing physical systems from observations - an important task. Being able to capture succinct and interpretable rules which a physical system follows is of great importance. However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field. ----------------The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence: --------\""Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t. the weights are sparse (L1)\"".--------The learnt network weights with its fixed structure are then presented as the learnt equation. ----------------This research uses tools from literature from the '90s (I haven't seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then. I would encourage the authors to review modern literature and continue"":-4.679142952,""Thank you for an interesting read. ----------------To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as \""extrapolation\""). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.----------------Quesions and comments:----------------1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future?----------------2. Fitting an EQL vs. fitting a polynomial. It seems"":-4.7427010536},""."":{""Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.                                                                                                                                                  --------       "":-190.4069824219,""The authors attempt to extract analytical equations governing physical systems from observations - an important task. Being able to capture succinct and interpretable rules which a physical system follows is of great importance. However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field. ----------------The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence: --------\""Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t. the weights are sparse (L1)\"".--------The learnt network weights with its fixed structure are then presented as the learnt equation. ----------------This research uses tools from literature from the '90s (I haven't seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then. I would encourage the authors to review modern literature and continue"":-189.2606201172,""Thank you for an interesting read. ----------------To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as \""extrapolation\""). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.----------------Quesions and comments:----------------1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future?----------------2. Fitting an EQL vs. fitting a polynomial. It seems"":-186.324508667},""Theory: Multiplication units can also be represented by a neural network in the usual sense. EQL is very interesting from the perspective of interpretability, which is crucial for data analysis."":{""Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc.                                                                                                                                                  --------       "":-5.5868067741,""The authors attempt to extract analytical equations governing physical systems from observations - an important task. Being able to capture succinct and interpretable rules which a physical system follows is of great importance. However, the authors do this with simple and naive tools which will not scale to complex tasks, offering no new insights or advances to the field. ----------------The contribution of the paper (and the first four pages of the submission!) can be summarised in one sentence: --------\""Learn the weights of a small network with cosine, sinusoid, and input elements products activation functions s.t. the weights are sparse (L1)\"".--------The learnt network weights with its fixed structure are then presented as the learnt equation. ----------------This research uses tools from literature from the '90s (I haven't seen the abbreviation ANN (page 3) for a long time) and does not build on modern techniques which have advanced a lot since then. I would encourage the authors to review modern literature and continue"":-5.5415067673,""Thank you for an interesting read. ----------------To my knowledge, very few papers have looked at transfer learning with **no** target domain data (the authors called this task as \""extrapolation\""). This paper clearly shows that the knowledge of the underlying system dynamics is crucial in this case. The experiments clearly showed the promising potential of the proposed EQL model. I think EQL is very interesting also from the perspective of interpretability, which is crucial for data analysis in scientific domains.----------------Quesions and comments:----------------1. Multiplication units. By the universal approximation theorem, multiplication can also be represented by a neural network in the usual sense. I agree with the authors' explanation of interpolation and extrapolation, but I still don't quite understand why multiplication unit is crucial here. I guess is it because this representation generalises better when training data is not that representative for the future?----------------2. Fitting an EQL vs. fitting a polynomial. It seems"":-2.4856898785}}","This paper proposes using functions such as sin and cos as basis functions, then training a neural network with L1 regularization to obtain a simple estimate of functions that can extrapolate under some circumstances.    Pros:  - the paper has a wide-ranging discussion connecting extrapolation in regression problems to adjacent fields of system identification and causal learning.  - the method is sensible enough, and should probably be a baseline in the time-series literature. It also seems like an advance on the hard-to-optimize Eureqa method.    Cons:  I agree with the authors that Reviewer 5's comments aren't very helpful, but this paper really does ignore or dismiss a lot of recent progress and related methods. Specifically:  - The authors claim that cross-validation can't be used to choose the model, since it wouldn't encourage extrapolation - but why not partition the data in contiguous chunks, as is done in time-series methods?  - The authors introduce an annealing trick to help with the L1 objective, but there is a rich literature on gradient-based optimization methods with L1 regularization that address exactly this problem.  - The authors mostly consider toy data, limiting the potential impact of their method.  - The authors don't compare against closely related methods developed to address the exact same setting. Namely, Schmit + Lipson's Eureqa method, and the Gaussian process methods of Duvenaud, Lloyd, Grosse, Tenenbaum and Ghahramani.  - The authors invent their own ad-hoc model-selection procedure, again ignoring a massive literature.    Given the many ""cons"", it is recommended that this paper not be presented at the conference track, but be featured at the workshop track."
https://openreview.net/forum?id=ryXZmzNeg,"['The paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z.'
 'The authors propose to sample from VAEs through a Markov chain. The paper uses confusing notation and oversells the novelty. The qualitative difference between regular sampling and this Gibbs chain is not very convincing.'
 'The paper is not clearly written. The notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. The empirical results consist entirely of qualitative results (samples and reconstructions) from']","['The paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z.'
 'The authors propose to sample from VAEs through a Markov chain. The paper uses confusing notation and oversells the novelty. The qualitative difference between regular sampling and this Gibbs chain is not very convincing.'
 'The paper is not clearly written. The notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. The empirical results consist entirely of qualitative results (samples and reconstructions) from']","The paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z.                                                                                                                                                                                       0.865606
The authors propose to sample from VAEs through a Markov chain. The paper uses confusing notation and oversells the novelty. The qualitative difference between regular sampling and this Gibbs chain is not very convincing.                                                                    0.839222
The paper is not clearly written. The notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. The empirical results consist entirely of qualitative results (samples and reconstructions) from    0.488549
dtype: float32","The paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z.                                                                                                                                                                                       1.098612
The authors propose to sample from VAEs through a Markov chain. The paper uses confusing notation and oversells the novelty. The qualitative difference between regular sampling and this Gibbs chain is not very convincing.                                                                    1.098612
The paper is not clearly written. The notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. The empirical results consist entirely of qualitative results (samples and reconstructions) from    1.098612
dtype: float32","{""The paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z."":{""This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.----------------The paper in its current form is not acceptable due to the following reasons:--------1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.--------2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors \""must be doing\"". What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it's P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior"":-18.2216091156,""The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.----------------Comments: -------- - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper-------- - Notation is nonstandard \/ confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.---------"":-21.8304157257,""The authors argues that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial--------Autoencoder) imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov Chain Monte Carlo approach that avoids the need to specify a simple parametric form for the prior.----------------The paper is not clearly written. Most critically, the notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. For example, the authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are parametrized. For this to be true the model must either be trivially simple, or an energy-based model. There is no indication that they are speaking of an energy-based model. Another example of possible confusion is the statement that"":-21.8905487061},""The authors propose to sample from VAEs through a Markov chain. The paper uses confusing notation and oversells the novelty. The qualitative difference between regular sampling and this Gibbs chain is not very convincing."":{""This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.----------------The paper in its current form is not acceptable due to the following reasons:--------1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.--------2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors \""must be doing\"". What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it's P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior"":-6.0760583878,""The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.----------------Comments: -------- - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper-------- - Notation is nonstandard \/ confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.---------"":-2.6127455235,""The authors argues that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial--------Autoencoder) imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov Chain Monte Carlo approach that avoids the need to specify a simple parametric form for the prior.----------------The paper is not clearly written. Most critically, the notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. For example, the authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are parametrized. For this to be true the model must either be trivially simple, or an energy-based model. There is no indication that they are speaking of an energy-based model. Another example of possible confusion is the statement that"":-6.1351046562},""The paper is not clearly written. The notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. The empirical results consist entirely of qualitative results (samples and reconstructions) from"":{""This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution.----------------The paper in its current form is not acceptable due to the following reasons:--------1. No quantitative evaluation. The authors do include samples from the generative model, which however are insufficient to judge performance of the model. See comment 2.--------2. The description of the model is very unclear. I had to indulge in a lot of charity to interpret what the authors \""must be doing\"". What does Q(Z) mean? Does it mean the true posterior P(Z | X) ? What is the generative model here? Typically, it's P(Z)P(X|Z). VAEs use a variational approximation Q(Z | X) to the true posterior P(Z | X). Are you trying to say that your model can sample from the true posterior"":-4.4197125435,""The authors propose to sample from VAEs through a Markov chain [z_t ~ q(z|x=x_{t-1}), x_t ~ p(x|z=z_t)]. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results. The qualitative difference between regular sampling and this Gibbs chain is not very convincing, judging from the figures. It would be a great workshop paper (perhaps more), if the authors fix the notation, fix the discussion to related work, and produce more convincing (perhaps simply upscaled?) figures.----------------Comments: -------- - Rezende et al's (2014) original VAE paper already discusses the Markov chain, which is ignored in this paper-------- - Notation is nonstandard \/ confusing. At page 1, it\u2019s unclear what the authors mean with \u201cp(x|z) which is approximated as q(x|z)\u201d.---------"":-4.4950566292,""The authors argues that the standard ancestral sampling from stochastic autoencoders (such as the Variational Autoencoder and the Adversarial--------Autoencoder) imposes the overly-restrictive constraint that the encoder distribution must marginally match the latent variable prior. They propose, as an alternative, a Markov Chain Monte Carlo approach that avoids the need to specify a simple parametric form for the prior.----------------The paper is not clearly written. Most critically, the notation the authors use is either deeply flawed, or there are simple misunderstanding with respect to the manipulations of probability distributions. For example, the authors seem to suggest that both distributions Q(Z|X) and Q(X|Z) are parametrized. For this to be true the model must either be trivially simple, or an energy-based model. There is no indication that they are speaking of an energy-based model. Another example of possible confusion is the statement that"":-2.2900524139}}","This approach taken in this paper is topical, especially since the importance of sampling and generating diverse samples is increasingly discussed in work on generative models. There were several concerns from reviewers, in three areas particularly: connection and comparison to related work; lack of clarity and understanding of the paper; experiments that are not sufficiently convincing. These have been addressed to some extent by the authors, discussing in more detail the related work, especially in connection to Rezende et al., and GSN of Bengio et al., and with improved figures. But these points are still of concern especially in terms of assessing sample diversity in relation to much of the recent work on richer variational posterior methods and other techniques. For these reasons, the paper is not yet ready for acceptance at this years conference."
https://openreview.net/forum?id=ryb-q1Olg,"['The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering.'
 'The proposed algorithm appears to be a useful tool for unsupervised data modelling.'
 'This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods.']","['The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering.'
 'The proposed algorithm appears to be a useful tool for unsupervised data modelling.'
 'This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods.']","The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering.                             0.735590
The proposed algorithm appears to be a useful tool for unsupervised data modelling.                                                             0.616517
This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods.    0.767313
dtype: float32","The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering.                             1.098612
The proposed algorithm appears to be a useful tool for unsupervised data modelling.                                                             1.098612
This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods.    1.098612
dtype: float32","{""The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering."":{""The paper presents a repurposing of rectified factor networks proposed--------earlier by the same authors to biclustering. The method seems--------potentially quite interesting but the paper has serious problems in--------the presentation.------------------------Quality:----------------The method relies mainly on techniques presented in a NIPS 2015 paper--------by (mostly) the same authors. The experimental procedure should be--------clarified further. The results (especially Table 2) seem to depend--------critically upon the sparsity of the reported clusters, but the authors--------do not explain in sufficient detail how the sparsity hyperparameter is--------determined.------------------------Clarity:----------------The style of writing is terrible and completely unacceptable as a--------scientific publication. The text looks more like an industry white--------paper or advertisement, not an objective scientific paper. A complete--------rewrite would be needed before the paper can be considered for--------publication. Specifically, all references to companies using your--------methods must be deleted"":-4.9967050552,""Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.----------------Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. ----------------Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)----------------Quality: The experiments are high-quality. ----------------Comments:--------1) The introduction claims that this method is much faster than FABIA because the use"":-8.0290241241,""This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. ----------------This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of  and .  is a hidden unit, but what is ? I could not find any definition. Furthermore, I could not know how  is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. ----------------Totally, I am not sure that this paper is suitable for publication. ----------------Prons:--------Empirical performance is good.----------------Cons:--------Novelty of the proposed method--------Some description in"":-7.9936008453},""The proposed algorithm appears to be a useful tool for unsupervised data modelling."":{""The paper presents a repurposing of rectified factor networks proposed--------earlier by the same authors to biclustering. The method seems--------potentially quite interesting but the paper has serious problems in--------the presentation.------------------------Quality:----------------The method relies mainly on techniques presented in a NIPS 2015 paper--------by (mostly) the same authors. The experimental procedure should be--------clarified further. The results (especially Table 2) seem to depend--------critically upon the sparsity of the reported clusters, but the authors--------do not explain in sufficient detail how the sparsity hyperparameter is--------determined.------------------------Clarity:----------------The style of writing is terrible and completely unacceptable as a--------scientific publication. The text looks more like an industry white--------paper or advertisement, not an objective scientific paper. A complete--------rewrite would be needed before the paper can be considered for--------publication. Specifically, all references to companies using your--------methods must be deleted"":-14.7429304123,""Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.----------------Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. ----------------Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)----------------Quality: The experiments are high-quality. ----------------Comments:--------1) The introduction claims that this method is much faster than FABIA because the use"":-12.1015539169,""This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. ----------------This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of  and .  is a hidden unit, but what is ? I could not find any definition. Furthermore, I could not know how  is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. ----------------Totally, I am not sure that this paper is suitable for publication. ----------------Prons:--------Empirical performance is good.----------------Cons:--------Novelty of the proposed method--------Some description in"":-14.6126880646},""This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods."":{""The paper presents a repurposing of rectified factor networks proposed--------earlier by the same authors to biclustering. The method seems--------potentially quite interesting but the paper has serious problems in--------the presentation.------------------------Quality:----------------The method relies mainly on techniques presented in a NIPS 2015 paper--------by (mostly) the same authors. The experimental procedure should be--------clarified further. The results (especially Table 2) seem to depend--------critically upon the sparsity of the reported clusters, but the authors--------do not explain in sufficient detail how the sparsity hyperparameter is--------determined.------------------------Clarity:----------------The style of writing is terrible and completely unacceptable as a--------scientific publication. The text looks more like an industry white--------paper or advertisement, not an objective scientific paper. A complete--------rewrite would be needed before the paper can be considered for--------publication. Specifically, all references to companies using your--------methods must be deleted"":-4.3615803719,""Clarity: The novel contribution of the paper --- Section 2.2 --- was very difficult to understand. The notation seemed inconsistent (particularly the use of l, p, and m), and I am still not confident that I understand the model being used.----------------Originality: The novelty comes from applying the RFN model (including the ReLU non-linearity and dropout training) to the problem of biclustering. It sounds like a good idea. ----------------Significance: The proposed algorithm appears to be a useful tool for unsupervised data modelling, and the authors make a convincing argument that it is significant. (I.E. The previous state-of-the-art, FABIA, is widely used and this method both outperforms and addresses some of the practical difficulties with that method.)----------------Quality: The experiments are high-quality. ----------------Comments:--------1) The introduction claims that this method is much faster than FABIA because the use"":-3.5815372467,""This paper applies RFN for biclustering to overcome the drawbacks in FABIA. The proposed method performs best among 14 biclustering methods, However, my first concern is that from the methodological point of view, the novelty of the proposed method seems small. The authors replied to the same question which another reviewer gave, but the replies were not so convincing. ----------------This paper was actually difficult for me to follow. For instance, in Figure 1, a bicluster matrix is constructed as an outer product of  and .  is a hidden unit, but what is ? I could not find any definition. Furthermore, I could not know how  is estimated in this method. Therefore, I do NOT understand how this method performs biclustering. ----------------Totally, I am not sure that this paper is suitable for publication. ----------------Prons:--------Empirical performance is good.----------------Cons:--------Novelty of the proposed method--------Some description in"":-0.7739211917}}","The reviewers pointed out several issues with the paper, and all recommended rejection."
https://openreview.net/forum?id=ryhqQFKgl,"[""I think the ideas and approaches are good, and certainly worth publishing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. The extensive use of notation did not help the clarity.""
 ""The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. Sonority (column of 4 MIDI numbers) acts as word in language model.""
 'This paper proposes an interesting framework to learn compositional rules used to compose better music. The system consists of two components, a generative component and a discriminative component.']","[""I think the ideas and approaches are good, and certainly worth publishing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. The extensive use of notation did not help the clarity.""
 ""The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. Sonority (column of 4 MIDI numbers) acts as word in language model.""
 'This paper proposes an interesting framework to learn compositional rules used to compose better music. The system consists of two components, a generative component and a discriminative component.']","I think the ideas and approaches are good, and certainly worth publishing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. The extensive use of notation did not help the clarity.    0.885482
The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. Sonority (column of 4 MIDI numbers) acts as word in language model.                                       0.928396
This paper proposes an interesting framework to learn compositional rules used to compose better music. The system consists of two components, a generative component and a discriminative component.                0.699351
dtype: float32","I think the ideas and approaches are good, and certainly worth publishing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. The extensive use of notation did not help the clarity.    1.098612
The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. Sonority (column of 4 MIDI numbers) acts as word in language model.                                       1.098612
This paper proposes an interesting framework to learn compositional rules used to compose better music. The system consists of two components, a generative component and a discriminative component.                1.098612
dtype: float32","{""I think the ideas and approaches are good, and certainly worth publishing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. The extensive use of notation did not help the clarity."":{""After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper.----------------Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity.----------------I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion"":-0.5824559927,""Summary: --------The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and \""satisfactory gaps\"" have been generated in terms of sets of learned rules. ----------------1. Quality:-------- a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and"":-4.2863440514,""This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach\u2019s chorales) and propose new rules for the student to learn so that it could improve.----------------The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple"":-4.396873951},""The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. Sonority (column of 4 MIDI numbers) acts as word in language model."":{""After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper.----------------Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity.----------------I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion"":-8.096405983,""Summary: --------The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and \""satisfactory gaps\"" have been generated in terms of sets of learned rules. ----------------1. Quality:-------- a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and"":-3.6729619503,""This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach\u2019s chorales) and propose new rules for the student to learn so that it could improve.----------------The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple"":-7.4361243248},""This paper proposes an interesting framework to learn compositional rules used to compose better music. The system consists of two components, a generative component and a discriminative component."":{""After the discussion below, I looked at previous work by the authors (MUS-ROVER) on which this paper was based. On one hand, this was very helpful for me to better understand the current paper. On the other hand, this was very needed for me to better understand the current paper.----------------Overall, while I think that I like this work, and while I am familiar with the JSB chorales, with probabilistic approaches, with n-grams, etc, I did find the paper quite hard to follow at various parts. The extensive use of notation did not help the clarity.----------------I think the ideas and approaches are good, and certainly worth publishing and worth pursuing. I am not sure that, in the paper's current form, ICLR is an appropriate venue. (Incidentally, the issue is not the application as I think that music applications can be very appropriate, nor is the problem necessarily with the approach... see my next suggestion"":-7.6509809494,""Summary: --------The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales, which extends their previous work in: 1) the rule hierarchy in both conceptual and informational dimensions; 2) adaptive 2-D memory selection which assumes the features follow Dirichlet Distribution. Sonority (column of 4 MIDI numbers) acts as word in language model: unigram statistics have been used to learn the fundamental rules in music theory, while n-grams with higher order help characterize part writing. Sonorities have been clustered together based on feature functions through iterations. The partition induced by the features is recognized as a rule if it is sufficiently significant. As a result, two sample syllabi with different difficulty strides and \""satisfactory gaps\"" have been generated in terms of sets of learned rules. ----------------1. Quality:-------- a) Strengths: In the paper, the exploration of hierarchies in two dimensions makes the learning process more cognitive and"":-7.0673661232,""This paper proposes an interesting framework (as a follow-up work of the author's previous paper) to learn compositional rules used to compose better music. The system consists of two components, a generative component (student) and a discriminative component (teacher). The generative component is a Probabilistic Graphical Models, generating the music following learned rules. The teacher compares the generated music with the empirical distribution of exemplar music (e.g, Bach\u2019s chorales) and propose new rules for the student to learn so that it could improve.----------------The framework is different from GANs that the both the generative and discriminative components are interpretable. From the paper, it seems that the system can indeed learn sensible rules from the composed music and apply them in the next iteration, if trained in a curriculum manner. However, there is no comparison between the proposed system and its previous version, nor comparison between the proposed system and other simple"":-4.461435318}}","Given that all reviewers were positive aobut this paper and given the unusual application domain, we recommend to accept this paper for poster presentation at the main conference."
https://openreview.net/forum?id=ryuxYmvel,"['Author is not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. While most of the paper is well written, there are some sections that are not clear.'
 '. The paper is well-written in terms of presentation and argumentation. The related work seems to be well-covered.'
 '. The success of methods like AlphaGo suggests that for hard combinatorial problems, having a curated set of expert data is a good launching point for possibly super-human performance.']","['Author is not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. While most of the paper is well written, there are some sections that are not clear.'
 '. The paper is well-written in terms of presentation and argumentation. The related work seems to be well-covered.'
 '. The success of methods like AlphaGo suggests that for hard combinatorial problems, having a curated set of expert data is a good launching point for possibly super-human performance.']","Author is not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. While most of the paper is well written, there are some sections that are not clear.    0.731587
. The paper is well-written in terms of presentation and argumentation. The related work seems to be well-covered.                                                                                   0.664325
. The success of methods like AlphaGo suggests that for hard combinatorial problems, having a curated set of expert data is a good launching point for possibly super-human performance.             0.867681
dtype: float32","Author is not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. While most of the paper is well written, there are some sections that are not clear.    1.098612
. The paper is well-written in terms of presentation and argumentation. The related work seems to be well-covered.                                                                                   1.098612
. The success of methods like AlphaGo suggests that for hard combinatorial problems, having a curated set of expert data is a good launching point for possibly super-human performance.             1.098612
dtype: float32","{""Author is not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. While most of the paper is well written, there are some sections that are not clear."":{""Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. ----------------I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2):--------- Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow.  --------- Section 2 describes how the data was splits into train and test set. One thing"":-0.5930487514,""The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving."":-3.4015922546,""The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.----------------It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new the"":-3.8069796562},"". The paper is well-written in terms of presentation and argumentation. The related work seems to be well-covered."":{""Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. ----------------I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2):--------- Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow.  --------- Section 2 describes how the data was splits into train and test set. One thing"":-11.8763570786,""The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving."":-9.2104501724,""The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.----------------It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new the"":-12.0320386887},"". The success of methods like AlphaGo suggests that for hard combinatorial problems, having a curated set of expert data is a good launching point for possibly super-human performance."":{""Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. ----------------I am not an expert on ITP or theorem proving, so I will present a review from more of a ML perspective. I feel one of the goals of the paper should be to present the problem to a ML audience in a way that is easy for them to grasp. While most of the paper is well written, there are some sections that are not clear (especially section 2):--------- Terms such as LCF, OCaml-top level, deBruijn indices have been used without explaining or any references. These terms might be trivial in ITP literature, but were hard for me to follow.  --------- Section 2 describes how the data was splits into train and test set. One thing"":-6.8099484444,""The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving."":-6.6403245926,""The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.----------------It would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new the"":-3.0719680786}}","The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.    As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no ""overlap"" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?"
