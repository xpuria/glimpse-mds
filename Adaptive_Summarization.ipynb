{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVigsbsKGrel",
        "outputId": "cb3ba9ad-1d68-4e26-ab7d-0cdadef7cd88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'glimpse-mds'...\n",
            "remote: Enumerating objects: 432, done.\u001b[K\n",
            "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 432 (delta 89), reused 58 (delta 58), pack-reused 326 (from 3)\u001b[K\n",
            "Receiving objects: 100% (432/432), 32.17 MiB | 22.50 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/xpuria/glimpse-mds.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd glimpse-mds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfNMGYSbKDf6",
        "outputId": "23746439-f9c3-444e-ca35-9bf27c1cb08d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/glimpse-mds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python glimpse/data_loading/data_processing.py --dataset_path /content/glimpse-mds/data/all_reviews_2017.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kfIyxbBKE3v",
        "outputId": "428197cb-69ce-4c2b-c879-87f9f7109320"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/glimpse-mds/glimpse/data_loading/data_processing.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sub_dataset.rename(columns={\"review\": \"text\", \"metareview\": \"gold\"}, inplace=True)\n",
            "/content/glimpse-mds/glimpse/data_loading/data_processing.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sub_dataset.rename(columns={\"review\": \"text\", \"metareview\": \"gold\"}, inplace=True)\n",
            "/content/glimpse-mds/glimpse/data_loading/data_processing.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sub_dataset.rename(columns={\"review\": \"text\", \"metareview\": \"gold\"}, inplace=True)\n",
            "/content/glimpse-mds/glimpse/data_loading/data_processing.py:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  sub_dataset.rename(columns={\"review\": \"text\", \"metareview\": \"gold\"}, inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install bert_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zMfjyIbKguI",
        "outputId": "4fa94260-25e7-493e-dc05-435cf6e0a44c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.48.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (1.26.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.28.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m130.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CydyisEK9UF",
        "outputId": "8918103c-ce38-465c-cc02-8303ff8441f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " aspect-based-summarization.csv                 'multimodels_summarization (1).ipynb'\n",
            "'Aspect_Summerizer_final (1).ipynb'              multi_model_summarization.csv\n",
            " \u001b[0m\u001b[01;34mdata\u001b[0m/                                           pyproject.toml\n",
            " \u001b[01;34mexamples\u001b[0m/                                       Readme.md\n",
            " \u001b[01;34mglimpse\u001b[0m/                                        requirements\n",
            "'Glimspe_Hybrid_Summarization_Final (1).ipynb'   \u001b[01;34mrsasumm\u001b[0m/\n",
            " hybrid-summarization                            \u001b[01;34mscripts\u001b[0m/\n",
            " \u001b[01;34mmds\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from bert_score import score as bertscore_score\n",
        "\n",
        "# Import your original RSA reranking implementation.\n",
        "# (Ensure that the module rsasumm is in your Colab path.)\n",
        "from rsasumm.rsa_reranker import RSAReranking\n",
        "\n",
        "# ------------------------------------------\n",
        "# Adaptive RSA Reranking Extension\n",
        "# ------------------------------------------\n",
        "class AdaptiveRSAReranking(RSAReranking):\n",
        "    # Override compute_conditionned_likelihood to pass a maximum length.\n",
        "    def compute_conditionned_likelihood(self, x, y, mean=True):\n",
        "        loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "        # Force truncation to the model's max length (1024 for BART-large-cnn)\n",
        "        x_enc = self.tokenizer(\n",
        "            x,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "        ).to(self.device)\n",
        "        y_enc = self.tokenizer(\n",
        "            y,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=1024\n",
        "        ).to(self.device)\n",
        "        outputs = self.model(\n",
        "            input_ids=x_enc.input_ids,\n",
        "            decoder_input_ids=y_enc.input_ids,\n",
        "            attention_mask=x_enc.attention_mask,\n",
        "            decoder_attention_mask=y_enc.attention_mask,\n",
        "        )\n",
        "        logits = outputs.logits\n",
        "        # Shift logits and target tokens for next-token prediction\n",
        "        shifted_logits = logits[..., :-1, :].contiguous()\n",
        "        shifted_ids = y_enc.input_ids[..., 1:].contiguous()\n",
        "        loss = loss_fn(shifted_logits.view(-1, shifted_logits.size(-1)), shifted_ids.view(-1))\n",
        "        loss = loss.view(len(x), -1).sum(-1)\n",
        "        if mean:\n",
        "            non_pad = (y_enc.input_ids != self.tokenizer.pad_token_id).float().sum(-1)\n",
        "            loss = loss / non_pad\n",
        "        # Return negative loss as likelihood (higher is better)\n",
        "        return -loss\n",
        "\n",
        "    def adaptive_rerank(self, max_iter=10, epsilon=1e-3, alpha=0.5):\n",
        "        \"\"\"\n",
        "        Adaptive RSA reranking extension that:\n",
        "         - Iterates the RSA calculations until convergence.\n",
        "         - Selects the best candidate summary based solely on RSA probabilities.\n",
        "         - Optionally combines RSA with BERTScore F1 (weighted by alpha).\n",
        "\n",
        "        Parameters:\n",
        "          - max_iter: Maximum number of RSA iterations.\n",
        "          - epsilon: Convergence threshold (mean absolute difference).\n",
        "          - alpha: Weight for RSA score (with 1 - alpha for BERTScore).\n",
        "\n",
        "        Returns a dictionary containing:\n",
        "          - \"best_rsa\": Best candidate (per source) based solely on RSA.\n",
        "          - \"best_combined\": Best candidate based on combined RSA+BERTScore.\n",
        "          - \"rsa_probs\": Final RSA probability matrix.\n",
        "          - \"bertscore\": BERTScore F1 matrix.\n",
        "          - \"combined_score\": Final combined score matrix.\n",
        "        \"\"\"\n",
        "        # Compute the initial likelihood matrix using the imported RSAReranking method.\n",
        "        self.initial_speaker_probas = self.likelihood_matrix()  # shape: (num_sources, num_candidates)\n",
        "\n",
        "        # Initialize RSA distributions in log-space.\n",
        "        S_prev = torch.log_softmax(self.initial_speaker_probas, dim=-1)\n",
        "        L_prev = torch.log_softmax(self.initial_speaker_probas, dim=0)\n",
        "\n",
        "        # Adaptive iteration: update S and L until convergence.\n",
        "        for t in range(1, max_iter + 1):\n",
        "            S_new = torch.log_softmax(self.rationality * L_prev, dim=-1)\n",
        "            L_new = torch.log_softmax(S_new, dim=0)\n",
        "            diff = torch.abs(torch.exp(S_new) - torch.exp(S_prev)).mean().item()\n",
        "            print(f\"Iteration {t}: mean difference = {diff:.6f}\")\n",
        "            if diff < epsilon:\n",
        "                print(f\"Convergence reached at iteration {t} (diff = {diff:.6f})\")\n",
        "                break\n",
        "            S_prev, L_prev = S_new, L_new\n",
        "\n",
        "        rsa_probs = torch.exp(S_new)  # Final RSA probabilities\n",
        "\n",
        "        # Select best candidate based solely on RSA.\n",
        "        best_rsa_indices = torch.argmax(rsa_probs, dim=-1)\n",
        "        best_rsa = [self.candidates[i] for i in best_rsa_indices.cpu().tolist()]\n",
        "\n",
        "        # Compute BERTScore F1 for each candidate vs. each source text.\n",
        "        num_sources = len(self.source_texts)\n",
        "        num_candidates = len(self.candidates)\n",
        "        bertscore_matrix = torch.zeros((num_sources, num_candidates), device=self.device)\n",
        "        for j, candidate in enumerate(self.candidates):\n",
        "            candidates_list = [candidate] * num_sources\n",
        "            references_list = self.source_texts\n",
        "            # Compute BERTScore (F1 scores)\n",
        "            _, _, F1 = bertscore_score(candidates_list, references_list, lang=\"en\", verbose=False)\n",
        "            bertscore_matrix[:, j] = F1.to(self.device)\n",
        "\n",
        "        # Combine RSA probabilities and BERTScore (both assumed to be in [0, 1])\n",
        "        combined_score = alpha * rsa_probs + (1 - alpha) * bertscore_matrix\n",
        "        best_combined_indices = torch.argmax(combined_score, dim=-1)\n",
        "        best_combined = [self.candidates[i] for i in best_combined_indices.cpu().tolist()]\n",
        "\n",
        "        return {\n",
        "            \"best_rsa\": best_rsa,\n",
        "            \"best_combined\": best_combined,\n",
        "            \"rsa_probs\": rsa_probs,\n",
        "            \"bertscore\": bertscore_matrix,\n",
        "            \"combined_score\": combined_score\n",
        "        }\n",
        "\n",
        "# ------------------------------------------\n",
        "# Candidate Summary Generation using BART\n",
        "# ------------------------------------------\n",
        "def generate_candidate_summaries(model, tokenizer, text, num_candidates=8, max_length=100):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"longest\",\n",
        "        max_length=1024\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=num_candidates,\n",
        "        do_sample=True,\n",
        "        top_p=0.95,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    candidates = [tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n",
        "    return candidates\n",
        "\n",
        "# ------------------------------------------\n",
        "# Main Pipeline (Variables set for Colab)\n",
        "# ------------------------------------------\n",
        "def main():\n",
        "    # Set variables directly (since we are not using argparse in Colab)\n",
        "    input_csv = \"/content/glimpse-mds/data/processed/all_reviews_2017.csv\"       # Path to your input CSV (upload to Colab)\n",
        "    output_csv = \"adaptive_summarization.csv\"     # Path to save results\n",
        "    num_candidates = 4           # Number of candidate summaries per source\n",
        "    max_length = 100              # Maximum summary length\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # Read the CSV file.\n",
        "    df = pd.read_csv(input_csv)\n",
        "    df = df.iloc[:314]\n",
        "    # Expected CSV columns: id, text, gold\n",
        "    # For example:\n",
        "    # id                                      text                                   gold\n",
        "    # https://openreview.net/forum?id=...      Summary: The paper presents ...        The program committee appreciates...\n",
        "    # https://openreview.net/forum?id=...      Results on the VQA task are good ...   The program committee appreciates...\n",
        "\n",
        "    # Group rows by id: concatenate all texts and take the first gold summary.\n",
        "    grouped = df.groupby(\"id\").agg({\n",
        "        \"text\": lambda texts: \" \".join(texts),\n",
        "        \"gold\": \"first\"\n",
        "    }).reset_index()\n",
        "    print(f\"Found {len(grouped)} unique ids.\")\n",
        "\n",
        "    # Load BART model and tokenizer (facebook/bart-large-cnn)\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    results_list = []\n",
        "    # Process each unique id.\n",
        "    for _, row in grouped.iterrows():\n",
        "        source_id = row[\"id\"]\n",
        "        source_text = row[\"text\"]\n",
        "        gold_summary = row[\"gold\"]\n",
        "        print(f\"Processing id: {source_id}\")\n",
        "\n",
        "        # Generate candidate summaries.\n",
        "        candidates = generate_candidate_summaries(\n",
        "            model, tokenizer, source_text,\n",
        "            num_candidates=num_candidates,\n",
        "            max_length=max_length\n",
        "        )\n",
        "        print(\"Generated candidates:\")\n",
        "        for idx, cand in enumerate(candidates, start=1):\n",
        "            print(f\"  Candidate {idx}: {cand}\")\n",
        "\n",
        "        # Prepare for RSA reranking (using the concatenated source text as a one-element list).\n",
        "        adaptive_reranker = AdaptiveRSAReranking(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            candidates=candidates,\n",
        "            source_texts=[source_text],\n",
        "            batch_size=8,\n",
        "            rationality=3,  # Adjust as needed.\n",
        "            device=device\n",
        "        )\n",
        "        rerank_results = adaptive_reranker.adaptive_rerank(max_iter=10, epsilon=1e-3, alpha=0.5)\n",
        "        best_rsa = rerank_results[\"best_rsa\"][0]         # Best candidate based solely on RSA.\n",
        "        best_combined = rerank_results[\"best_combined\"][0] # Best candidate based on combined RSA+BERTScore.\n",
        "\n",
        "        result_entry = {\n",
        "            \"id\": source_id,\n",
        "            \"source_text\": source_text,\n",
        "            \"gold\": gold_summary,\n",
        "            \"candidates\": candidates,\n",
        "            \"best_rsa\": best_rsa,\n",
        "            \"best_combined\": best_combined,\n",
        "            \"rsa_probs\": rerank_results[\"rsa_probs\"].cpu().numpy().tolist(),\n",
        "            \"bertscore\": rerank_results[\"bertscore\"].cpu().numpy().tolist(),\n",
        "            \"combined_score\": rerank_results[\"combined_score\"].cpu().numpy().tolist()\n",
        "        }\n",
        "        results_list.append(result_entry)\n",
        "\n",
        "    # Save results to an output CSV.\n",
        "    output_df = pd.DataFrame(results_list)\n",
        "    output_df.to_csv(output_csv, index=False)\n",
        "    print(f\"Results saved to {output_csv}\")\n",
        "\n",
        "# Run the main pipeline\n",
        "main()"
      ],
      "metadata": {
        "id": "sMDe432HLh2W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79dfa151-d6bd-47b7-8895-806135200084"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 103 unique ids.\n",
            "Processing id: https://openreview.net/forum?id=B1TTpYKgx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied. The paper is too obscure and too long. The work may have some interesting ideas but it does not seem to be properly replaced in context.\n",
            "  Candidate 2: This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions. There doesn't seem to be a solid justification for why the newly introduced measures of expressivity really measure expressivity.\n",
            "  Candidate 3: This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies and trajectory length.\n",
            "  Candidate 4: This paper presents a theoretical and empirical approach to the problem of understanding the expressivity of deep networks. Random networks (deep networks with random Gaussian weights, hard tanh or ReLU activation) are studied according to several criterions: number of neutron transitions, activation patterns, dichotomies.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.173798\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=B1jnyXXJx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods. It would be very valuable to think of a clear diagram illustrating how it differs from momentum.\n",
            "  Candidate 2: This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods. Authors of the paper show results over several different datasets.\n",
            "  Candidate 3: This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods. It would be very valuable to think of a clear diagram illustrating how this differs from momentum.\n",
            "  Candidate 4: This paper proposes a novel method for accelerating optimization near saddle points. The basic idea is to repel the current parameter vector from a running average of recent parameter values. This method is shown to optimize faster than a variety of other methods. Authors of the paper show results over several different dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.154095\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJ3filKll\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. The results are novel to the best of my knowledge, but they are hardly surprising given what we already know about the representational power of deep nets. It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions on the data manifold are vacuous.\n",
            "  Candidate 2: The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. The results are novel to the best of my knowledge, but they are hardly surprising given what we already know about the representational power of deep nets. It is difficult to see how the analysis generalizes to more complex data in which local linearity assumptions are vacuous.\n",
            "  Candidate 3: The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. The results are novel to the best of my knowledge, but they are hardly surprising given what we already know about the representational power of deep nets. It is difficult to see how the analysis generalizes to more complex data.\n",
            "  Candidate 4: The paper presents an analysis of the ability of deep networks with ReLU functions to represent particular types of low-dimensional manifolds. The results are novel to the best of my knowledge, they are hardly surprising given what we already know about the representational power of deep nets.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.217466\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJ46w6Ule\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper addresses the problem of learning compact binary data representations. While the results seem promising, the paper exposition needs significant improvement. The horse data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize. This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen.\n",
            "  Candidate 2: The paper addresses the problem of learning compact binary data representations. While the results seem promising, the paper exposition needs significant improvement. The experiments are only illustrative. They don’t compare with other methods (such as an RBM or VAE) nor do they give any quantitative results. We are left with eyeballing some images.\n",
            "  Candidate 3: The paper addresses the problem of learning compact binary data representations. While the results seem promising, the paper exposition needs significant improvement. The data set is quite small with respect to the feature dimension, and so the conclusions may not necessarily generalize. This paper proposes a new kind of expert model where a sparse subset of most reliable experts is chosen.\n",
            "  Candidate 4: The paper addresses the problem of learning compact binary data representations. While the results seem promising, the paper exposition needs significant improvement. The experiments are only illustrative. They don’t compare with other methods (such as an RBM or VAE) nor do they give any quantitative results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.099963\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJAA4wKxg\n",
            "Generated candidates:\n",
            "  Candidate 1: The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The authors present a convincing set of results over many translation tasks and compare with very competitive baselines.\n",
            "  Candidate 2: The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. The only concern I have is that this paper perhaps fits better in an NLP conference.\n",
            "  Candidate 3: The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings. One or two figures would definitely be required to help clarify the architecture.\n",
            "  Candidate 4: The system described works comparably to bi-directional LSTM baseline for NMT, and CNN's are naturally parallelizable. Key ideas include the use of two stacked CNN's (one for each of encoding and decoding) for translation, with res connections and position embeddings.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.159655\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJO-BuT1g\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al. to train a single network. This enables to easily incorporate new styles into an existing network by fine-tuning. The quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks.\n",
            "  Candidate 2: This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al. to train a single network. This enables to easily incorporate new styles into an existing network by fine-tuning. Quality of the generated stylisations is comparable to existing feed-forward single-style transfer networks.\n",
            "  Candidate 3: This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al. to train a single network. This enables to easily incorporate new styles into an existing network by fine-tuning. The quality of the generated stylisations is comparable to existing feed-forward style transfer networks.\n",
            "  Candidate 4: This paper addresses the problem of efficient neural stylization. Instead of training a separate network for N different styles, this paper extends the instance normalization work of Ulyanov et al. to train a single network. This enables to easily incorporate new styles into an existing network by fine-tuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.121370\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJh6Ztuxl\n",
            "Generated candidates:\n",
            "  Candidate 1: The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common embedding strategies.\n",
            "  Candidate 2: The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentenceembedding strategies.\n",
            "  Candidate 3: The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embeddings.\n",
            "  Candidate 4: The authors present a methodology for analyzing sentence embedding techniques. They examine several popular embedding methods including autoencoding LSTMs, averaged word vectors, and skip-thought vectors. The experiments are thorough and provide interesting insights into the representational power of common sentence embeding strategies.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.017260\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJm4T4Kgx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper investigates the phenomenon of the adversarial examples and adversarial training on the dataset of ImageNet. The authors also uncover and explain the label leaking effect which is an important contribution. The paper is well written and easy to follow. Although I still have some concerns about the paper, this paper has good contributions and worth to publish.\n",
            "  Candidate 2: This paper investigates the phenomenon of the adversarial examples and adversarial training on the dataset of ImageNet. The authors also uncover and explain the label leaking effect which is an important contribution. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments. The paper is well written and easy to follow.\n",
            "  Candidate 3: This paper investigates the phenomenon of the adversarial examples and adversarial training on the dataset of ImageNet. The authors also uncover and explain the label leaking effect which is an important contribution. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.\n",
            "  Candidate 4: This paper investigates the phenomenon of the adversarial examples and adversarial training on the dataset of ImageNet. The authors also uncover and explain the label leaking effect which is an important contribution. The paper is well written and easy to follow. Although I still have some concerns about the paper, this paper has good contributions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.136633\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJuysoFeg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. Overall I think this paper is more fit as a short workshop presentation rather than a full conference paper.\n",
            "  Candidate 2: This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. The authors are using much stronger base CNN which may account for the bulk of the reported improvement.\n",
            "  Candidate 3: This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. I think the paper is more suitable for a workshop track rather than for the main conference track.\n",
            "  Candidate 4: This paper proposes a simple domain adaptation technique in which batch normalization is performed separately in each domain. The experiments demonstrate that the method compares favorably with existing methods on standard domain adaptation tasks. I think the paper is more suitable for a workshop track rather than the main conference track.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.037399\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJwFrvOeg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes to incorporate knowledge base facts into language modeling. At each time step, a word is either generated from the full vocabulary or relevant KB entities. The architecture appears sound, but the writing makes it hard to fully understand completely so I can not give a higher rating.\n",
            "  Candidate 2: This paper proposes to incorporate knowledge base facts into language modeling. At each time step, a word is either generated from the full vocabulary or relevant KB entities. The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts. This paper addresses the practical problem of generating rare or unseen words.\n",
            "  Candidate 3: This paper proposes to incorporate knowledge base facts into language modeling. At each time step, a word is either generated from the full vocabulary or relevant KB entities. The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts.\n",
            "  Candidate 4: This paper proposes to incorporate knowledge base facts into language modeling. At each time step, a word is either generated from the full vocabulary or relevant KB entities. The authors demonstrate the effectiveness on a new generated dataset WikiFacts which aligns Wikipedia articles with Freebase facts. The writing could be improved.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.233942\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BJxhLAuxg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps.\n",
            "  Candidate 2: This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature.\n",
            "  Candidate 3: This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames. The method is tested on several Atari games and is able to predict the reward quite well.\n",
            "  Candidate 4: This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.117444\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Bk67W4Yxl\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper tests various feedforward network architectures for supervised training to predict a human’s next move. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. The results are better than previously reported, but there is no mention of computational time and efficiency.\n",
            "  Candidate 2: The paper tests various feedforward network architectures for supervised training to predict a human’s next move. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. The best proposed architecture outperforms previous results on KGS move prediction dataset.\n",
            "  Candidate 3: The paper tests various feedforward network architectures for supervised training. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. The best proposed architecture outperforms previous results on KGS move prediction dataset.\n",
            "  Candidate 4: The paper tests various feedforward network architectures for supervised training to predict a human’s next move. It trains on human play data taken from KGX, augmenting the data by considering all 8 rotations/reflections of each board position. The results are better than previously reported.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.204182\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Bk8N0RLxx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques. The authors present several strategies to select a small subset of target vocabulary to work with per source sentence, which results in significant speedup.\n",
            "  Candidate 2: This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques. The authors present several strategies to select a small subset of target vocabulary to work with per source sentence.\n",
            "  Candidate 3: This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques. The results are convincing and I think this paper offers practical values to general seq2seq approaches to language tasks.\n",
            "  Candidate 4: This paper compares several strategies for guessing a short list of vocabulary for the target language in neural machine translation. The primary findings are that word alignment dictionaries work better than a variety of other techniques. It would be useful to know what the coverage rate of the actual full vocabulary would be.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.163911\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BkCPyXm1l\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR.\n",
            "  Candidate 2: The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016) This is probably not enough for ICLR.\n",
            "  Candidate 3: The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. Very similar method was proposed in Section 6 from (Hinton et al. 2016, Distilling the Knowledge in a Neural Network) The authors failed to bring the novel idea.\n",
            "  Candidate 4: The paper introduced a regularization scheme through soft-target that are produced by mixing between the true hard label and the current model prediction. The authors failed to bring the novel idea. It is very similar to (Hinton et al. 2016). This is probably not enough for ICLR. See efg for more information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.117162\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BkLhzHtlg\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition. The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.\n",
            "  Candidate 2: The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition. The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.\n",
            "  Candidate 3: The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition. The method uses a classical encoder-decoder pipeline with skip connections allowing direct communication between the encoder and the decoder on respective levels of abstraction.\n",
            "  Candidate 4: The paper presents a method for joint motion prediction and activity classification from sequences with two different applications: motion of fruit flies and online handwriting recognition. The method uses a classical encoder-decoder pipeline, with skip connections allowing direct communication between the encoder and the decoder.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.074779\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BkVsEMYel\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank. The paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced. Still, I think some the authors could make some effort to make the key concepts more accessible.\n",
            "  Candidate 2: This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank. The paper provides a highly complex algebraic machinery to analyze the type of functions covered by convolutional network.\n",
            "  Candidate 3: This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank. The paper is very technical to read, every concept is clearly stated and mathematical terminology properly introduced.\n",
            "  Candidate 4: This paper investigates the fact why deep networks perform well in practice and how modifying the geometry of pooling can make the polynomially--------sized deep network to provide a function with exponentially high separation rank. The paper is very technical to read but every concept is clearly stated and mathematical terminology properly introduced.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.334009\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BkfiXiUlg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences. The main weakness of the paper is the experiments.\n",
            "  Candidate 2: This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples. The main weakness of the paper is the experiments.\n",
            "  Candidate 3: This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.\n",
            "  Candidate 4: This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells. This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.82s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.194149\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ByOK0rwlx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. Some of this results are feasible with float representation but probably imposible for restricted representations.\n",
            "  Candidate 2: This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. In VGG-16 the method reaches compression ratios of 20x and experiences a speed-up of 15x. The paper is very well written and clearly exposes the details of the methodology and the results.\n",
            "  Candidate 3: This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights. I do need to see the results in a clear table. Original results and results when compression is applied for all the tasks.\n",
            "  Candidate 4: This paper addresses to reduce test-time computational load of DNNs. Another factorization approach is proposed and shows good results. The comparison to the other methods is not comprehensive, the paper provides good insights. I do need to see the results in a clear table.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.116249\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ByQPVFull\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012. The ideas presented in this paper are novel and show some promise, but are currently not sufficiently ablated for readers to understand what aspects of the method are important.\n",
            "  Candidate 2: This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012. The ideas presented in this paper are novel and show some promise, but are not sufficiently ablated for readers to understand what aspects of the method are important.\n",
            "  Candidate 3: This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The technique is applied in the setting of image classification with “privileged information” in the form of foreground segmentation masks. The paper proposes a modification to ConvNet training so that the feature activations before the linear classifier are divided into groups.\n",
            "  Candidate 4: This paper proposes to learn groups of orthogonal features in a convnet by penalizing correlation among features in each group. The method is evaluated on two standard and relatively large-scale vision datasets: ImageNet and PASCAL VOC 2012. The ideas presented in this paper are novel and show some promise.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.149211\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Byj72udxe\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.\n",
            "  Candidate 2: This paper proposes augmenting RNN-based language models with a pointer network. The pointer network can point to words in the recent context. The prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset, which overcomes some of the shortcomings of previous datasets.\n",
            "  Candidate 3: This paper proposes augmenting RNN-based language models with a pointer network in order to deal better with rare words. The pointer network can point to words in the recent context, and hence the prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. This work is basically a combined pointer network applied on language modelling.\n",
            "  Candidate 4: This paper proposes augmenting RNN-based language models with a pointer network. The pointer network can point to words in the recent context. The prediction for each time step is a mixture between the usual softmax output and the pointer distribution over the recent words. The paper also introduces a new language modelling dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.136204\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=BymIbLKgl\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper is generally well written and shows an interesting application of the Siamese architecture. Authors show that a contrastive loss can be used for learning representations for planar curves. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales, instead of recognizing different shapes.\n",
            "  Candidate 2: The paper is generally well written and shows an interesting application of the Siamese architecture. However, the experimental evaluation and the results show that these are rather preliminary results as not many of the choices are validated. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales.\n",
            "  Candidate 3: The paper is generally well written and shows an interesting application of the Siamese architecture. Authors show that a contrastive loss can be used for learning representations for planar curves. My biggest concern is in the choice of the negative samples, as the network basically learns only to distinguish between shapes at different scales.\n",
            "  Candidate 4: The paper is generally well written and shows an interesting application of the Siamese architecture. Authors show that a contrastive loss can be used for learning representations for planar curves. The paper sounds like an applied maths paper, but further analysis on the nature of the representation could be done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.161387\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=H1kjdOYlx\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting and the empirical results are promising. However, there are two main drawbacks of the current incarnation of this work. The experiments are not described in enough detail in the paper.\n",
            "  Candidate 2: The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting and the empirical results are promising. However, there are two main drawbacks of the current incarnation of this work.\n",
            "  Candidate 3: The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting and the empirical results are promising. The ideas presented in the paper have all been explored in other work.\n",
            "  Candidate 4: The paper presents an approach to learning shared neural representations of temporal abstractions in hierarchical RL, based on actor-critic methods. The idea of providing symbolic descriptions of tasks and learning corresponding \"implementations\" is potentially interesting. The experiments are not described in enough detail in the paper.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.283630\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=H1oyRlYgg\n",
            "Generated candidates:\n",
            "  Candidate 1: Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatterminima have better generalization ability.\n",
            "  Candidate 2: Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well. The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatterminima have better generalization ability.\n",
            "  Candidate 3: Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well. The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatter Minima have better generalization ability.\n",
            "  Candidate 4: Interesting paper, definitely provides value to the community by discussing why large batch gradient descent does not work too well. The paper is an empirical study to justify that: 1. SGD with smaller batch sizes converges to flatter minima, 2. flatterMinima have better generalization ability.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.019820\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HJ0NvFzxl\n",
            "Generated candidates:\n",
            "  Candidate 1: This seems to be the first implementation of a differentiable memory as graph. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction. The only trick that is essential for proper performance is the ‘direct reference’ , which actually has nothing to do with the graph building process.\n",
            "  Candidate 2: This seems to be the first implementation of a differentiable memory as graph. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction. Clarity is a major issue, but from an initial version that was constructive and better read by a computer than a human, the author proposed a hugely improved version.\n",
            "  Candidate 3: The main contribution of this paper seems to be an introduction of a set of differential graph transformations. This maps naturally to a task of learning a cellular automaton represented as sequence of graphs. In that task, the graph of nodes grows at each iteration, with nodes pointing to neighbors and special nodes 0/1 representing the values.\n",
            "  Candidate 4: This seems to be the first implementation of a differentiable memory as graph. The performance on the bAbI task is comparable to the best memory networks, but still worse than more traditional rule induction. The only trick that is essential for proper performance is the ‘direct reference’\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.193452\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HJ6idTdgg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search. The proposed method is evaluated in INRIA and ETH dataset. Results are reasonable but not state-of-the-art.\n",
            "  Candidate 2: This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search. Results are shown on the INRIA and ETH pedestrian datasets. The proposed method shows good performance, but not state-of-the-art.\n",
            "  Candidate 3: This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search. Results are shown on the INRIA and ETH pedestrian datasets. The proposed method shows good performance(but not state-of-the-art).\n",
            "  Candidate 4: This paper proposes a pedestrian detection method using Fast RCNN framework with batch normalization. EdgeBoxes is used to collect pedestrian proposals instead of selective search. Results are shown on the INRIA and ETH pedestrian datasets. The proposed method shows good performance(but not state-of-the-art)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.064305\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HJ7O61Yxe\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach. Relations between trajectories are hard coded based on pre-existing knowledge. The model appears to be fit using gradient simple descent. Experimental results are positive but not convincing.\n",
            "  Candidate 2: The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach. Relations between trajectories are hard coded based on pre-existing knowledge, i.e. latent state trajectories for neighboring (wind speed) base stations should be similar.\n",
            "  Candidate 3: The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach. Relations between trajectories are hard coded based on pre-existing knowledge, i.e. latent state trajectories for neighboring (wind speed) base stations should be similar. Experimental results are not convincing.\n",
            "  Candidate 4: The paper proposes to model relational (i.e., correlated) time series using a deep learning-inspired latent variable approach. Relations between trajectories are hard coded based on pre-existing knowledge. The model appears to be fit using gradient simple descent. Experimental results are not convincing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.169234\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HJWzXsKxx\n",
            "Generated candidates:\n",
            "  Candidate 1: The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique.\n",
            "  Candidate 2: The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. The experiments do not compare with existing published results on this dataset.\n",
            "  Candidate 3: The findings of applying sparsity in the backward gradients for training LSTMs is interesting. The paper seems incomplete without the proper experimental justification. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique.\n",
            "  Candidate 4: The findings of applying sparsity in the backward gradients for training LSTMs is interesting. But the paper seems incomplete without the proper experimental justification. Proper testing results and commonly reported evaluation criterion need to be included to support the claim of no degradation when applying the proposed sparsity technique.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.291264\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HJeqWztlg\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me. This paper presents a generative model for binary images by placing a set of binary features at locations.\n",
            "  Candidate 2: The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me. The claim that this paper is the first to discover such parts should be removed.\n",
            "  Candidate 3: The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images. Unfortunately the experimental results are on smaller scale data and extension of the proposed algorithm to more natural images seems non-trivial to me. I'll reconsider my score in light of the answers.\n",
            "  Candidate 4: The paper discusses a method to learn interpretable hierarchical template representations from given data. The authors illustrate their approach on binary images. Unfortunately the experimental results are on smaller scale data. An extension of the proposed algorithm to more natural images seems non-trivial to me.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.28s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.189767\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HJpfMIFll\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD) The experiments in this paper done on SemEval-2010 are not very persuasive.\n",
            "  Candidate 2: The paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD) The experiments in this paper done on SemEval- 2010 are not very persuasive.\n",
            "  Candidate 3: The paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold). On the minus side, the paper mostly ignores the long history of Word Sense Induction (WSI) and Word Sense Disambiguation (WSD). The experiments in this paper done on SemEval-2010 are not very persuasive.\n",
            "  Candidate 4: The paper proposes a mathematically interesting model for a context of a word (i.e., a Grassmanian manifold) The experiments in this paper done on SemEval-2010 are not very persuasive. The V-measure seems to be tilted towards systems that have high number of senses per word.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.111499\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Hk8TGSKlg\n",
            "Generated candidates:\n",
            "  Candidate 1: This work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art. The approach is interesting, and result is good. The concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond Cloze-style QA.\n",
            "  Candidate 2: This work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art. The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one. The approach is interesting, and result is good.\n",
            "  Candidate 3: This work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art. The approach is interesting, and result is good. The concept of read/compose/write operations seem to be more general and can be potentially applied to other reasoning tasks beyond cloze-style QA.\n",
            "  Candidate 4: This work introduces a novel memory based artificial neural network for reading comprehension. Experiments show improvement on state of the art. The originality of the approach seems to be on the implementation of an iterative procedure with a loop testing that the current answer is the correct one.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.112020\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HkNKFiGex\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents two main contributions: A novel model visualization and photo manipulation technique that allows to transform an image using a paintbrush, much like in an image editing software. The main problem I have with the paper is that it feels very much like two papers in one with a very loose story tying the two together. I think the paper proposes interesting ideas, but given its lack of focus on a single, cohesive story I think it is not yet ready for publication.\n",
            "  Candidate 2: This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling architectures.\n",
            "  Candidate 3: This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm along with various modifications to standard generative modeling.\n",
            "  Candidate 4: This paper proposes a hybridization of a VAE and a GAN whereby the generator both generates random samples and produces reconstructions of the real data. The discriminator attempts to classify each true data point, sample, and reconstruction as being real, fake, or reconstructed. It also proposes a user-facing interface with an interactive image editing algorithm.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.40s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.289108\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HkljfjFee\n",
            "Generated candidates:\n",
            "  Candidate 1: The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. The paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective. The improvements relative to standard Sparse Coding seem very small.\n",
            "  Candidate 2: The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. The paper is well written, and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.\n",
            "  Candidate 3: The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. The paper is well written and pretty complete. It is not extremely original in its main ideas though, but the actual algorithm and implementation seem new and effective.\n",
            "  Candidate 4: The work proposes to use the geometry of data (that is considered to be known a priori) in order to have more consistent sparse coding. The paper is well written, and pretty complete. It is not extremely original in its main ideas though. The improvements relative to standard Sparse Coding seem very small.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.269687\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HkwoSDPgg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data. The student performs prediction based on public data labeled by teachers through noisy voting. The work advances the state of the art on differentially-private deep learning, is quite well-written and relatively thorough.\n",
            "  Candidate 2: This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data. The student performs prediction based on public data labeled by teachers through noisy voting. The work advances the state of the art on differentially-private deep learning.\n",
            "  Candidate 3: This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. The work advances the state of the art on differentially-private deep learning, is quite well-written and relatively thorough.\n",
            "  Candidate 4: This paper addresses the problem of achieving differential privacy in a very general scenario where a set of teachers is trained on disjoint subsets of sensitive data and the student performs prediction based on public data labeled by teachers through noisy voting. The work advances the state of the art on differentially-private deep learning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.175947\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HkzuKpLgg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. Overall, the results presented in the paper are interesting, but the writing can be improved. The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature.\n",
            "  Candidate 2: This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. Overall, the results presented in the paper are interesting, but the writing can be improved. The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred to as ring based approach in Allreduce literature.\n",
            "  Candidate 3: This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. Overall, the results presented in the paper are interesting, but the writing can be improved. The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred to ring based approach in Allreduce literature.\n",
            "  Candidate 4: This paper presents a linear pipeline All-reduce approach for parallel neural networks on multiple GPU. Overall, the results presented in the paper are interesting, but the writing can be improved. The name linear pipeline is somewhat confusing to the readers as the technique is usually referred as ring based approach in Allreduce literature.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.96s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.030736\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Hy3_KuYxg\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper is about using the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. The writing style pictures the method as very general, but falls back on very low level details specific to each task. High level ideas are mixed with low-level tricks required to get the model to work. The solution isn’t generic enough to be applicable to unknown problems - the networks require tricks specific to\n",
            "  Candidate 2: The paper is about using the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. The writing style pictures the method as very general, but falls back on very low level details specific to each task. High level ideas are mixed with low-level tricks required to get the model to work. The solution isn’t generic enough to be applicable to unknown problems. The main promise of the paper\n",
            "  Candidate 3: The paper is about using the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. The writing style pictures the method as very general, but falls back on very low level details specific to each task. High level ideas are mixed with low-level tricks required to get the model to work. The solution isn’t generic enough to be applicable to unknown problems.\n",
            "  Candidate 4: The paper is about using the powerful “divide and conquer” algorithm design strategy to learn better programs for tasks such as sorting or planar convex hull. The writing style pictures the method as very general, but falls back on very low level details specific to each task. High level ideas are mixed with low-level tricks required to get the model to work. The solution isn’t generic enough to be applicable to unknown problems. The paper would be much more\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.054675\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HyAbMKwxe\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper analyses the misclassification error of discriminators. While uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. As a fix, an optimization procedure based on recomputing the bound is proposed.\n",
            "  Candidate 2: The paper analyses the misclassification error of discriminators. While uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. As a fix, an optimization procedure based on recomputing the bound is proposed. The paper is well written.\n",
            "  Candidate 3: The paper analyses the misclassification error of discriminators. While uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. The optimized upper bound (log-loss) gets looser as a fix.\n",
            "  Candidate 4: The paper analyses the misclassification error of discriminators. While uniform probability prior of the classes makes sense early in the optimization, the distribution deviates from this prior significantly as the parameters move away from the initial values. The optimized upper bound (log-loss) gets looser.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.170067\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HyAddcLge\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposed a synchronous parallel SGD by employing several backup machines. The idea is very simple, but in practice it can be quite useful in industry settings. I suggest two revisions: provide more experiments to show the performance with different efficiency distributions of learners, and cut off updates of too much staledness just as the proposed method does.\n",
            "  Candidate 2: This paper proposed a synchronous parallel SGD by employing several backup machines. The idea is very simple, but in practice it can be quite useful in industry settings. I suggest two revisions: provide more experiments to show the performance with different efficiency distributions of learners, and cut off updates of too much staledness just as the proposed method.\n",
            "  Candidate 3: This paper proposed a synchronous parallel SGD by employing several backup machines. The idea is very simple, but in practice it can be quite useful in industry settings. I suggest two revisions: provide more experiments to show the performance with different efficiency distributions of learners, and cut off updates of too much staledness.\n",
            "  Candidate 4: This paper proposed a synchronous parallel SGD by employing several backup machines. The idea is very simple, but in practice it can be quite useful in industry settings. I suggest two revisions: provide more experiments to show the performance with different efficiency distributions of learners, and cut off updates too much.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.149386\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HyET6tYex\n",
            "Generated candidates:\n",
            "  Candidate 1: The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\" The idea of the described universality is very interesting. However I see several shortcomings in the paper. In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. The dependence of epsilon on N is troubling.\n",
            "  Candidate 2: The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\" The idea of the described universality is very interesting. However I see several shortcomings in the paper. In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one. I found the paper quite hard to read.\n",
            "  Candidate 3: The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\" The idea of the described universality is very interesting. However I see several shortcomings in the paper. In order to be of practical relevance, the actual stopping time might be more relevant than the scaled one.\n",
            "  Candidate 4: The authors explore whether the halting time distributions for various algorithms in various settings exhibit \"universality\" The idea of the described universality is very interesting. However I see several shortcomings in the paper. For example, the actual stopping time might be more relevant than the scaled one.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.211382\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HyEeMu_xx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes an attention mechanism which is essentially a gating on every spatial feature. The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections] There is a lack of novelty and no significant results.\n",
            "  Candidate 2: This paper proposes an attention mechanism which is essentially a gating on every spatial feature. The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections] There is no significant performance gain on any standard datasets.\n",
            "  Candidate 3: This paper proposes an attention mechanism which is essentially a gating on every spatial feature. The idea of progressive attention on features is good, but has been done in [Spatial Transformer Networks, Deep Networks with Internal Selective Attention through Feedback Connections] The authors provide results on a synthetic dataset in addition to doing attribute prediction.\n",
            "  Candidate 4: This paper presents a hierarchical attention model that uses multiple stacked layers of soft attention in a convnet. The authors provide results on a synthetic dataset in addition to doing attribute prediction on the Visual Genome dataset. The approach is novel and interesting to my knowledge and speaks for acceptance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.118160\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=HyQWFOVge\n",
            "Generated candidates:\n",
            "  Candidate 1: I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method because the loss function is designed to assign the correct class for the given image. All the experimental results show that the softmax features work better than Rippel et al DML method.\n",
            "  Candidate 2: I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method. All the experimental results show that the softmax features work better than Rippel et al DML method. But does it support the claim that softmax-based features work much better than DML learned features?\n",
            "  Candidate 3: I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method. All the experimental results show that the softmax features work better than Rippel et al DML method. But does it support the claim that softmax-based features work much better than DML learned features? I have doubts.\n",
            "  Candidate 4: I agree with the other two reviewers that it is an interesting topic to investigate the feature learned by DML. For classification task though, I feel intuitively softmax should have advantages over distance metric learning method. All the experimental results show that the softmax features work better than Rippel et al DML method.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.185714\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1AG8zYeg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper extends the \"order matters\" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering. While I am not entirely convinced by the task of sentence ordering, this approach seems promising to learn sentence representations.\n",
            "  Candidate 2: This paper extends the \"order matters\" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering. The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques.\n",
            "  Candidate 3: This paper extends the \"order matters\" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering. The model described in the paper does not really introduce anything new, but is a natural combination of existing techniques to solve the task of sentence Order.\n",
            "  Candidate 4: This paper extends the \"order matters\" idea in (Vinyals et al., 2015) from the sentence level to an interesting application on discourse level. Experiments in this paper show the capacity of the proposed model on both order discrimination task and sentence ordering. While I am not entirely convinced by the task of sentence ordering, this approach seems promising.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.152845\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1Bm3T_lg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a new learning model \"Compositional Kernel Machines (CKMs)\" That extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. In NORB compositions, CKMs seem to be better than convnets at classifying images. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.\n",
            "  Candidate 2: This paper proposes a new learning model \"Compositional Kernel Machines (CKMs)\" That extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. In NORB compositions, CKMs seem to be better than convnets at classifying images by their dominant objects.\n",
            "  Candidate 3: This paper proposes a new learning model \"Compositional Kernel Machines (CKMs)\" That extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. The paper was an interesting read, with thoughtful methodology, but has partially unsupported and potentially misleading claims.\n",
            "  Candidate 4: This paper proposes a new learning model \"Compositional Kernel Machines (CKMs)\" That extends the classic kernel machines by constructing compositional kernel functions using sum-product networks. In NORB compositions, CKMs seem to be better than convnets at classifying images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.343297\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1HEBe_Jl\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.\n",
            "  Candidate 2: The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key, while Eve should be unable to encrypt the message. Experiments are performed in a simple symmetric 16 bit encryption task and an application on privacy.\n",
            "  Candidate 3: The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages. Experiments are performed in a simple symmetric 16 bit encryption task, and an application on privacy.\n",
            "  Candidate 4: The paper deals with an interesting application of adversarial training to encryption. It considers the standard scenario of Alice, Eve and Bob, where A and B aim to exchange messages conditioned on a shared key. Experiments are performed in a simple symmetric 16 bit encryption task.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.271143\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1J0E-71l\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. The experiment is only conducted on one dataset, reporting state-of-the-art result. There are already more than four papers reporting better numbers than the one reported in this task, however the author did not cite them.\n",
            "  Candidate 2: This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. The experiment is only conducted on one dataset, reporting state-of-the-art result. There are already more than four papers reporting better numbers than the one reported in this task.\n",
            "  Candidate 3: This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. The experiment is only conducted on one dataset, reporting state-of-the-art result, but unfortunately this is not true.\n",
            "  Candidate 4: This paper proposes to use previous error signal of the output layer as an additional input to recurrent update function in order to enhance the modelling power of a dynamic system such as RNNs. The experiment is only conducted on one dataset, reporting state-of-the-art result.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.263713\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1Jhfftgx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, I didn't find the approach very convincing and the paper has a few problems regarding the empirical evaluation.\n",
            "  Candidate 2: This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured prediction energy networks (SPENs), recently proposed by Belanger et al.\n",
            "  Candidate 3: This paper proposes a way of enforcing constraints (or penalizing violations of those constraints) on outputs in structured prediction problems. The idea is to tweak the neural network parameters to make those output constraints hold. The underlying model is that of structured prediction energy networks (SPENs)\n",
            "  Candidate 4: This paper attempted to solve an interesting problem -- incorporating hard constraints in seq2seq model. The main idea is to modify the weight of the neural network in order to find a feasible solution. Overall, I didn't find the approach very convincing and the paper has a few problems.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.153894\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1OufnIlx\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. Some observations of the experiments are interesting, for example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. In some sense, the Sharif et al. work \"scooped\" this paper, but the spirit of the work remains somewhat different.\n",
            "  Candidate 2: The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. Some observations of the experiments are interesting, for example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods. The results of this paper seems not really improving the understanding of the adversarial example phenomenon.\n",
            "  Candidate 3: The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. This method could be useful when the number of classes in the dataset is huge. The results of this paper seems not really improving the understanding of the adversarial example phenomenon.\n",
            "  Candidate 4: The paper proposes the iterative LL method, which is efficient in both computation and success rate in generating adversarial examples. Some observations of the experiments are interesting, for example, overall photo transformation does not affect much the accuracy on clean image, but could destroy some adversarial methods.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.290578\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1RP6GLle\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents a new framework to solve the SR problem - amortized MAP inference. It adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solving the problem of minimizing cross-entropy.\n",
            "  Candidate 2: The paper presents a new framework to solve the SR problem - amortized MAP inference. It adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solving the problem of minimizing cross-entropy. Overall, it is a great paper.\n",
            "  Candidate 3: The paper presents a new framework to solve the SR problem - amortized MAP inference. It adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solving the problem of minimize cross-entropy.\n",
            "  Candidate 4: The paper presents a new framework to solve the SR problem - amortized MAP inference. It adopts a pre-learned affine projection layer to ensure the output is consistent with LR. Also, it proposes three different methods to solve  the problem of minimizing cross-entropy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.19s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.190140\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1di0sfgl\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated within a recurrent neuralnetwork framework, and shows the state-of-the-art performance on several benchmarks.\n",
            "  Candidate 2: This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The authors propose using the straight-through estimator with a slope-annealing trick during training.\n",
            "  Candidate 3: This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated and shows the state-of-the-art performance on several benchmarks. The paper is well written.\n",
            "  Candidate 4: This paper proposes a new multiscale recurrent neural network, where each layer has different time scale, and the scale is not fixed but variable and determined by a neural network. The method is elegantly formulated and shows the state-of-the-art performance on several benchmarks.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.158891\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1j4RqYxg\n",
            "Generated candidates:\n",
            "  Candidate 1: The algorithm is interesting, quite simple and nice, with many possible applications. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing. The fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets.\n",
            "  Candidate 2: Algorithm is interesting, quite simple and nice, with many possible applications. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing. The fit with the main scope of ICLR is far from obvious with this work, that should probably re-submitted to better targets.\n",
            "  Candidate 3: The algorithm is interesting, quite simple and nice, with many possible applications. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing. The fit with the main scope of ICLR is far from obvious with this work, that should probably be re-submitted to better targets.\n",
            "  Candidate 4: The algorithm is interesting, quite simple and nice, with many possible applications. The main contribution of this work is not significant enough. The experiments are incomplete and not convincing. The fit with the main scope of ICLR is far from obvious with this work. Many details of the experimental settings are missing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.042356\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1jmAotxg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper presents an approach which modifies the variational auto-encoder framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. The resulting model is named the SB-VAE which also has a semi-supervised extension.\n",
            "  Candidate 2: This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. The resulting model is named the SB-VAE and has a semi-supervised extension.\n",
            "  Candidate 3: This paper presents an approach which modifies the variational auto-encoder (VAE) framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. The resulting model is named the SB-VAE.\n",
            "  Candidate 4: This paper presents an approach which modifies the variational auto-encoder framework so as to use stochastic latent dimensionality. This is achieved by using an inherently infinite prior, the stick-breaking process. The resulting model is named the SB-VAE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.187722\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1vyujVye\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patch from diferent images.\n",
            "  Candidate 2: This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score from diferent images.\n",
            "  Candidate 3: This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. building triplets from patches of input images and learning a presentation that assigns a high score for patches coming from the same image and a low score for patch from diferent images.\n",
            "  Candidate 4: This paper presents a novel way to do unsupervised pretraining in a deep convolutional network setting. The method is that of ‘spatial constrasting’, i.e. of building triplets from patches of input images and learning a presentation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.123796\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=S1xh5sYgx\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitectures and microarchitecture to develop Squeezenet, which is composed of fire modules. The architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks.\n",
            "  Candidate 2: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitectures and microarchitecture to develop Squeezenet, which is composed of fire modules. The architecture was only tested on ImageNet, so it's unclear whether the ideas transfer to other tasks.\n",
            "  Candidate 3: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitectures and microarchitecture to develop Squeezenet, which is composed of fire modules. Achieves x50 less memory usage than AlexNet while keeping similar accuracy.\n",
            "  Candidate 4: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitectures and microarchitecture to develop Squeezenet, which is composed of fire modules. Achieves x50 less memory usage than AlexNet, keeping similar accuracy.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.69s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.093805\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJAr0QFxe\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model. I don't agree with the claims in the paper on non-linear networks.\n",
            "  Candidate 2: This paper shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I do not think the paper has fully resolved the linear vs nonlinear issue. The behavior for when weights are 0 is not revealing of how the model works in general.\n",
            "  Candidate 3: This paper shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model. I don't agree with the claims in the paper on non- linear networks.\n",
            "  Candidate 4: This paper shows mathematically that for 2-shortcuts and zero initialization, the Hessian has condition number independent of depth. I think the work also underestimates the effect of the nonlinearities on the learning dynamics of the model. The results of the paper also might be somewhat misleading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.108263\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJDaqqveg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. experiments are shown in a synthetic denoising task as well as in machine translation. While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor.\n",
            "  Candidate 2: This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. experiments are shown in a synthetic denoising task as well as in machine translation. While the idea makes sense, the authors needed to use many heuristics to make the model to work.\n",
            "  Candidate 3: This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. experiments are shown in a synthetic denoising task as well as in machine translation. While the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g. using a delayed actor.\n",
            "  Candidate 4: This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. experiments are shown in a synthetic denoising task as well as in machine translation. While the idea makes sense, the authors needed to use many heuristics to make the model work.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.181649\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJGPL9Dex\n",
            "Generated candidates:\n",
            "  Candidate 1: 'Neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding. This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010) A theoretical analysis attempts to explain the non-asymptotic acceleration property of LisTA (via Theorem 2.2. and Corollary 2.3)\n",
            "  Candidate 2: 'Neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding. This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010) A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LisTA (via Theorem 2.2 and Corollary 2.3)\n",
            "  Candidate 3: 'Neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding. This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010) A theoretical analysis is presented that attempts to explain the non-asymptotic acceleration property of LisTA.\n",
            "  Candidate 4: 'Neural sparse coding' is actually rather 'LISTA', or 'neural network acceleration of sparse coding. This paper proposes a method for neural sparse coding inspired by LISTA (Gregor and LeCun 2010) A theoretical analysis attempts to explain the non-asymptotic acceleration property of LisTA.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.62s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.217443\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJNDWNOlg\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks. The setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of the deeper VGG-19 network.\n",
            "  Candidate 2: The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks. The setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly from the usage of the deeper VGG-19 network.\n",
            "  Candidate 3: The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks. The setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters, but mainly due to the usage of a deeper VGG-19 network.\n",
            "  Candidate 4: The paper conducts a detailed evaluation of different CNN architectures applied to image retrieval. The authors focus on testing various architectural choices, but do not propose or compare to end-to-end learning frameworks. The setting of the state-of-the-art results is quite misleading as it doesn't really come from the good choice of parameters.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.43s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.113358\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJQNqLFgl\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFar-100, but seem to achieve relatively poor performance on these datasets.\n",
            "  Candidate 2: The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CifAR-100, but seem to achieve relatively poor performance on these datasets.\n",
            "  Candidate 3: The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CIFar-100, but seem to achieve relatively poor performance.\n",
            "  Candidate 4: The paper formulates a number of rules for designing convolutional neural network architectures for image processing and computer vision problems. It also proposes a few new architectural ideas inspired by these rules. These are experimentally evaluated on CIFAR-10 and CifAR-100, but seem to achieve relatively poor performance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.075696\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJRpRfKxx\n",
            "Generated candidates:\n",
            "  Candidate 1: The authors formulate a recurrent deep neural network to predict human fixation locations in videos. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features for action recognition.\n",
            "  Candidate 2: The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with C3D features for action recognition.\n",
            "  Candidate 3: The authors formulate a recurrent deep neural network to predict human fixation locations in videos as a mixture of Gaussians. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with the C3D features.\n",
            "  Candidate 4: The authors formulate a recurrent deep neural network to predict human fixation locations in videos. They train the model using maximum likelihood with actual fixation data. Apart from evaluating how good the model performs at predicting fixations, they combine the saliency predictions with C3D features for action recognition.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.126983\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJU4ayYgl\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper develops a simple and reasonable algorithm for graph node prediction/classification. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method. The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer.\n",
            "  Candidate 2: The paper develops a simple and reasonable algorithm for graph node prediction/classification. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method.\n",
            "  Candidate 3: The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method. The authors mention that more complex filters could be learned by stacking layers but they limit their architecture to one hidden layer.\n",
            "  Candidate 4: The paper develops a simple and reasonable algorithm for graph node prediction/classification. The proposed algorithm has a limited complexity and it is shown to scale well on a large dataset. The comparison with baselines on different datasets show a clear jump of performance with the proposed method. The authors mention that more complex filters could be learned by stacking layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.277652\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJqaCVLxx\n",
            "Generated candidates:\n",
            "  Candidate 1: The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. The paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard way of training.\n",
            "  Candidate 2: The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. The paper is extremely hard to follow and it is not at all clear what the training algorithm is and how is it better than standard back-prop.\n",
            "  Candidate 3: The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. Unfortunately the paper is extremely hard to understand and it is not at all clear what the exact training algorithm is.\n",
            "  Candidate 4: The authors seems to have proposed a genetic algorithm for learning the features of a convolutional network (LeNet-5 to be precise). The algorithm is validated on some version of the MNIST dataset. The paper is extremely hard to follow and it is not at all clear what the training algorithm is.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.175479\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SJx7Jrtgl\n",
            "Generated candidates:\n",
            "  Candidate 1: This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent. A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters that control their strength.\n",
            "  Candidate 2: This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent. A negative issue of this paper is that all crucial regularizations rely upon ad-hoc parameters.\n",
            "  Candidate 3: This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term).\n",
            "  Candidate 4: This submission proposes an approach to adapting the variational auto-encoder framework (VAE) to the clustering scenario. First the model has to be adapted (with a Gaussian mixture as a prior) and then the inference has to become consistent (by introducing a regularization term)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.230920\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Sk2iistgg\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections. The clarity of the paper has scope for improvement (particularly, Introduction)\n",
            "  Candidate 2: The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections. The clarity of the paper has scope for improvement.\n",
            "  Candidate 3: The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation and estimating non-rigid 3D structures from a sequence of 2D orthographic projections. The clarity of the paper has scope for improvement.\n",
            "  Candidate 4: The paper proposes a nonlinear regularizer for solving ill-posed inverse problems. Empirical results are reported on two tasks involving linear inverse problems -- missing feature imputation, and estimating non-rigid 3D structures from a sequence of 2D orthographic projections.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.137819\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Sk36NgFeg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper aims to characterize the perceptual ability of a neural network under different input conditions. It isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss. The ability to perform color reconstruction given a color glimpse has much to do with disambiguating the color of an object/scene.\n",
            "  Candidate 2: This paper aims to characterize the perceptual ability of a neural network under different input conditions. It isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss. There are some interesting measurements here, such as the amount of color needed in the foveation to reconstruct a color image.\n",
            "  Candidate 3: This paper aims to characterize the perceptual ability of a neural network under different input conditions. It isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss. The ability to perform color reconstruction given a color glimpse I think has much to do with disambiguating the color of an object.\n",
            "  Candidate 4: This paper aims to characterize the perceptual ability of a neural network under different input conditions. It isn't clear to me what reconstruction behaviors are caused by a fundamental perception of the input, and what are artifacts of the autoencoder and pixelwise l2 loss. There are some interesting measurements here, such as the amount of color needed to reconstruct a color image.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.20s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.078743\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SkBsEQYll\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. I cannot recommend it for acceptance.\n",
            "  Candidate 2: This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel.\n",
            "  Candidate 3: This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. It proposes to use feed-forward neural networks to learn similarity preserving embeddings. The evaluation of the method is far from convincing. I cannot recommend it for acceptance.\n",
            "  Candidate 4: This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity. While this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture is far from novel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.199699\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SkgSXUKxx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes and it was shown to aid low-shot learning performance. The paper provides two examples where a feature penalty favors a better representation. The connection with Batch Normalization could have broader impact.\n",
            "  Candidate 2: This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes. It was shown to aid low-shot learning performance. The paper provides two examples where a feature penalty--------favors a better representation.\n",
            "  Candidate 3: This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes and it was shown to aid low-shot learning performance. The paper provides two examples where a feature penalty Favors a better representation.\n",
            "  Candidate 4: This paper extends and analyzes the gradient regularizer of Hariharan and Girshick 2016. In that paper a regularizer was proposed which penalizes gradient magnitudes and it was shown to aid low-shot learning performance. The paper provides two examples where a feature penalty favors a better representation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.229536\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SkpSlKIel\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network. The paper is well written and clear. The arguments and proofs are easy to follow.\n",
            "  Candidate 2: This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network. The paper is well written and clear.\n",
            "  Candidate 3: This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network that uses O(log(1/eps) layers.\n",
            "  Candidate 4: This paper contributes to the description and comparison of the representational power of deep vs shallow neural networks with ReLU and threshold units. The main contribution of this paper is a construction to eps-approximate a piecewise smooth function with a multilayer neural network.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.199411\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SkqMSCHxe\n",
            "Generated candidates:\n",
            "  Candidate 1: The architecture allows several RNNs to compete to make the best predictions. Preliminary experimental results show that this scheme can yield reduced prediction error. It is not clear how the best-performing RNN is chosen for each time point at test time. It seems possible that a larger generic RNN might be able to generate accurate predictions.\n",
            "  Candidate 2: This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. The architecture allows several RNNs to compete to make the best predictions, with only the best receiving back propagation training at each time step. Preliminary experimental results show that this scheme can yield reduced prediction error.\n",
            "  Candidate 3: The architecture allows several RNNs to compete to make the best predictions. Preliminary experimental results show that this scheme can yield reduced prediction error. It is not clear how the best-performing RNN is chosen for each time point at test time. The paper is understandable but could benefit from some copy editing.\n",
            "  Candidate 4: This paper proposes a neural network architecture for car state prediction while driving based on competitive learning. The architecture allows several RNNs to compete to make the best predictions. Preliminary experimental results show that this scheme can yield reduced prediction error. The paper is understandable but could benefit from some copy editing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.156447\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Sy1rwtKxg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations. The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.\n",
            "  Candidate 2: This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations. The motivation is to recover the same effect compared with sequential SGD, by using a proposed sound combiner. Experiments shows the proposed method has better speedup than previous methods like Hog wild! and Allreduce.\n",
            "  Candidate 3: This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations. The motivation is to recover the same effect compared with sequential SGD. Experiments shows the proposed method has better speedup than previous methods like Hogwild! and Allreduce.\n",
            "  Candidate 4: This paper propose a parallel mechanism for stochastic gradient descent method (SGD) in case of gradient can be computed via linear operations. The idea in this paper is interesting and the paper is well-written and well-motivated. However, I think it is not ready to publish in ICLR for the following reasons.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.24s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.131373\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Sy7m72Ogg\n",
            "Generated candidates:\n",
            "  Candidate 1: The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP. The method is presented against popular adaptive first-order methods for training deep networks. The results are interesting but difficult to assess in a true apples-to-apples manner.\n",
            "  Candidate 2: The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks (Adagrad, Adam, RMSProp, etc)\n",
            "  Candidate 3: The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The results are interesting but difficult to assess in a true apples-to-apples manner.\n",
            "  Candidate 4: The authors present a method for adaptively setting the step size for SGD by treating the learning rate as an action in an MDP whose reward is the change in loss function. The method is presented against popular adaptive first-order methods for training deep networks. The results are interesting but difficult to assess.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.121266\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SyCSsUDee\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively. Overall, I believe the proposed method is not very well justified and has limited novelty.\n",
            "  Candidate 2: The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively. The proposed feature augmentation sounds like simply adding gaussian noise to the pre-softmax neurons.\n",
            "  Candidate 3: The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done. Overall, I believe the proposed method is not very well justified and has limited novelty.\n",
            "  Candidate 4: The paper introduces supervised deep learning with layer-wise reconstruction loss (in addition to the supervised loss) and class-conditional semantic additive noise. Experiments on MNIST and CIFAR-10 datasets while changing the number of training examples per class are done extensively.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.14s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.129336\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SyEiHNKxx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics. The key selling point of the proposed software is its speed, however there is no comparison to other physics engines.\n",
            "  Candidate 2: This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics. The key selling point of the proposed software is its speed, but there is no comparison to other physics engines.\n",
            "  Candidate 3: This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics. However, the proposed method is only useful for robotics if the learned policy can transfer the real world.\n",
            "  Candidate 4: This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics. The key selling point of the proposed software is its speed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.107184\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SygvTcYee\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing the parameters to move between machines.\n",
            "  Candidate 2: The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. While the speedup factors are encouraging, it is hard to get a sense of their importance as the binary autoencoder model considered is not well studied by other researchers.\n",
            "  Candidate 3: The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines allowing parameters to move between machines.\n",
            "  Candidate 4: The paper presents an architecture to parallelize the optimization of nested functions based on the method of auxiliary coordinates (MAC) This method decomposes the optimization into training individual layers and updating the auxiliary coordinates. The paper focuses on binary autoencoders and proposes to partition the data onto several machines.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.239553\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=SypU81Ole\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results. I'm very much on the fence since I think the techniques are useful and this paper should be read by those interested in generating modeling.\n",
            "  Candidate 2: This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results. I'm not sure the paper is appropriate for an ICLR conference track as it doesn't provide any greater theoretical insights.\n",
            "  Candidate 3: This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results. Overall, I'm very much on the fence since I think the techniques are useful.\n",
            "  Candidate 4: This paper proposes a variety of techniques for visualizing learned generative models, focussing specifically on VAE and GAN models. This paper does not have one single clear message or idea, but rather proposed a set of techniques that seem to produce visually good looking results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.202004\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=Sywh5KYex\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness of this incremental approach.\n",
            "  Candidate 2: This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation. It is a natural simplification of highway networks to allow easily \"shutting off\" layers while keeping additional parameters low.\n",
            "  Candidate 3: This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation. It is a natural simplification of highway networks to allow easily \"shutting off\" layers.\n",
            "  Candidate 4: This paper proposes a network called Gated Residual Networks layer design that adds gating to shortcut connections with a scalar to regulate the gate. The claim is that such gating is easier to learn and allows a network to flexibly utilize computation. It is not sufficiently novel and the empirical results do not prove sufficient effectiveness.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.121569\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r10FA8Kxg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification. To this end the authors conducted a series of experiments on the CIFAR10 dataset. They find that there is a significant performance gap in favour of deep CNNs.\n",
            "  Candidate 2: This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification. To this end the authors conducted a series of experiments on the CIFAR10 dataset. They find that there is a significant performance gap between the two approaches.\n",
            "  Candidate 3: This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification. To this end the authors conducted a series of experiments on the CIFAR10 dataset. They find that there is a significant performance gap between the two approaches, in favour of deep CNN.\n",
            "  Candidate 4: This paper aims to investigate the question if shallow non-convolutional networks can be as affective as deep convolutional ones for image classification. To this end the authors conducted a series of experiments on the CIFAR10 dataset. The results show that there is a significant performance gap between the two approaches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.106017\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1X3g2_xl\n",
            "Generated candidates:\n",
            "  Candidate 1: The authors propose to apply virtual adversarial training to semi-supervised classification. The proposed method is simple and effective and can be easily be applied after reading the paper. The paper is in general well written and easy to follow. Theoretical and experimental comparison with past work is missing. Overall the paper is worth to publish.\n",
            "  Candidate 2: The authors propose to apply virtual adversarial training to semi-supervised classification. The proposed method is simple and effective and can be easily be applied after reading the paper. The paper is in general well written and easy to follow. Overall the paper is worth to publish. Theoretical and experimental comparison with past work is missing.\n",
            "  Candidate 3: The authors propose to apply virtual adversarial training to semi-supervised classification. The proposed method is simple and effective and can be easily be applied after reading the paper. The paper is in general well written and easy to follow. Overall the paper is worth to publish.\n",
            "  Candidate 4: The authors propose to apply virtual adversarial training to semi-supervised classification. The proposed method is simple and effective and can be easily be applied after reading the paper. The paper is in general well written and easy to follow. Theoretical and experimental comparison with past work is missing.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.188865\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1aPbsFle\n",
            "Generated candidates:\n",
            "  Candidate 1: This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. The idea of sharing or tying weights between input and output word embeddings is not new (as noted by others in this thread)\n",
            "  Candidate 2: This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. The idea of sharing or tying weights between input and output word embeddings is not new.\n",
            "  Candidate 3: This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. The experimental results are good and provide support for the approximate derivation done in section 4.\n",
            "  Candidate 4: This work offers a theoretical justification for reusing the input word embedding in the output projection layer. It does by proposing an additional loss that is designed to minimize the distance between the predictive distribution and an estimate of the true data distribution. This is a nice setup since it can effectively smooth over the labels given as input.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.192488\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1fYuytex\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. Using random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun and others.\n",
            "  Candidate 2: The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. Using random connections is not a new idea in CNNs. The sparsity in fully connected layer decreases the computational burden but it is difficult to speed up.\n",
            "  Candidate 3: The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. Using random connections is not a new idea in CNNs. It was used between CNN layers in a 1998 paper by Yann LeCun.\n",
            "  Candidate 4: The paper proposes a sparsely connected network and an efficient hardware architecture that can save up to 90% of memory compared to the conventional implementations of fully connected neural networks. The results looks good but the baselines proposed are quite bad. The authors reply still does not convince me.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.109824\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1kGbydxg\n",
            "Generated candidates:\n",
            "  Candidate 1: Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies.\n",
            "  Candidate 2: Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. The video is nice.\n",
            "  Candidate 3: Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies. It is illustrated that more abstract parameterizations are in fact better and result in more robust and high quality policies.\n",
            "  Candidate 4: Paper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations have on the performance of reinforcement learning and optimized control policies. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality Policies.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.116543\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1kQkVFgl\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. The paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. They release a Python open source dataset.\n",
            "  Candidate 2: This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. The paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers. The construction and filtering of the Python corpus sounds promising.\n",
            "  Candidate 3: This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. The paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types, like identifiers.\n",
            "  Candidate 4: This paper uses a pointer network over a sparse window of identifiers to improve code suggestion for dynamically-typed languages. The paper takes a standard auto-regressive model of source code and augments it with a fixed attention policy that tracks the use of certain token types.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.208074\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1osyr_xg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main weaknesses of the paper are shortcomings in the experimental evaluation and in the model exploration. While the paper is grammatically mostly correct, it would benefit from revision with the help of a native English speaker.\n",
            "  Candidate 2: This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods. The results are somewhat inconclusive.\n",
            "  Candidate 3: This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The evaluation does not convincingly determine whether the model is a significant improvement over simpler methods.\n",
            "  Candidate 4: This paper proposes a method for estimating the context sensitivity of paraphrases and uses that to inform a word embedding learning model. The main idea and model are presented convincingly and seem plausible. The evaluation does not convincingly determine whether the model is a significant improvement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.200608\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1rhWnZkg\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) The proposed model outperforms the current state-of-art on VQA by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below)\n",
            "  Candidate 2: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) The proposed model outperforms the current state-of-art on VQA by 0.42%. However, I have concerns about the statistical significance of the performance (see weaknesses below).\n",
            "  Candidate 3: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) The proposed model outperforms the current state-of-art on VQA by 0.42%. However, I have concerns about the statistical significance of the performance.\n",
            "  Candidate 4: The paper presents low-rank bilinear pooling that uses Hadamard product (commonly known as element-wise multiplication) The proposed model outperforms the current state-of-art on VQA by 0.42%. However, I am skeptical that the performance of the proposed model is statistically significantly better than theCurrent state- of-art.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.18s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.186928\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1tHvHKge\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\" The paper is well written including some rather poetic language. The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature.\n",
            "  Candidate 2: This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\" The paper is well written including some rather poetic language. The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. The approach presented is not very principled.\n",
            "  Candidate 3: This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\" The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. Overall, the approach presented is not very principled.\n",
            "  Candidate 4: This paper presents a heuristic for avoiding large negative rewards which have already been experienced by distilling such events into a \"danger model\" The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. The paper addresses an important and timely topic in a creative way.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.169350\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1xUYDYgg\n",
            "Generated candidates:\n",
            "  Candidate 1: The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable. The performance of sukiyaki2 vs AMD's Caffe port is impressive. There is no new algorithm here, nor is there any UI/ meta-design improvement to make it easier to design and train neural network systems.\n",
            "  Candidate 2: The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable. The performance of sukiyaki2 vs AMD's Caffe port is impressive. There is no new algorithm here, nor is there any UI/meta-design improvement to make it easier to design and train neural network systems.\n",
            "  Candidate 3: The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable. The performance of sukiyaki2 vs AMD's Caffe port is impressive. There is no new algorithm here, nor is there any UI/ meta-design improvement.\n",
            "  Candidate 4: The grand vision of a DLTraining@Home is exciting. Having a solid WebCL foundation seems valuable. The performance of sukiyaki2 vs AMD's Caffe port is impressive. There is no new algorithm here, nor is there any UI/meta-design improvement.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.207526\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=r1y1aawlg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on ‘right context’ The motivation given is that often translators (and text generators) use a process of refinement in generating outputs.\n",
            "  Candidate 2: This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on ‘right context’ The motivation given is that often translators (and text generators generally) use a process of refinement.\n",
            "  Candidate 3: This paper proposes a model for iteratively refining translation hypotheses. This has several benefits, including enabling the translation model to condition not only on “left context”, but also on ‘right context’ The motivation given is that often translators use a process of refinement in generating outputs.\n",
            "  Candidate 4: This paper proposes a method for improving the output of an existing machine translation by identifying potential mistakes and proposing a substitution. It is motivated by the method in which (it is assumed) human translators operate. The paper is interesting and imaginative, but I am somewhat sceptical of this kind of approach.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.102708\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rJEgeXFex\n",
            "Generated candidates:\n",
            "  Candidate 1: This is a well written, organized, and presented paper that I enjoyed reading. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.\n",
            "  Candidate 2: This is a well written, organized, and presented paper that I enjoyed reading. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy datasets in machine learning.\n",
            "  Candidate 3: This is a well written, organized, and presented paper that I enjoyed reading. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and, frankly, more interesting than the typical toy dataset in machine learning.\n",
            "  Candidate 4: This is a well written, organized, and presented paper that I enjoyed reading. While it did not present any new methodology or architecture, it instead addressed an important application of predicting the medications a patient is using, given the record of billing codes. The dataset they use is impressive and useful and more interesting than the typical toy datasets in machine learning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.23s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.100579\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rJLS7qKel\n",
            "Generated candidates:\n",
            "  Candidate 1: Algorithm was the winner of the Visual Doom AI competition. Key idea of their algorithm is to use additional low-dimensional observations as a supervised target for prediction. This prediction is conditioned on a goal vector and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal.\n",
            "  Candidate 2: Algorithm was the winner of the Visual Doom AI competition. Key idea of their algorithm is to use additional low-dimensional observations as a supervised target for prediction. This prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen.\n",
            "  Candidate 3: Algorithm was the winner of the Visual Doom AI competition. Key idea is to use additional low-dimensional observations as a supervised target for prediction. Algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not.\n",
            "  Candidate 4: Algorithm was the winner of the Visual Doom AI competition. Key idea is to use low-dimensional observations as a supervised target for prediction. Algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.229505\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rJRhzzKxl\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. It is not clear what is the advantage of the proposed method vs existing feature learning techniques.\n",
            "  Candidate 2: This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. The idea is straight-forward, albeit fairly heuristic in several cases.\n",
            "  Candidate 3: This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset. The results of the experiments could be improved but still justify the validity of applying knowledge distillation for transfer learning. The idea is straight-forward, albeit fairly heuristic in several cases.\n",
            "  Candidate 4: This paper studies the problem of transfer learning in the context of domain adaptation. Several settings are presented along with experiments on the Amazon Reviews dataset. The results of the experiments could be improved but still justify the validity of applying distillation for transfer learning. The paper is well written and easy to follow.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.092663\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rJXTf9Bxg\n",
            "Generated candidates:\n",
            "  Candidate 1: This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated, and independently to predict the class label corresponding to the sample.\n",
            "  Candidate 2: This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The authors attempt at evaluating the method quantitatively by looking at the discriminability and diversity of samples.\n",
            "  Candidate 3: This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The method also encourages correspondence between the latent c and the real class labels for the real examples.\n",
            "  Candidate 4: This submission proposes method for class-conditional generative image modeling using auxiliary classifiers. Compared to normal GANs the generator also receives a randomly sampled class label c from the class distribution. The discriminator has two outputs and two corresponding objectives: determine whether a sample is real or generated.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.270579\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rJfMusFll\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper extends neural conversational models into the batch reinforcement learning setting. The approach is well motivated and the paper is well written, except for some intuitions for why the batch version outperforms the on-line version. The primary dataset used (restaurant recommendations) is very small (6000 conversations)\n",
            "  Candidate 2: This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Wen et al. (2016) are able to do this on a similarly small restaurant dataset.\n",
            "  Candidate 3: This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. The author propose to use a off-policy actor-critic algorithm in a batch-setting.\n",
            "  Candidate 4: This paper extends neural conversational models into the batch reinforcement learning setting. The idea is that you can collect human scoring data for some responses from a dialogue model, however such scores are expensive. Wen et al. are able to do this on a similarly small restaurant dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.107947\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rJo9n9Feg\n",
            "Generated candidates:\n",
            "  Candidate 1: Game of tic-tac-toe is considered. There are 18 possible moves - 2 players x 9 locations. CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to the winning board locations.\n",
            "  Candidate 2: Game of tic-tac-toe is considered. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes. Authors find that predictions correspond to winning board locations.\n",
            "  Candidate 3: Game of tic-tac-toe is considered. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs. Authors find that predictions correspond to the winning board locations.\n",
            "  Candidate 4: Game of tic-tac-toe is considered. There are 18 possible moves - 2 players x 9 locations. A CNN is trained from a visual rendering of the game board to these 18 possible outputs. CAM technique is used to visualize the salient regions in the inputs responsible for the prediction that CNN makes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.154839\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rJzaDdYxx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The main interesting observation in this paper is that scaling an image by a small alpha places more “importance” on pixels on the object related to the correct class prediction.\n",
            "  Candidate 2: This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. The main interesting observation in this paper is that scaling an image by a small alpha places more ‘importance’ on pixels on the object related to the correct class prediction.\n",
            "  Candidate 3: This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. While motivation and qualitative examples are appealing, the paper lacks both qualitative and quantitative comparison to prior work.\n",
            "  Candidate 4: This paper proposes a new method, interior gradients, for analysing feature importance in deep neural networks. The authors propose to measure “feature importance”, or specifically, which pixels contribute most to a network’s classification of an image. While motivation and qualitative examples are appealing, the paper lacks both quantitative comparison to prior work.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.30s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.226273\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rkGabzZgl\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.\n",
            "  Candidate 2: The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML.\n",
            "  Candidate 3: The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.\n",
            "  Candidate 4: The paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.121265\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rkKCdAdgx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\" The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions. The main motivation of the paper is the reduction of model size for deep neural networks.\n",
            "  Candidate 2: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\" The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions. The paper simply applies this idea, testing it on seven data sets.\n",
            "  Candidate 3: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\" The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array. The main motivation of the paper is the reduction of model size for deep neural networks.\n",
            "  Candidate 4: This paper aims at compressing binary inputs and outputs of neural network models with unsupervised \"Bloom embeddings\" The embedding is based on Bloom filters: projecting an element of a set to different positions of a binary array by several independent hash functions, which allows membership checking with no missed but with possibly some false positives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.074454\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=rkpACe1lx\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. However, as the results show, the authors could not get better results with less parameters.\n",
            "  Candidate 2: The paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task. The idea itself is very inspiring, and the experiments are very solid.\n",
            "  Candidate 3: The paper proposes an interesting new method for training neural networks, i.e., a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task.\n",
            "  Candidate 4: The paper proposes an interesting new method for training neural networks, i.e. a hypernetwork is used to generate the model parameters of the main network. The authors demonstrated that the total number of model parameters could be smaller while achieving competitive results on the image classification task.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.239069\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ry54RWtxx\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. An LSTM language model is trained over correct code, and then low probability tokens are highlighted as sources of possible error.\n",
            "  Candidate 2: This paper takes a first step towards learning to static analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. An LSTM language model is trained over correct code, and then low probability tokens are highlighted as sources of possible error.\n",
            "  Candidate 3: This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. An LSTM language model is trained over correct code and then low probability tokens are highlighted as sources of possible error.\n",
            "  Candidate 4: This paper takes a first step towards learning to statically analyze source code. It develops a simple toy programming language that includes loops and branching. The aim is to determine whether all variables in the program are defined before they are used. An LSTM language model is trained over correct code, and then low probability tokens are highlighted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.067007\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ryAe2WBee\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper proposes a semantic embedding based approach to multilabel classification. SEM models the labels for an instance as draws from a multinomial distribution. The proposed training algorithm is slightly more complicated than vanilla backprop. The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear.\n",
            "  Candidate 2: The paper proposes a semantic embedding based approach to multilabel classification. SEM models the labels for an instance as draws from a multinomial distribution. The proposed training algorithm is slightly more complicated than vanilla backprop. The significance of the results compared to NNML is not very clear.\n",
            "  Candidate 3: The paper proposes a semantic embedding based approach to multilabel classification. The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. The experimental results are not significant enough to compensate the lack of conceptual novelty.\n",
            "  Candidate 4: The paper proposes a semantic embedding based approach to multilabel classification. SEM models the labels for an instance as draws from a multinomial distribution. The proposed training algorithm is slightly more complicated than vanilla backprop. The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not veryclear.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:04<00:00,  4.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.216880\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ryMxXPFex\n",
            "Generated candidates:\n",
            "  Candidate 1: Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds. In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters.\n",
            "  Candidate 2: Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds (induced by the discrete variables) In essence the model clusters the data and at the same time learns a continuous manifold representation.\n",
            "  Candidate 3: Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models the actual manifolds. In essence the model clusters the data and at the same time learns a continuous manifold representation.\n",
            "  Candidate 4: Paper proposes a novel Variational Encoder architecture that contains discrete variables. Model contains an undirected discrete component that captures distribution over disconnected manifolds and a directed hierarchical continuous component that models actual manifolds. In essence the model clusters the data and at the same time learns a continuous manifold representation for the clusters.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.090831\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ryUPiRvge\n",
            "Generated candidates:\n",
            "  Candidate 1: Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. The approach is rather simple and hence can be applied to existing methods. The major difference is incorporating functions with 2 or more inputs.\n",
            "  Candidate 2: Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. The approach is rather simple and hence can be applied to existing methods.\n",
            "  Candidate 3: Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. The approach is rather simple and hence can be applied to existing methods.\n",
            "  Candidate 4: Thank you for an interesting perspective on the neural approaches to approximate physical phenomenon. This paper describes a method to extrapolate a given dataset and predict formulae with naturally occurring functions like sine, cosine, multiplication etc. The approach is rather simple and can be applied to existing methods.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.273325\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ryXZmzNeg\n",
            "Generated candidates:\n",
            "  Candidate 1: This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution. The paper in its current form is not acceptable due to the following reasons: No quantitative evaluation. The description of the model is very unclear.\n",
            "  Candidate 2: This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution. The paper uses confusing notation, oversells the novelty, and ignores some relevant previous results.\n",
            "  Candidate 3: This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution. The paper uses confusing notation, oversells the novelty, ignoring some relevant previous results.\n",
            "  Candidate 4: This paper attempts to learn a Markov chain to estimate a probability distribution over latent variables Z, such that P(X | Z) can be eased to generate samples from a data distribution. The paper in its current form is not acceptable due to the following reasons: There is no quantitative evaluation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.212163\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ryb-q1Olg\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The method relies mainly on techniques presented in a NIPS 2015 paper. The style of writing is terrible and completely unacceptable as a scientific publication. The experimental procedure should beclarified further.\n",
            "  Candidate 2: The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The method seems potentially quite interesting but the paper has serious problems in the presentation. The style of writing is terrible and completely unacceptable as a scientific publication.\n",
            "  Candidate 3: The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The style of writing is terrible and completely unacceptable as a scientific publication. The novel contribution of the paper --- Section 2.2 --- was very difficult to understand.\n",
            "  Candidate 4: The paper presents a repurposing of rectified factor networks proposed earlier by the same authors to biclustering. The method seems potentially quite interesting but the paper has serious problems in the presentation. The style of writing is terrible and completely unacceptable as ascientific publication.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.57s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.185730\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ryhqQFKgl\n",
            "Generated candidates:\n",
            "  Candidate 1: The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. The system consists of two components, a generative component (teacher and student) and a generistic component (probabilistic) The extensive use of notation did not help the clarity of the paper.\n",
            "  Candidate 2: The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. The system consists of two components, a generative component (teacher and student) and a generistic component (probabilistic) The extensive use of notation did not help the clarity.\n",
            "  Candidate 3: The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. The system consists of two components, a generative component (teacher and student) and a generistic component (probabilistic) The extensive use of notation did not help the clarity of this paper.\n",
            "  Candidate 4: The paper presents an advanced self-learning model that extracts compositional rules from Bach's chorales. The system consists of two components, a generative component (teacher and student) and a generistic component (probabilistic) The extensive use of notation did not help clarity.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.087009\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing id: https://openreview.net/forum?id=ryuxYmvel\n",
            "Generated candidates:\n",
            "  Candidate 1: Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines. The experimental results are impressively good for a first baseline.\n",
            "  Candidate 2: Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines.\n",
            "  Candidate 3: Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture or not. This is posed as a binary classification task. Authors propose a dataset and some deep learning based baselines.\n",
            "  Candidate 4: Use of ML in ITP is an interesting direction of research. Authors consider the problem of predicting whether a given statement would be useful in a proof of a conjecture. This is posed as a binary classification task and authors propose a dataset and some deep learning based baselines.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:07<00:00,  7.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: mean difference = 0.291686\n",
            "Iteration 2: mean difference = 0.000000\n",
            "Convergence reached at iteration 2 (diff = 0.000000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to adaptive_summarization.csv\n"
          ]
        }
      ]
    }
  ]
}
