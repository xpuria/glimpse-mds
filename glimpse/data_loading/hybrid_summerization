import argparse
from pathlib import Path
import pandas as pd
import datetime
import torch
from datasets import Dataset
from tqdm import tqdm
import nltk
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

GENERATION_CONFIGS = {
    "top_p_sampling": {
        "max_new_tokens": 200,
        "do_sample": True,
        "top_p": 0.95,
        "temperature": 1.0,
        "num_return_sequences": 8,
        "num_beams": 1,
        "min_length": 0,
        "early_stopping": True,
    }
}

def parse_args():
    parser = argparse.ArgumentParser(description="Hybrid Summarization (Extractive + Abstractive)")
    parser.add_argument("--dataset_path", type=Path, required=True, 
                       help="Path to the input dataset (CSV)")
    parser.add_argument("--output_dir", type=str, default="data/hybrid_candidates",
                       help="Directory to save output files")
    parser.add_argument("--model_name", type=str, default="facebook/bart-large-cnn",
                       help="Model name for abstractive summarization")
    parser.add_argument("--batch_size", type=int, default=16)
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--limit", type=int, default=None,
                       help="Limit number of samples to process")
    return parser.parse_args()

def prepare_dataset(dataset_path: Path) -> Dataset:
    """Load and prepare dataset, ensuring required columns exist"""
    try:
        dataset = pd.read_csv(dataset_path)
        required_columns = ['id', 'text', 'gold']
        if not all(col in dataset.columns for col in required_columns):
            raise ValueError(f"Dataset must contain columns: {required_columns}")
        return Dataset.from_pandas(dataset)
    except Exception as e:
        raise ValueError(f"Error reading dataset: {e}")

def extractive_summarization(dataset: Dataset) -> Dataset:
    """Generate extractive summaries by sentence tokenization"""
    summaries = []
    print("Generating extractive summaries...")
    
    for sample in tqdm(dataset):
        text = sample["text"]
        text = text.replace('-----', '\n')
        sentences = nltk.sent_tokenize(text)
        sentences = [sent for sent in sentences if sent.strip()]
        # Take first 3 sentences or pad with empty strings if less than 3
        while len(sentences) < 3:
            sentences.append("")
        sentences = sentences[:3]  # Limit to first 3 sentences
        summaries.append(sentences)
    
    return dataset.map(lambda example: {"extractive_summary": summaries.pop(0)})

def process_batch(batch_texts, model, tokenizer, device):
    """Process a batch of texts through the model"""
    inputs = tokenizer(
        batch_texts,
        max_length=1024,
        padding="max_length",
        truncation=True,
        return_tensors="pt"
    ).to(device)

    outputs = model.generate(
        **inputs,
        **GENERATION_CONFIGS["top_p_sampling"]
    )

    # Reshape outputs and decode
    summaries = []
    for output in outputs.reshape(len(batch_texts), -1, outputs.shape[-1]):
        summaries.append([
            tokenizer.decode(seq, skip_special_tokens=True)
            for seq in output
        ])
    return summaries

def abstractive_summarization(
    model, tokenizer, dataset: Dataset, 
    batch_size: int, device: str
) -> Dataset:
    """Generate abstractive summaries using the transformer model"""
    print("Generating abstractive summaries...")
    all_summaries = []
    
    # Process in batches without using DataLoader
    for i in tqdm(range(0, len(dataset), batch_size)):
        batch_data = dataset[i:i + batch_size]
        # Join extractive summaries into single texts
        batch_texts = [" ".join(summary) for summary in batch_data["extractive_summary"]]
        
        try:
            batch_summaries = process_batch(batch_texts, model, tokenizer, device)
            all_summaries.extend(batch_summaries)
        except Exception as e:
            print(f"Error processing batch {i}: {e}")
            # Add empty summaries for failed batch
            all_summaries.extend([[""]] * len(batch_texts))
    
    return dataset.map(lambda example, idx: {
        "abstractive_summary": all_summaries[idx]
    }, with_indices=True)

def main():
    args = parse_args()
    
    # Initialize NLTK
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    
    # Load dataset
    print("Loading dataset...")
    dataset = prepare_dataset(args.dataset_path)
    
    if args.limit:
        dataset = dataset.select(range(min(args.limit, len(dataset))))
    
    # Step 1: Extractive Summarization
    dataset = extractive_summarization(dataset)
    
    # Step 2: Abstractive Summarization
    model = AutoModelForSeq2SeqLM.from_pretrained(args.model_name).to(args.device)
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    
    # Set padding token if not set
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        
    dataset = abstractive_summarization(
        model, tokenizer, dataset,
        args.batch_size, args.device
    )
    
    # Convert to DataFrame and save
    df_dataset = dataset.to_pandas()
    
    # Process summaries for RSA computation format
    df_final = pd.DataFrame()
    for idx, row in df_dataset.iterrows():
        abstractive_summaries = row['abstractive_summary']
        
        # For RSA computation format
        for id_candidate, summary in enumerate(abstractive_summaries):
            if summary.strip():  # Only include non-empty summaries
                df_final = pd.concat([df_final, pd.DataFrame({
                    'index': [idx],
                    'id': [row['id']],  # Use original ID from dataset
                    'text': [row['text']],
                    'gold': [row['gold']],  # Include gold summary from original data
                    'summary': [summary],
                    'id_candidate': [id_candidate]
                })], ignore_index=True)
    
    # Save results
    output_path = Path(args.output_dir) / f"hybrid_summaries_{datetime.datetime.now():%Y%m%d_%H%M%S}.csv"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df_final.to_csv(output_path, index=False)
    print(f"Saved hybrid summaries to {output_path}")

if __name__ == "__main__":
    main()